{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = (X_train - 127.5) / 127.5\n",
    "X_test = (X_test - 127.5) / 127.5\n",
    "\n",
    "n_labeled = 100\n",
    "X_labeled, X_unlabeled = X_train[:n_labeled], X_train[n_labeled:]\n",
    "y_labeled = y_train[:n_labeled]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator:\n",
    "    \n",
    "    def __init__(self):\n",
    "        fc = tf.keras.layers.Dense(7*7*128, activation=tf.nn.relu, name=\"fc_generator\")\n",
    "        \n",
    "        reshape = tf.keras.layers.Reshape((7, 7, 128))\n",
    "\n",
    "        conv_t1 = tf.keras.layers.Conv2DTranspose(filters=64,\n",
    "                                                  kernel_size=(5,5),\n",
    "                                                  strides=(1,1),\n",
    "                                                  padding='same',\n",
    "                                                  name=\"conv_t_generator1\")\n",
    "        bn1 = tf.keras.layers.BatchNormalization()\n",
    "        act1 = tf.keras.layers.ReLU()\n",
    "\n",
    "        conv_t2 = tf.keras.layers.Conv2DTranspose(filters=32,\n",
    "                                                  kernel_size=(5,5),\n",
    "                                                  strides=(2,2),\n",
    "                                                  padding='same',\n",
    "                                                  name=\"conv_t_generator2\")\n",
    "        bn2 = tf.keras.layers.BatchNormalization()\n",
    "        act2 = tf.keras.layers.ReLU()\n",
    "\n",
    "        conv_t3 = tf.keras.layers.Conv2DTranspose(filters=1,\n",
    "                                                  kernel_size=(5,5),\n",
    "                                                  strides=(2,2),\n",
    "                                                  activation=tf.nn.tanh,\n",
    "                                                  padding='same',\n",
    "                                                  name=\"conv_t_generator3\")\n",
    "\n",
    "        self.layers = [fc, reshape, conv_t1, bn1, act1, conv_t2, bn2, act2, conv_t3]\n",
    "        \n",
    "    def generate(self, rand_noise):\n",
    "        x = rand_noise\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "class GeneratorMLP:\n",
    "    \n",
    "    def __init__(self):\n",
    "        fc1 = tf.keras.layers.Dense(128, activation=tf.nn.relu, name=\"fc_generator1\")\n",
    "        fc2 = tf.keras.layers.Dense(512, activation=tf.nn.relu, name=\"fc_generator2\")\n",
    "        fc3 = tf.keras.layers.Dense(28*28, activation=tf.nn.tanh, name=\"fc_generator3\")\n",
    "\n",
    "        reshape = tf.keras.layers.Reshape((28, 28, 1))\n",
    "\n",
    "        self.layers = [fc1, fc2, fc3, reshape]\n",
    "        \n",
    "    def generate(self, rand_noise):\n",
    "        x = rand_noise\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier:\n",
    "        \n",
    "    def __init__(self):\n",
    "        dropout1 = tf.keras.layers.Dropout(rate=0.3)\n",
    "        conv1 = tf.keras.layers.Conv2D(filters=32,\n",
    "                                       kernel_size=(5,5),\n",
    "                                       strides=(3,3),\n",
    "                                       activation=tf.nn.leaky_relu,\n",
    "                                       name=\"conv_discriminator1\")\n",
    "\n",
    "        dropout2 = tf.keras.layers.Dropout(rate=0.3)\n",
    "        conv2 = tf.keras.layers.Conv2D(filters=64,\n",
    "                                       kernel_size=(4,4),\n",
    "                                       strides=(2,2),\n",
    "                                       activation=tf.nn.leaky_relu,\n",
    "                                       name=\"conv_discriminator2\")\n",
    "        \n",
    "        dropout3 = tf.keras.layers.Dropout(rate=0.3)\n",
    "        conv3 = tf.keras.layers.Conv2D(filters=128,\n",
    "                                       kernel_size=(4,4),\n",
    "                                       strides=(2,2),\n",
    "                                       activation=tf.nn.leaky_relu,\n",
    "                                       name=\"conv_discriminator3\")\n",
    "        \n",
    "        flatten = tf.keras.layers.Flatten()\n",
    "        \n",
    "        fc = tf.keras.layers.Dense(units=1, name=\"fc_discriminator\")\n",
    "\n",
    "        self.layers = [dropout1, conv1, dropout2, conv2, dropout3, conv3, flatten, fc]\n",
    "            \n",
    "    def classify(self, image):\n",
    "        x = image\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "class ClassifierMLP:\n",
    "        \n",
    "    def __init__(self, nb_classes):\n",
    "        flatten = tf.keras.layers.Flatten()\n",
    "        \n",
    "        dropout1 = tf.keras.layers.Dropout(rate=0.3)\n",
    "        fc1 = tf.keras.layers.Dense(128, activation=tf.nn.leaky_relu, name=\"fc_discriminator1\")\n",
    "          \n",
    "        dropout2 = tf.keras.layers.Dropout(rate=0.3)\n",
    "        fc2 = tf.keras.layers.Dense(64, activation=tf.nn.leaky_relu, name=\"fc_discriminator2\")\n",
    "        \n",
    "        dropout3 = tf.keras.layers.Dropout(rate=0.3)\n",
    "        fc3 = tf.keras.layers.Dense(nb_classes, name=\"fc_discriminator3\")\n",
    "\n",
    "        self.layers_features = [flatten, dropout1, fc1, dropout2, fc2]\n",
    "        self.layers_classif = [dropout3, fc3]\n",
    "            \n",
    "    def classify(self, image, is_training=True):\n",
    "        x = image\n",
    "        # Extract features\n",
    "        for layer in self.layers_features:\n",
    "            x = layer(x)#, training=is_training)\n",
    "        features = x\n",
    "        # Classification\n",
    "        for layer in self.layers_classif:\n",
    "            x = layer(x)\n",
    "        return features, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN:\n",
    "    \n",
    "    def __init__(self, im_shape, dim_noise, nb_classes):\n",
    "        self.im_shape = im_shape\n",
    "        self.dim_noise = dim_noise\n",
    "        self.nb_classes = nb_classes\n",
    "\n",
    "        with tf.variable_scope(\"GAN\"):\n",
    "            self.generator = GeneratorMLP()\n",
    "            self.classifier = ClassifierMLP(self.nb_classes)\n",
    "\n",
    "            self.batch_size = tf.placeholder(tf.int64, None, name=\"batch_size\")\n",
    "\n",
    "            # Labeled data from mnist\n",
    "            self.labeled_image = tf.placeholder(tf.float32, (None, *(self.im_shape)), name=\"original_image\")\n",
    "            self.image_label = tf.placeholder(tf.int32, (None,), name=\"image_label\")\n",
    "            dataset_labeled = tf.data.Dataset.from_tensor_slices((self.labeled_image, self.image_label)).shuffle(10000).batch(self.batch_size).repeat()\n",
    "            self.iterator_labeled = dataset_labeled.make_initializable_iterator()\n",
    "            \n",
    "            batch_images_labeled, batch_labels = self.iterator_labeled.get_next()\n",
    "            batch_images_labeled = tf.expand_dims(batch_images_labeled, -1)\n",
    "            batch_labels_oh = tf.one_hot(batch_labels, depth=self.nb_classes)\n",
    "            \n",
    "            # Unlabeled data from mnist\n",
    "            self.unlabeled_image = tf.placeholder(tf.float32, (None, *(self.im_shape)), name=\"unlabeled_image\")\n",
    "            dataset_unlabled = tf.data.Dataset.from_tensor_slices(self.unlabeled_image).shuffle(10000).batch(self.batch_size).repeat()\n",
    "            self.iterator_unlabeled = dataset_unlabled.make_initializable_iterator()\n",
    "            \n",
    "            batch_images_unlabeled = self.iterator_unlabeled.get_next()\n",
    "            batch_images_unlabeled = tf.expand_dims(batch_images_unlabeled, -1)\n",
    "            \n",
    "            # Sample and generate fake images\n",
    "            with tf.variable_scope(\"generator\"):\n",
    "                bs = tf.shape(batch_images_unlabeled)[0]\n",
    "                rand_noise = tf.random_normal((bs, self.dim_noise), name=\"rand_noise\")\n",
    "                generated_images = self.generator.generate(rand_noise)\n",
    "\n",
    "            # Use classifier\n",
    "            with tf.variable_scope(\"classifier\"):\n",
    "                features_labeled, class_logits_labeled = self.classifier.classify(batch_images_labeled)\n",
    "                features_unlabeled, _ = self.classifier.classify(batch_images_unlabeled)\n",
    "                features_fake, class_logits_fake = self.classifier.classify(generated_images)\n",
    "\n",
    "                Z_real = tf.reduce_logsumexp(class_logits_labeled, 1)\n",
    "                discrim_real = tf.reduce_logsumexp(class_logits_labeled, axis=1) - tf.reduce_logsumexp(tf.pad(class_logits_labeled, [[0,0], [0,1]]), axis=1)\n",
    "                discrim_fake = tf.reduce_logsumexp(class_logits_fake, axis=1) - tf.reduce_logsumexp(tf.pad(class_logits_fake, [[0,0], [0,1]]), axis=1)\n",
    "                \n",
    "                mean_features_unlabeled = tf.reduce_mean(features_unlabeled, axis=0)\n",
    "                mean_features_fake = tf.reduce_mean(features_fake, axis=0)\n",
    "\n",
    "            # Compute losses\n",
    "            loss_classif_supervised = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=class_logits_labeled,\n",
    "                                                                                                labels=batch_labels_oh))\n",
    "            loss_classif_unsupervised = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=discrim_real,\n",
    "                                                                                               labels=.9 * tf.ones_like(discrim_real))) + \\\n",
    "                                        tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=discrim_fake,\n",
    "                                                                                               labels=tf.zeros_like(discrim_fake)))\n",
    "            loss_classifier = loss_classif_supervised + loss_classif_unsupervised\n",
    "            \n",
    "            loss_generator = tf.reduce_mean((mean_features_unlabeled - mean_features_fake) ** 2)\n",
    "            \n",
    "            accuracy_classif_supervised = tf.reduce_mean(tf.cast(tf.equal(batch_labels,\n",
    "                                                                          tf.argmax(class_logits_labeled, axis=1, output_type=tf.int32)),\n",
    "                                                                 tf.float32))\n",
    "            \n",
    "            # Separate trainable variables\n",
    "            generator_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"GAN/generator\")\n",
    "            classifier_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"GAN/classifier\")\n",
    "\n",
    "            # Optimization\n",
    "            self.learning_rate = tf.placeholder(tf.float32, None, name=\"learning_rate\")\n",
    "            optimizer_generator = tf.train.AdamOptimizer(self.learning_rate)\n",
    "            optimizer_classifier = tf.train.AdamOptimizer(self.learning_rate)\n",
    "\n",
    "            self.classifier_train_op = optimizer_classifier.minimize(loss_classifier, var_list=classifier_variables)\n",
    "            self.generator_train_op = optimizer_generator.minimize(loss_generator, var_list=generator_variables)\n",
    "            \n",
    "            # Validate classification on test set\n",
    "            with tf.variable_scope(\"validation\"):\n",
    "                self.val_image = tf.placeholder(tf.float32, (None, *(self.im_shape)), name=\"val_image\")\n",
    "                self.val_label = tf.placeholder(tf.int32, (None,), name=\"val_label\")\n",
    "                self.val_batch_size = tf.placeholder(tf.int64, None, name=\"val_batch_size\")\n",
    "                val_dataset = tf.data.Dataset.from_tensor_slices((self.val_image, self.val_label)).shuffle(10000).batch(self.val_batch_size).repeat()\n",
    "                self.iterator_val = val_dataset.make_initializable_iterator()\n",
    "\n",
    "                val_batch_images, val_batch_labels = self.iterator_val.get_next()\n",
    "                val_batch_images = tf.expand_dims(val_batch_images, -1)\n",
    "            \n",
    "                _, val_classif_logits = self.classifier.classify(val_batch_images, is_training=False)\n",
    "                \n",
    "                self.val_accuracy_classif = tf.reduce_mean(tf.cast(tf.equal(val_batch_labels,\n",
    "                                                                            tf.argmax(val_classif_logits, axis=1, output_type=tf.int32)),\n",
    "                                                                   tf.float32))\n",
    "            \n",
    "            # Summaries\n",
    "            tf.summary.scalar(\"loss_classif_supervised\", loss_classif_supervised)\n",
    "            tf.summary.scalar(\"loss_classif_unsupervised\", loss_classif_unsupervised)\n",
    "            tf.summary.scalar(\"loss_classifier\", loss_classifier)\n",
    "            tf.summary.scalar(\"loss_generator\", loss_generator)\n",
    "            tf.summary.scalar(\"accuracy_classif_supervised\", accuracy_classif_supervised)            \n",
    "            tf.summary.scalar(\"val_accuracy_classif\", self.val_accuracy_classif)            \n",
    "            tf.summary.image(\"generated_images\", (generated_images + 1) / 2, 16)\n",
    "            self.merged_summaries = tf.summary.merge_all()\n",
    "            \n",
    "            self.saver = tf.train.Saver()\n",
    "            \n",
    "    def train(self, X_labeled, y_labeled, X_unlabeled, X_test, y_test,\n",
    "              batch_size, val_batch_size, nb_steps, learning_rate, classifier_steps, log_every, save_every, sess):\n",
    "        summary_writer = tf.summary.FileWriter(\"./tensorboard/\", sess.graph)\n",
    "\n",
    "        sess.run(self.iterator_labeled.initializer, feed_dict={self.labeled_image: X_labeled,\n",
    "                                                               self.image_label: y_labeled,\n",
    "                                                               self.batch_size: batch_size})\n",
    "        sess.run(self.iterator_unlabeled.initializer, feed_dict={self.unlabeled_image: X_unlabeled,\n",
    "                                                                 self.batch_size: batch_size})\n",
    "        sess.run(self.iterator_val.initializer, feed_dict={self.val_image: X_test,\n",
    "                                                           self.val_label: y_test,\n",
    "                                                           self.val_batch_size: val_batch_size})\n",
    "        \n",
    "        for step in range(1, nb_steps + 1):\n",
    "            # Train discriminator\n",
    "            for k in range(classifier_steps):\n",
    "                sess.run(self.classifier_train_op,\n",
    "                         feed_dict={self.learning_rate: learning_rate,\n",
    "                                    self.batch_size: batch_size})\n",
    "\n",
    "            # Train generator\n",
    "            sess.run(self.generator_train_op,\n",
    "                     feed_dict={self.learning_rate: learning_rate,\n",
    "                                self.batch_size: batch_size})\n",
    "            \n",
    "            if step % log_every == 0:\n",
    "                # Eval and summaries\n",
    "                _, summaries = sess.run([self.val_accuracy_classif, self.merged_summaries],\n",
    "                                        feed_dict={self.learning_rate: learning_rate,\n",
    "                                                   self.batch_size: batch_size})\n",
    "                print(\"Write summaries\")\n",
    "                summary_writer.add_summary(summaries, step)\n",
    "        \n",
    "            if step % save_every == 0:\n",
    "                print(\"Save model\")\n",
    "                self.saver.save(sess, \"./model/model.ckpt\")\n",
    "                \n",
    "    def restore(self, sess, ckpt_file):\n",
    "        self.saver.restore(sess, ckpt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_shape = (28, 28)\n",
    "dim_noise = 100\n",
    "nb_classes = 10\n",
    "\n",
    "batch_size = 128\n",
    "learning_rate = 1e-5\n",
    "val_batch_size = 512\n",
    "\n",
    "classifier_steps = 1\n",
    "\n",
    "nb_steps = 500000\n",
    "log_every = 1000\n",
    "save_every = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gan = GAN(im_shape, dim_noise, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    gan.restore(sess, \"./model/model.ckpt\")\n",
    "    gan.train(X_labeled, y_labeled, X_unlabeled, X_test, y_test,\n",
    "              batch_size, val_batch_size, nb_steps, learning_rate, classifier_steps,\n",
    "              log_every, save_every, sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py:1419: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "0.107421875\n",
      "0.6660156\n",
      "0.671875\n",
      "0.6972656\n",
      "0.68359375\n",
      "0.70703125\n",
      "0.65625\n",
      "0.6953125\n",
      "0.6796875\n",
      "0.6777344\n"
     ]
    }
   ],
   "source": [
    "# Test only the classifier\n",
    "with tf.Session() as sess:\n",
    "    # Labeled data from mnist\n",
    "    labeled_image = tf.placeholder(tf.float32, (None, 28, 28), name=\"image\")\n",
    "    image_label = tf.placeholder(tf.int32, (None,), name=\"image_label\")\n",
    "    dataset_labeled = tf.data.Dataset.from_tensor_slices((labeled_image, image_label)).shuffle(10000).batch(128).repeat()\n",
    "    iterator_labeled = dataset_labeled.make_initializable_iterator()\n",
    "\n",
    "    batch_images_labeled, batch_labels = iterator_labeled.get_next()\n",
    "    batch_images_labeled = tf.expand_dims(batch_images_labeled, -1)\n",
    "    batch_labels_oh = tf.one_hot(batch_labels, depth=nb_classes)\n",
    "    \n",
    "    plain_classifier = ClassifierMLP(nb_classes)\n",
    "    _, class_logits_labeled = plain_classifier.classify(batch_images_labeled)\n",
    "    \n",
    "    loss_classif_supervised = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=class_logits_labeled,\n",
    "                                                                                        labels=batch_labels_oh))\n",
    "    accuracy_classif_supervised = tf.reduce_mean(tf.cast(tf.equal(batch_labels,\n",
    "                                                                  tf.argmax(class_logits_labeled, axis=1, output_type=tf.int32)),\n",
    "                                                         tf.float32))\n",
    "            \n",
    "    with tf.variable_scope(\"validation\"):\n",
    "        val_image = tf.placeholder(tf.float32, (None, *(im_shape)), name=\"val_image\")\n",
    "        val_label = tf.placeholder(tf.int32, (None,), name=\"val_label\")\n",
    "        val_dataset = tf.data.Dataset.from_tensor_slices((val_image, val_label)).shuffle(10000).batch(512).repeat()\n",
    "        iterator_val = val_dataset.make_initializable_iterator()\n",
    "\n",
    "        val_batch_images, val_batch_labels = iterator_val.get_next()\n",
    "        val_batch_images = tf.expand_dims(val_batch_images, -1)\n",
    "\n",
    "        _, val_classif_logits = plain_classifier.classify(val_batch_images, is_training=False)\n",
    "\n",
    "        val_accuracy_classif = tf.reduce_mean(tf.cast(tf.equal(val_batch_labels,\n",
    "                                                               tf.argmax(val_classif_logits, axis=1, output_type=tf.int32)),\n",
    "                                                      tf.float32))\n",
    "        \n",
    "    train_op = tf.train.AdamOptimizer(1e-3).minimize(loss_classif_supervised)\n",
    "\n",
    "    tf.summary.scalar(\"accuracy_classif_supervised\", accuracy_classif_supervised)            \n",
    "    tf.summary.scalar(\"val_accuracy_classif\", val_accuracy_classif)            \n",
    "    merged_summaries = tf.summary.merge_all()\n",
    "    \n",
    "    sess.run(iterator_labeled.initializer, feed_dict={labeled_image: X_labeled,\n",
    "                                                      image_label: y_labeled})\n",
    "    sess.run(iterator_val.initializer, feed_dict={val_image: X_test,\n",
    "                                                  val_label: y_test})\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    \n",
    "    summary_writer = tf.summary.FileWriter(\"./tensorboard_plain_classif/\", sess.graph)\n",
    "\n",
    "    for step in range(10000):\n",
    "        sess.run(train_op)\n",
    "        \n",
    "        if step % log_every == 0:\n",
    "            # Eval and summaries\n",
    "            val_acc, summaries = sess.run([val_accuracy_classif, merged_summaries])\n",
    "            print(val_acc)\n",
    "            summary_writer.add_summary(summaries, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
