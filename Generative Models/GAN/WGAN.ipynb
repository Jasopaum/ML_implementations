{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = np.concatenate((X_train, X_test))\n",
    "X_train = (X_train - 127.5) / 127.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator:\n",
    "    \n",
    "    def __init__(self):\n",
    "        fc = tf.keras.layers.Dense(7*7*128, activation=tf.nn.relu, name=\"fc_generator\")\n",
    "        \n",
    "        reshape = tf.keras.layers.Reshape((7, 7, 128))\n",
    "\n",
    "        conv_t1 = tf.keras.layers.Conv2DTranspose(filters=64,\n",
    "                                                  kernel_size=(5,5),\n",
    "                                                  strides=(1,1),\n",
    "                                                  padding='same',\n",
    "                                                  name=\"conv_t_generator1\")\n",
    "        bn1 = tf.keras.layers.BatchNormalization()\n",
    "        act1 = tf.keras.layers.ReLU()\n",
    "\n",
    "        conv_t2 = tf.keras.layers.Conv2DTranspose(filters=32,\n",
    "                                                  kernel_size=(5,5),\n",
    "                                                  strides=(2,2),\n",
    "                                                  padding='same',\n",
    "                                                  name=\"conv_t_generator2\")\n",
    "        bn2 = tf.keras.layers.BatchNormalization()\n",
    "        act2 = tf.keras.layers.ReLU()\n",
    "\n",
    "        conv_t3 = tf.keras.layers.Conv2DTranspose(filters=1,\n",
    "                                                  kernel_size=(5,5),\n",
    "                                                  strides=(2,2),\n",
    "                                                  activation=tf.nn.tanh,\n",
    "                                                  padding='same',\n",
    "                                                  name=\"conv_t_generator3\")\n",
    "\n",
    "        self.layers = [fc, reshape, conv_t1, bn1, act1, conv_t2, bn2, act2, conv_t3]\n",
    "        \n",
    "    def generate(self, rand_noise):\n",
    "        x = rand_noise\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic:\n",
    "    \n",
    "    def __init__(self):\n",
    "        dropout1 = tf.keras.layers.Dropout(rate=0.2)\n",
    "        conv1 = tf.keras.layers.Conv2D(filters=32,\n",
    "                                       kernel_size=(5,5),\n",
    "                                       strides=(3,3),\n",
    "                                       activation=tf.nn.leaky_relu,\n",
    "                                       name=\"conv_critic1\")\n",
    "\n",
    "        dropout2 = tf.keras.layers.Dropout(rate=0.2)\n",
    "        conv2 = tf.keras.layers.Conv2D(filters=64,\n",
    "                                       kernel_size=(5,5),\n",
    "                                       strides=(3,3),\n",
    "                                       activation=tf.nn.leaky_relu,\n",
    "                                       name=\"conv_critic2\")\n",
    "\n",
    "        flatten = tf.keras.layers.Flatten()\n",
    "        \n",
    "        fc = tf.keras.layers.Dense(units=1, name=\"fc_discriminator\")\n",
    "\n",
    "        self.layers = [dropout1, conv1, dropout2, conv2, flatten, fc]\n",
    "            \n",
    "    def evaluate(self, image):\n",
    "        x = image\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WGAN:\n",
    "    \n",
    "    def __init__(self, original_im_shape, dim_noise):\n",
    "        self.original_im_shape = original_im_shape\n",
    "        self.dim_noise = dim_noise\n",
    "\n",
    "        with tf.variable_scope(\"WGAN\"):\n",
    "            self.generator = Generator()\n",
    "            self.critic = Critic()\n",
    "\n",
    "            # Data from mnist\n",
    "            self.original_image = tf.placeholder(tf.float32, (None, *(self.original_im_shape)), name=\"original_image\")\n",
    "            self.batch_size = tf.placeholder(tf.int64, None, name=\"batch_size\")\n",
    "            self.dataset = tf.data.Dataset.from_tensor_slices(self.original_image).shuffle(10000).batch(self.batch_size).repeat()\n",
    "            self.iterator = self.dataset.make_initializable_iterator()\n",
    "\n",
    "            self.original_image_exp = tf.expand_dims(self.iterator.get_next(), -1)\n",
    "\n",
    "            # Sample and generate fake images\n",
    "            with tf.variable_scope(\"generator\"):\n",
    "                #self.rand_noise = tf.random_uniform((self.batch_size, self.dim_noise), minval=-1, maxval=1, name=\"rand_noise\")\n",
    "                self.rand_noise = tf.random_normal((tf.shape(self.original_image_exp)[0], self.dim_noise), name=\"rand_noise\")\n",
    "                self.generated_images = self.generator.generate(self.rand_noise)\n",
    "\n",
    "            # Use critic\n",
    "            with tf.variable_scope(\"critic\"):\n",
    "                self.score_real = self.critic.evaluate(self.original_image_exp)\n",
    "                self.score_fake = self.critic.evaluate(self.generated_images)\n",
    "                \n",
    "            # Gradient Penalty\n",
    "            epsilon = tf.random_uniform((), 0., 1.)\n",
    "            x_between = epsilon * self.original_image_exp + (1 - epsilon) * self.generated_images\n",
    "            score_between = self.critic.evaluate(x_between)\n",
    "            grad_between = tf.gradients(score_between, x_between)\n",
    "            self.grad_penalty = tf.square(tf.norm(grad_between[0], 2) - 1)\n",
    "            \n",
    "            # Compute losses\n",
    "            self.lambda_gp = tf.placeholder(tf.float32, None, name=\"lambda_gp\")\n",
    "            self.loss_generator = - tf.reduce_mean(self.score_fake)\n",
    "            self.loss_critic = - tf.reduce_mean(self.score_real - self.score_fake) + self.lambda_gp * self.grad_penalty\n",
    "\n",
    "            # Separate trainable variables\n",
    "            self.generator_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"WGAN/generator\")\n",
    "            self.critic_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"WGAN/critic\")\n",
    "\n",
    "            # Optimization\n",
    "            self.learning_rate = tf.placeholder(tf.float32, None, name=\"learning_rate\")\n",
    "            self.optimizer_generator = tf.train.AdamOptimizer(self.learning_rate, 0, 0.9)\n",
    "            self.optimizer_critic = tf.train.AdamOptimizer(self.learning_rate, 0, 0.9)\n",
    "\n",
    "            self.generator_train_op = self.optimizer_generator.minimize(self.loss_generator, var_list=self.generator_variables)\n",
    "            self.critic_train_op = self.optimizer_critic.minimize(self.loss_critic, var_list=self.critic_variables)\n",
    "            \n",
    "            # Summaries   \n",
    "            tf.summary.scalar(\"loss_generator\", self.loss_generator)\n",
    "            tf.summary.scalar(\"loss_critic\", self.loss_critic)\n",
    "            tf.summary.scalar(\"grad_penalty\", self.grad_penalty)\n",
    "            tf.summary.image(\"generated_images\", (self.generated_images + 1) / 2, 16)\n",
    "            self.merged_summaries = tf.summary.merge_all()\n",
    "            \n",
    "            self.saver = tf.train.Saver()\n",
    "            \n",
    "    def train(self, X_train, batch_size, nb_steps, critic_steps, learning_rate, lambda_gp, log_every, save_every, sess):\n",
    "        summary_writer = tf.summary.FileWriter(\"./tensorboard/\", sess.graph)\n",
    "\n",
    "        sess.run(self.iterator.initializer, feed_dict={self.original_image: X_train,\n",
    "                                                       self.batch_size: batch_size})\n",
    "        \n",
    "        for step in range(1, nb_steps + 1):\n",
    "            # Train discriminator\n",
    "            for k in range(critic_steps):\n",
    "                _ = sess.run(self.critic_train_op,\n",
    "                             feed_dict={self.learning_rate: learning_rate,\n",
    "                                        self.lambda_gp: lambda_gp,\n",
    "                                        self.batch_size: batch_size})\n",
    "\n",
    "            # Train generator\n",
    "            _, summaries = sess.run([self.generator_train_op, self.merged_summaries],\n",
    "                                     feed_dict={self.learning_rate: learning_rate,\n",
    "                                                self.lambda_gp: lambda_gp,\n",
    "                                                self.batch_size: batch_size})\n",
    "            \n",
    "            if step % log_every == 0:\n",
    "                print(\"Write summaries\")\n",
    "                summary_writer.add_summary(summaries, step)\n",
    "        \n",
    "            if step % save_every == 0:\n",
    "                print(\"Save model\")\n",
    "                self.saver.save(sess, \"./model/model.ckpt\")\n",
    "        \n",
    "    def restore(self, sess, ckpt_file):\n",
    "        self.saver.restore(sess, ckpt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_im_shape = (28, 28)\n",
    "dim_noise = 100\n",
    "\n",
    "batch_size = 128\n",
    "learning_rate = 2e-4\n",
    "\n",
    "critic_steps = 5\n",
    "lambda_gp = 10\n",
    "\n",
    "nb_steps = 50000\n",
    "log_every = 250\n",
    "save_every = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "wgan = WGAN(original_im_shape, dim_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    wgan.train(X_train, batch_size, nb_steps, critic_steps, learning_rate, lambda_gp, log_every, save_every, sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
