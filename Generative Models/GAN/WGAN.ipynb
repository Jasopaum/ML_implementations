{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = np.concatenate((X_train, X_test))\n",
    "X_train = (X_train - 127.5) / 127.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator:\n",
    "    \n",
    "    def __init__(self):\n",
    "        fc1 = tf.keras.layers.Dense(7*7*128, use_bias=False, name=\"fc_generator1\")\n",
    "        bn1 = tf.keras.layers.BatchNormalization()\n",
    "        act1 = tf.keras.layers.ReLU()\n",
    "        \n",
    "        reshape = tf.keras.layers.Reshape((7, 7, 128))\n",
    "\n",
    "        conv_t2 = tf.keras.layers.Conv2DTranspose(filters=64,\n",
    "                                                  kernel_size=(5,5),\n",
    "                                                  strides=(1,1),\n",
    "                                                  padding='same',\n",
    "                                                  name=\"conv_t_generator1\")\n",
    "        bn2 = tf.keras.layers.BatchNormalization()\n",
    "        act2 = tf.keras.layers.ReLU()\n",
    "\n",
    "        conv_t3 = tf.keras.layers.Conv2DTranspose(filters=32,\n",
    "                                                  kernel_size=(5,5),\n",
    "                                                  strides=(2,2),\n",
    "                                                  padding='same',\n",
    "                                                  name=\"conv_t_generator2\")\n",
    "        bn3 = tf.keras.layers.BatchNormalization()\n",
    "        act3 = tf.keras.layers.ReLU()\n",
    "\n",
    "        conv_t4 = tf.keras.layers.Conv2DTranspose(filters=1,\n",
    "                                                  kernel_size=(5,5),\n",
    "                                                  strides=(2,2),\n",
    "                                                  padding='same',\n",
    "                                                  name=\"conv_t_generator3\")\n",
    "        act4 = tf.keras.layers.Activation('tanh')\n",
    "\n",
    "        self.layers = [fc1, bn1, act1, reshape, conv_t2, bn2, act2, conv_t3, bn3, act3, conv_t4, act4]\n",
    "        \n",
    "    def generate(self, rand_noise):\n",
    "        x = rand_noise\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "class GeneratorMLP:\n",
    "    \n",
    "    def __init__(self):\n",
    "        fc1 = tf.keras.layers.Dense(128, name=\"fc_generator1\")\n",
    "        bn1 = tf.keras.layers.BatchNormalization()\n",
    "        act1 = tf.keras.layers.ReLU()\n",
    "\n",
    "        fc2 = tf.keras.layers.Dense(28*28, name=\"fc_generator2\")\n",
    "        bn2 = tf.keras.layers.BatchNormalization()\n",
    "        act2 = tf.keras.layers.Activation('tanh')\n",
    "\n",
    "        reshape = tf.keras.layers.Reshape((28, 28, 1))\n",
    "\n",
    "        self.layers = [fc1, bn1, act1, fc2, bn2, act2, reshape]\n",
    "        \n",
    "    def generate(self, rand_noise):\n",
    "        x = rand_noise\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic:\n",
    "        \n",
    "    def __init__(self):\n",
    "        conv1 = tf.keras.layers.Conv2D(filters=32,\n",
    "                                       kernel_size=(5,5),\n",
    "                                       strides=(2,2),\n",
    "                                       activation=tf.nn.relu,\n",
    "                                       name=\"conv_critic1\")\n",
    "        dropout1 = tf.keras.layers.Dropout(rate=0.3)\n",
    "\n",
    "        conv2 = tf.keras.layers.Conv2D(filters=64,\n",
    "                                       kernel_size=(5,5),\n",
    "                                       strides=(2,2),\n",
    "                                       activation=tf.nn.relu,\n",
    "                                       name=\"conv_critic2\")\n",
    "        dropout2 = tf.keras.layers.Dropout(rate=0.3)\n",
    "\n",
    "        fc = tf.keras.layers.Dense(units=1,\n",
    "                                   activation=tf.nn.relu,\n",
    "                                   name=\"fc_critic\")\n",
    "\n",
    "        self.layers = [conv1, dropout1, conv2, dropout2, fc]\n",
    "            \n",
    "    def evaluate(self, image):\n",
    "        x = image\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "class CriticMLP:\n",
    "        \n",
    "    def __init__(self):\n",
    "        flatten = tf.keras.layers.Flatten()\n",
    "        \n",
    "        dropout1 = tf.keras.layers.Dropout(rate=0.3)\n",
    "        fc1 = tf.keras.layers.Dense(128, activation=tf.nn.relu, name=\"fc_critic1\")\n",
    "                \n",
    "        dropout2 = tf.keras.layers.Dropout(rate=0.3)\n",
    "        fc2 = tf.keras.layers.Dense(1, activation=tf.nn.relu, name=\"fc_critic2\")\n",
    "\n",
    "        self.layers = [flatten, dropout1, fc1, dropout2, fc2]\n",
    "            \n",
    "    def evaluate(self, image):\n",
    "        x = image\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WGAN:\n",
    "    \n",
    "    def __init__(self, original_im_shape, dim_noise):\n",
    "        self.original_im_shape = original_im_shape\n",
    "        self.dim_noise = dim_noise\n",
    "\n",
    "        with tf.variable_scope(\"WGAN\"):\n",
    "            self.generator = GeneratorMLP()\n",
    "            self.critic = CriticMLP()\n",
    "\n",
    "            # Data from mnist\n",
    "            self.original_image = tf.placeholder(tf.float32, (None, *(self.original_im_shape)), name=\"original_image\")\n",
    "            self.batch_size = tf.placeholder(tf.int64, None, name=\"batch_size\")\n",
    "            self.dataset = tf.data.Dataset.from_tensor_slices(self.original_image).shuffle(10000).batch(self.batch_size).repeat()\n",
    "            self.iterator = self.dataset.make_initializable_iterator()\n",
    "\n",
    "            self.original_image_exp = tf.expand_dims(self.iterator.get_next(), -1)\n",
    "\n",
    "            # Sample and generate fake images\n",
    "            with tf.variable_scope(\"generator\"):\n",
    "                #self.rand_noise = tf.random_uniform((self.batch_size, self.dim_noise), minval=-1, maxval=1, name=\"rand_noise\")\n",
    "                self.rand_noise = tf.random_normal((self.batch_size, self.dim_noise), name=\"rand_noise\")\n",
    "                self.generated_images = self.generator.generate(self.rand_noise)\n",
    "\n",
    "            # Use critic\n",
    "            with tf.variable_scope(\"critic\"):\n",
    "                self.score_real = self.critic.evaluate(self.original_image_exp)\n",
    "                self.score_fake = self.critic.evaluate(self.generated_images)\n",
    "\n",
    "            # Compute losses\n",
    "            self.loss_generator = - tf.reduce_mean(self.score_fake)\n",
    "            self.loss_critic = - (tf.reduce_mean(self.score_real) - tf.reduce_mean(self.score_fake))\n",
    "            #self.loss_discriminator = - tf.reduce_mean(tf.log(self.prob_true_real + 1e-8) +\n",
    "            #                                           tf.log(1 - self.prob_true_fake + 1e-8))\n",
    "\n",
    "            # Separate trainable variables\n",
    "            self.generator_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"WGAN/generator\")\n",
    "            self.critic_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"WGAN/critic\")\n",
    "\n",
    "            # Optimization\n",
    "            self.learning_rate = tf.placeholder(tf.float32, None, name=\"learning_rate\")\n",
    "            self.optimizer_generator = tf.train.RMSPropOptimizer(self.learning_rate)\n",
    "            self.optimizer_critic = tf.train.RMSPropOptimizer(self.learning_rate)\n",
    "\n",
    "            self.generator_train_op = self.optimizer_generator.minimize(self.loss_generator, var_list=self.generator_variables)\n",
    "            self.critic_train_op = self.optimizer_critic.minimize(self.loss_critic, var_list=self.critic_variables)\n",
    "            \n",
    "            # Summaries   \n",
    "            tf.summary.scalar(\"loss_generator\", self.loss_generator)\n",
    "            tf.summary.scalar(\"loss_critic\", self.loss_critic)\n",
    "            tf.summary.image(\"generated_images\", (self.generated_images + 1) / 2, 16)\n",
    "            self.merged_summaries = tf.summary.merge_all()\n",
    "            \n",
    "            self.saver = tf.train.Saver()\n",
    "            \n",
    "    def train(self, X_train, batch_size, nb_steps, learning_rate, critic_steps, save_every, sess):\n",
    "        summary_writer = tf.summary.FileWriter(\"./tensorboard/\", sess.graph)\n",
    "\n",
    "        sess.run(self.iterator.initializer, feed_dict={self.original_image: X_train,\n",
    "                                                       self.batch_size: batch_size})\n",
    "        \n",
    "        for step in range(1, nb_steps + 1):\n",
    "            # Train discriminator\n",
    "            for k in range(critic_steps):\n",
    "                _ = sess.run(self.critic_train_op,\n",
    "                             feed_dict={self.learning_rate: learning_rate,\n",
    "                                        self.batch_size: batch_size})\n",
    "                # Clip weights\n",
    "                for p in self.critic_variables:\n",
    "                    p.assign(tf.clip_by_value(p, -0.01, 0.01))\n",
    "\n",
    "            # Train generator\n",
    "            _, summaries = sess.run([self.generator_train_op, self.merged_summaries],\n",
    "                                     feed_dict={self.learning_rate: learning_rate,\n",
    "                                                self.batch_size: batch_size})\n",
    "            \n",
    "            if step % save_every == 0:\n",
    "                print(\"Save and write summaries\")\n",
    "                self.saver.save(sess, \"./model/model.ckpt\")\n",
    "                summary_writer.add_summary(summaries, step)\n",
    "        \n",
    "    def restore(self, sess, ckpt_file):\n",
    "        self.saver.restore(sess, ckpt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_im_shape = (28, 28)\n",
    "dim_noise = 16\n",
    "\n",
    "batch_size = 64\n",
    "learning_rate = 1e-4\n",
    "\n",
    "critic_steps = 4\n",
    "\n",
    "nb_steps = 100000\n",
    "save_every = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py:1419: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "wgan = WGAN(original_im_shape, dim_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save and write summaries\n",
      "Save and write summaries\n",
      "Save and write summaries\n",
      "Save and write summaries\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-16277d4f5ef2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mwgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcritic_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_every\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-f677647d509b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X_train, batch_size, nb_steps, learning_rate, critic_steps, save_every, sess)\u001b[0m\n\u001b[1;32m     65\u001b[0m                 _ = sess.run(self.critic_train_op,\n\u001b[1;32m     66\u001b[0m                              feed_dict={self.learning_rate: learning_rate,\n\u001b[0;32m---> 67\u001b[0;31m                                         self.batch_size: batch_size})\n\u001b[0m\u001b[1;32m     68\u001b[0m                 \u001b[0;31m# Clip weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1317\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m   1319\u001b[0m           options, feed_dict, fetch_list, target_list, run_metadata)\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1350\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session_run_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1352\u001b[0;31m       \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExtendSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;31m# The threshold to run garbage collection to delete dead tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    wgan.train(X_train, batch_size, nb_steps, learning_rate, critic_steps, save_every, sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
