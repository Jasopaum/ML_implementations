{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = np.concatenate((X_train, X_test))\n",
    "#X_train = (X_train - 127.5) / 127.5\n",
    "X_train = X_train / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator:\n",
    "    \n",
    "    def __init__(self):\n",
    "        fc1 = tf.keras.layers.Dense(7*7*128, use_bias=False, name=\"fc_generator1\")\n",
    "        bn1 = tf.keras.layers.BatchNormalization()\n",
    "        act1 = tf.keras.layers.ReLU()\n",
    "        \n",
    "        reshape = tf.keras.layers.Reshape((7, 7, 128))\n",
    "\n",
    "        conv_t2 = tf.keras.layers.Conv2DTranspose(filters=64,\n",
    "                                                  kernel_size=(5,5),\n",
    "                                                  strides=(1,1),\n",
    "                                                  padding='same',\n",
    "                                                  name=\"conv_t_generator1\")\n",
    "        bn2 = tf.keras.layers.BatchNormalization()\n",
    "        act2 = tf.keras.layers.ReLU()\n",
    "\n",
    "        conv_t3 = tf.keras.layers.Conv2DTranspose(filters=32,\n",
    "                                                  kernel_size=(5,5),\n",
    "                                                  strides=(2,2),\n",
    "                                                  padding='same',\n",
    "                                                  name=\"conv_t_generator2\")\n",
    "        bn3 = tf.keras.layers.BatchNormalization()\n",
    "        act3 = tf.keras.layers.ReLU()\n",
    "\n",
    "        conv_t4 = tf.keras.layers.Conv2DTranspose(filters=1,\n",
    "                                                  kernel_size=(5,5),\n",
    "                                                  strides=(2,2),\n",
    "                                                  activation=tf.nn.tanh,\n",
    "                                                  padding='same',\n",
    "                                                  name=\"conv_t_generator3\")\n",
    "\n",
    "        self.layers = [fc1, bn1, act1, reshape, conv_t2, bn2, act2, conv_t3, bn3, act3, conv_t4]\n",
    "        \n",
    "    def generate(self, rand_noise):\n",
    "        x = rand_noise\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "class GeneratorMLP:\n",
    "    \n",
    "    def __init__(self):\n",
    "        fc1 = tf.keras.layers.Dense(128, activation=tf.nn.relu, name=\"fc_generator1\")\n",
    "        fc2 = tf.keras.layers.Dense(512, activation=tf.nn.relu, name=\"fc_generator2\")\n",
    "        fc3 = tf.keras.layers.Dense(28*28, activation=tf.nn.sigmoid, name=\"fc_generator3\")\n",
    "\n",
    "        reshape = tf.keras.layers.Reshape((28, 28, 1))\n",
    "\n",
    "        self.layers = [fc1, fc2, fc3, reshape]\n",
    "        \n",
    "    def generate(self, rand_noise):\n",
    "        x = rand_noise\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator:\n",
    "        \n",
    "    def __init__(self):\n",
    "        conv1 = tf.keras.layers.Conv2D(filters=32,\n",
    "                                       kernel_size=(5,5),\n",
    "                                       strides=(2,2),\n",
    "                                       activation=tf.nn.leaky_relu,\n",
    "                                       name=\"conv_discriminator1\")\n",
    "        dropout1 = tf.keras.layers.Dropout(rate=0.3)\n",
    "\n",
    "        conv2 = tf.keras.layers.Conv2D(filters=64,\n",
    "                                       kernel_size=(5,5),\n",
    "                                       strides=(2,2),\n",
    "                                       activation=tf.nn.leaky_relu,\n",
    "                                       name=\"conv_discriminator2\")\n",
    "        dropout2 = tf.keras.layers.Dropout(rate=0.3)\n",
    "\n",
    "        fc = tf.keras.layers.Dense(units=1,\n",
    "                                   activation=tf.nn.sigmoid,\n",
    "                                   name=\"fc_discriminator\")\n",
    "\n",
    "        self.layers = [conv1, dropout1, conv2, dropout2, fc]\n",
    "            \n",
    "    def discriminate(self, image):\n",
    "        x = image\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "class DiscriminatorMLP:\n",
    "        \n",
    "    def __init__(self):\n",
    "        flatten = tf.keras.layers.Flatten()\n",
    "        \n",
    "        dropout1 = tf.keras.layers.Dropout(rate=0.5)\n",
    "        fc1 = tf.keras.layers.Dense(128, activation=tf.nn.leaky_relu, name=\"fc_discriminator1\")\n",
    "                \n",
    "        dropout2 = tf.keras.layers.Dropout(rate=0.2)\n",
    "        #fc2 = tf.keras.layers.Dense(1, activation=tf.nn.sigmoid, name=\"fc_discriminator2\")\n",
    "        fc2 = tf.keras.layers.Dense(1, name=\"fc_discriminator2\")\n",
    "\n",
    "        self.layers = [flatten, dropout1, fc1, dropout2, fc2]\n",
    "            \n",
    "    def discriminate(self, image):\n",
    "        x = image\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN:\n",
    "    \n",
    "    def __init__(self, original_im_shape, dim_noise):\n",
    "        self.original_im_shape = original_im_shape\n",
    "        self.dim_noise = dim_noise\n",
    "\n",
    "        with tf.variable_scope(\"GAN\"):\n",
    "            self.generator = GeneratorMLP()\n",
    "            self.discriminator = DiscriminatorMLP()\n",
    "\n",
    "            # Data from mnist\n",
    "            self.original_image = tf.placeholder(tf.float32, (None, *(self.original_im_shape)), name=\"original_image\")\n",
    "            self.batch_size = tf.placeholder(tf.int64, None, name=\"batch_size\")\n",
    "            self.dataset = tf.data.Dataset.from_tensor_slices(self.original_image).shuffle(10000).batch(self.batch_size).repeat()\n",
    "            self.iterator = self.dataset.make_initializable_iterator()\n",
    "\n",
    "            self.original_image_exp = tf.expand_dims(self.iterator.get_next(), -1)\n",
    "\n",
    "            # Sample and generate fake images\n",
    "            with tf.variable_scope(\"generator\"):\n",
    "                #self.rand_noise = tf.random_uniform((self.batch_size, self.dim_noise), minval=-1, maxval=1, name=\"rand_noise\")\n",
    "                self.rand_noise = tf.clip_by_value(tf.random_normal((self.batch_size, self.dim_noise), name=\"rand_noise\"), -1, 1)\n",
    "                self.generated_images = self.generator.generate(self.rand_noise)\n",
    "\n",
    "            # Use discriminator\n",
    "            with tf.variable_scope(\"discriminator\"):\n",
    "                self.prob_true_real = self.discriminator.discriminate(self.original_image_exp)\n",
    "                self.prob_true_fake = self.discriminator.discriminate(self.generated_images)\n",
    "\n",
    "            # Compute losses\n",
    "            #self.loss_generator = - tf.reduce_mean(tf.log(self.prob_true_fake + 1e-8))\n",
    "            #self.loss_discriminator = - (tf.reduce_mean(tf.log(self.prob_true_real + 1e-8)) +\n",
    "            #                             tf.reduce_mean(tf.log(1 - self.prob_true_fake + 1e-8)))\n",
    "            self.loss_generator = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.prob_true_fake,\n",
    "                                                                                         labels=tf.ones_like(self.prob_true_fake)))\n",
    "            self.loss_discriminator = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.prob_true_real,\n",
    "                                                                                             labels=.9 * tf.ones_like(self.prob_true_real))) + \\\n",
    "                                      tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.prob_true_fake,\n",
    "                                                                                             labels=tf.zeros_like(self.prob_true_fake)))\n",
    "            # Separate trainable variables\n",
    "            self.generator_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"GAN/generator\")\n",
    "            self.discriminator_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"GAN/discriminator\")\n",
    "\n",
    "            # Optimization\n",
    "            self.learning_rate = tf.placeholder(tf.float32, None, name=\"learning_rate\")\n",
    "            self.optimizer_generator = tf.train.AdamOptimizer(self.learning_rate)\n",
    "            self.optimizer_discriminator = tf.train.AdamOptimizer(self.learning_rate)\n",
    "\n",
    "            self.generator_train_op = self.optimizer_generator.minimize(self.loss_generator, var_list=self.generator_variables)\n",
    "            self.discriminator_train_op = self.optimizer_discriminator.minimize(self.loss_discriminator, var_list=self.discriminator_variables)\n",
    "            \n",
    "            # Summaries   \n",
    "            tf.summary.scalar(\"loss_generator\", self.loss_generator)\n",
    "            tf.summary.scalar(\"loss_discriminator\", self.loss_discriminator)\n",
    "            #tf.summary.image(\"generated_images\", (self.generated_images + 1) / 2, 16)\n",
    "            tf.summary.image(\"generated_images\", self.generated_images, 16)\n",
    "            self.merged_summaries = tf.summary.merge_all()\n",
    "            \n",
    "            self.saver = tf.train.Saver()\n",
    "            \n",
    "    def train(self, X_train, batch_size, nb_steps, learning_rate, discriminator_steps, save_every, sess):\n",
    "        summary_writer = tf.summary.FileWriter(\"./tensorboard/\", sess.graph)\n",
    "\n",
    "        sess.run(self.iterator.initializer, feed_dict={self.original_image: X_train,\n",
    "                                                       self.batch_size: batch_size})\n",
    "        \n",
    "        for step in range(1, nb_steps + 1):\n",
    "            # Train discriminator\n",
    "            for k in range(discriminator_steps):\n",
    "                _ = sess.run(self.discriminator_train_op,\n",
    "                             feed_dict={self.learning_rate: learning_rate,\n",
    "                                        self.batch_size: batch_size})\n",
    "            # Train generator\n",
    "            _, summaries = sess.run([self.generator_train_op, self.merged_summaries],\n",
    "                                     feed_dict={self.learning_rate: learning_rate,\n",
    "                                                self.batch_size: batch_size})\n",
    "            \n",
    "            if step % save_every == 0:\n",
    "                print(\"Save and write summaries\")\n",
    "                self.saver.save(sess, \"./model/model.ckpt\")\n",
    "                summary_writer.add_summary(summaries, step)\n",
    "        \n",
    "    def restore(self, sess, ckpt_file):\n",
    "        self.saver.restore(sess, ckpt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_im_shape = (28, 28)\n",
    "dim_noise = 16\n",
    "\n",
    "batch_size = 128\n",
    "learning_rate = 4e-4\n",
    "\n",
    "discriminator_steps = 1\n",
    "\n",
    "nb_steps = 100000\n",
    "save_every = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gan = GAN(original_im_shape, dim_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #gan.restore(sess, \"./model/model.ckpt\")\n",
    "    gan.train(X_train, batch_size, nb_steps, learning_rate, discriminator_steps, save_every, sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
