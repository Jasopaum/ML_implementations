{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = np.concatenate((X_train, X_test))\n",
    "X_train = (X_train - 127.5) / 127.5\n",
    "y_train = np.concatenate((y_train, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator:\n",
    "    \n",
    "    def __init__(self):\n",
    "        fc = tf.keras.layers.Dense(7*7*128, activation=tf.nn.relu, name=\"fc_generator\")\n",
    "        \n",
    "        reshape = tf.keras.layers.Reshape((7, 7, 128))\n",
    "\n",
    "        conv_t1 = tf.keras.layers.Conv2DTranspose(filters=64,\n",
    "                                                  kernel_size=(5,5),\n",
    "                                                  strides=(1,1),\n",
    "                                                  padding='same',\n",
    "                                                  name=\"conv_t_generator1\")\n",
    "        bn1 = tf.keras.layers.BatchNormalization()\n",
    "        act1 = tf.keras.layers.ReLU()\n",
    "\n",
    "        conv_t2 = tf.keras.layers.Conv2DTranspose(filters=32,\n",
    "                                                  kernel_size=(5,5),\n",
    "                                                  strides=(2,2),\n",
    "                                                  padding='same',\n",
    "                                                  name=\"conv_t_generator2\")\n",
    "        bn2 = tf.keras.layers.BatchNormalization()\n",
    "        act2 = tf.keras.layers.ReLU()\n",
    "\n",
    "        conv_t3 = tf.keras.layers.Conv2DTranspose(filters=1,\n",
    "                                                  kernel_size=(5,5),\n",
    "                                                  strides=(2,2),\n",
    "                                                  activation=tf.nn.tanh,\n",
    "                                                  padding='same',\n",
    "                                                  name=\"conv_t_generator3\")\n",
    "\n",
    "        self.layers = [fc, reshape, conv_t1, bn1, act1, conv_t2, bn2, act2, conv_t3]\n",
    "        \n",
    "    def generate(self, rand_noise, class_vec):\n",
    "        x = tf.concat([rand_noise, class_vec], axis=1)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "class GeneratorMLP:\n",
    "    \n",
    "    def __init__(self):\n",
    "        fc1 = tf.keras.layers.Dense(128, activation=tf.nn.relu, name=\"fc_generator1\")\n",
    "        fc2 = tf.keras.layers.Dense(512, activation=tf.nn.relu, name=\"fc_generator2\")\n",
    "        fc3 = tf.keras.layers.Dense(28*28, activation=tf.nn.tanh, name=\"fc_generator3\")\n",
    "\n",
    "        reshape = tf.keras.layers.Reshape((28, 28, 1))\n",
    "\n",
    "        self.layers = [fc1, fc2, fc3, reshape]\n",
    "        \n",
    "    def generate(self, rand_noise, class_vec):\n",
    "        x = tf.concat([rand_noise, class_vec], axis=1)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator:\n",
    "        \n",
    "    def __init__(self):\n",
    "        dropout1 = tf.keras.layers.Dropout(rate=0.4)\n",
    "        conv1 = tf.keras.layers.Conv2D(filters=32,\n",
    "                                       kernel_size=(5,5),\n",
    "                                       strides=(3,3),\n",
    "                                       activation=tf.nn.leaky_relu,\n",
    "                                       name=\"conv_discriminator1\")\n",
    "\n",
    "        dropout2 = tf.keras.layers.Dropout(rate=0.2)\n",
    "        conv2 = tf.keras.layers.Conv2D(filters=64,\n",
    "                                       kernel_size=(5,5),\n",
    "                                       strides=(3,3),\n",
    "                                       activation=tf.nn.leaky_relu,\n",
    "                                       name=\"conv_discriminator2\")\n",
    "\n",
    "        flatten = tf.keras.layers.Flatten()\n",
    "        \n",
    "        # Discriminator\n",
    "        fc_discr = tf.keras.layers.Dense(1, name=\"fc_discriminator\")\n",
    "        # Classifier\n",
    "        fc_class = tf.keras.layers.Dense(nb_classes, name=\"fc_classifier\")\n",
    "        \n",
    "        self.layers_common = [dropout1, conv1, dropout2, conv2, flatten]\n",
    "        self.layers_discr = [fc_discr]\n",
    "        self.layers_class = [fc_class] \n",
    "    \n",
    "    def discriminate(self, image):\n",
    "        x = image\n",
    "        for layer in self.layers_common:\n",
    "            x = layer(x)\n",
    "            \n",
    "        x_discr = x\n",
    "        x_class = x\n",
    "        for layer in self.layers_discr:\n",
    "            x_discr = layer(x_discr)\n",
    "        for layer in self.layers_class:\n",
    "            x_class = layer(x_class)\n",
    "            \n",
    "        return x_discr, x_class\n",
    "    \n",
    "class DiscriminatorMLP:\n",
    "        \n",
    "    def __init__(self):\n",
    "        flatten = tf.keras.layers.Flatten()\n",
    "        \n",
    "        dropout1 = tf.keras.layers.Dropout(rate=0.4)\n",
    "        fc1 = tf.keras.layers.Dense(128, activation=tf.nn.leaky_relu, name=\"fc_discriminator1\")\n",
    "                \n",
    "        dropout2 = tf.keras.layers.Dropout(rate=0.2)\n",
    "        #fc2 = tf.keras.layers.Dense(64, activation=tf.nn.leaky_relu, name=\"fc_discriminator2\")\n",
    "\n",
    "        # Discriminator\n",
    "        fc_discr = tf.keras.layers.Dense(1, name=\"fc_discriminator2\")\n",
    "        # Classifier\n",
    "        fc_class = tf.keras.layers.Dense(nb_classes, name=\"fc_classifier2\")\n",
    "        \n",
    "        self.layers_common = [flatten, dropout1, fc1, dropout2]\n",
    "        self.layers_discr = [fc_discr]\n",
    "        self.layers_class = [fc_class]\n",
    "\n",
    "    def discriminate(self, image):\n",
    "        x = image\n",
    "        for layer in self.layers_common:\n",
    "            x = layer(x)\n",
    "            \n",
    "        x_discr = x\n",
    "        x_class = x\n",
    "        for layer in self.layers_discr:\n",
    "            x_discr = layer(x_discr)\n",
    "        for layer in self.layers_class:\n",
    "            x_class = layer(x_class)\n",
    "            \n",
    "        return x_discr, x_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACGAN:\n",
    "    \n",
    "    def __init__(self, original_im_shape, dim_noise, nb_classes):\n",
    "        self.original_im_shape = original_im_shape\n",
    "        self.dim_noise = dim_noise\n",
    "        self.nb_classes = nb_classes\n",
    "\n",
    "        with tf.variable_scope(\"AC-GAN\"):\n",
    "            self.generator = Generator()\n",
    "            self.discriminator = Discriminator()\n",
    "\n",
    "            # Data from mnist\n",
    "            self.original_image = tf.placeholder(tf.float32, (None, *(self.original_im_shape)), name=\"original_image\")\n",
    "            self.image_label = tf.placeholder(tf.int32, (None,), name=\"image_label\")\n",
    "            self.batch_size = tf.placeholder(tf.int64, None, name=\"batch_size\")\n",
    "            self.dataset = tf.data.Dataset.from_tensor_slices((self.original_image, self.image_label)).shuffle(10000).batch(self.batch_size).repeat()\n",
    "            self.iterator = self.dataset.make_initializable_iterator()\n",
    "\n",
    "            self.batch_images, self.batch_labels = self.iterator.get_next()\n",
    "            self.original_image_exp = tf.expand_dims(self.batch_images, -1)\n",
    "            self.batch_labels_oh = tf.one_hot(self.batch_labels, depth=self.nb_classes)\n",
    "            \n",
    "            # Sample and generate fake images\n",
    "            with tf.variable_scope(\"generator\"):\n",
    "                bs = tf.shape(self.original_image_exp)[0]\n",
    "                self.rand_noise = tf.random_normal((bs, self.dim_noise), name=\"rand_noise\")\n",
    "                self.rand_class = tf.random.uniform((bs,), maxval=self.nb_classes, dtype=tf.int32, name=\"rand_class\")\n",
    "                self.rand_class_oh = tf.one_hot(self.rand_class, depth=self.nb_classes, dtype=tf.float32, name=\"rand_class_oh\")\n",
    "                self.generated_images = self.generator.generate(self.rand_noise, self.rand_class_oh)\n",
    "\n",
    "            # Use discriminator\n",
    "            with tf.variable_scope(\"discriminator\"):\n",
    "                self.prob_true_real, self.class_real = self.discriminator.discriminate(self.original_image_exp)\n",
    "                self.prob_true_fake, self.class_fake = self.discriminator.discriminate(self.generated_images)\n",
    "\n",
    "            # Compute losses\n",
    "            self.loss_classif = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.class_real,\n",
    "                                                                                       labels=self.batch_labels_oh) +\n",
    "                                               tf.nn.sigmoid_cross_entropy_with_logits(logits=self.class_fake,\n",
    "                                                                                       labels=self.rand_class_oh))\n",
    "            self.loss_generator = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.prob_true_fake,\n",
    "                                                                                         labels=tf.ones_like(self.prob_true_fake)))\n",
    "            self.loss_discriminator = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.prob_true_real,\n",
    "                                                                                             labels=.9 * tf.ones_like(self.prob_true_real)) +\n",
    "                                                     tf.nn.sigmoid_cross_entropy_with_logits(logits=self.prob_true_fake,\n",
    "                                                                                             labels=tf.zeros_like(self.prob_true_fake)))\n",
    "            self.loss_discriminator += self.loss_classif\n",
    "            self.loss_generator += self.loss_classif\n",
    "            \n",
    "            # Separate trainable variables\n",
    "            self.generator_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"AC-GAN/generator\")\n",
    "            self.discriminator_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"AC-GAN/discriminator\")\n",
    "\n",
    "            # Optimization\n",
    "            self.learning_rate = tf.placeholder(tf.float32, None, name=\"learning_rate\")\n",
    "            self.optimizer_generator = tf.train.AdamOptimizer(self.learning_rate)\n",
    "            self.optimizer_discriminator = tf.train.AdamOptimizer(self.learning_rate)\n",
    "\n",
    "            self.generator_train_op = self.optimizer_generator.minimize(self.loss_generator, var_list=self.generator_variables)\n",
    "            self.discriminator_train_op = self.optimizer_discriminator.minimize(self.loss_discriminator, var_list=self.discriminator_variables)\n",
    "            \n",
    "            # Summaries   \n",
    "            tf.summary.scalar(\"loss_generator\", self.loss_generator)\n",
    "            tf.summary.scalar(\"loss_discriminator\", self.loss_discriminator)\n",
    "            tf.summary.scalar(\"loss_classif\", self.loss_classif)\n",
    "            tf.summary.image(\"generated_images\", (self.generated_images + 1) / 2, 16)\n",
    "            self.merged_summaries = tf.summary.merge_all()\n",
    "            \n",
    "            self.saver = tf.train.Saver()\n",
    "            \n",
    "    def train(self, X_train, y_train, batch_size, nb_steps, learning_rate, discriminator_steps, log_every, save_every, sess):\n",
    "        summary_writer = tf.summary.FileWriter(\"./tensorboard/\", sess.graph)\n",
    "\n",
    "        sess.run(self.iterator.initializer, feed_dict={self.original_image: X_train,\n",
    "                                                       self.image_label: y_train,\n",
    "                                                       self.batch_size: batch_size})\n",
    "        \n",
    "        for step in range(1, nb_steps + 1):\n",
    "            # Train discriminator\n",
    "            for k in range(discriminator_steps):\n",
    "                _ = sess.run(self.discriminator_train_op,\n",
    "                             feed_dict={self.learning_rate: learning_rate,\n",
    "                                        self.batch_size: batch_size})\n",
    "            # Train generator\n",
    "            _, summaries = sess.run([self.generator_train_op, self.merged_summaries],\n",
    "                                     feed_dict={self.learning_rate: learning_rate,\n",
    "                                                self.batch_size: batch_size})\n",
    "            \n",
    "            if step % log_every == 0:\n",
    "                print(\"Write summaries\")\n",
    "                summary_writer.add_summary(summaries, step)\n",
    "        \n",
    "            if step % save_every == 0:\n",
    "                print(\"Save model\")\n",
    "                self.saver.save(sess, \"./model/model.ckpt\")\n",
    "                \n",
    "    def restore(self, sess, ckpt_file):\n",
    "        self.saver.restore(sess, ckpt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_im_shape = (28, 28)\n",
    "dim_noise = 100\n",
    "nb_classes = 10\n",
    "\n",
    "batch_size = 128\n",
    "learning_rate = 4e-4\n",
    "\n",
    "discriminator_steps = 1\n",
    "\n",
    "nb_steps = 50000\n",
    "log_every = 200\n",
    "save_every = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acgan = ACGAN(original_im_shape, dim_noise, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #acgan.restore(sess, \"./model/model.ckpt\")\n",
    "    acgan.train(X_train, y_train, batch_size, nb_steps, learning_rate, discriminator_steps, log_every, save_every, sess)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
