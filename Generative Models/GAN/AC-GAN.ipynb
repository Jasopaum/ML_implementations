{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = np.concatenate((X_train, X_test))\n",
    "X_train = (X_train - 127.5) / 127.5\n",
    "y_train = np.concatenate((y_train, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.fc_embed_class = tf.keras.layers.Dense(7*7*128, name=\"fc_embed_class\")\n",
    "        self.fc_proj_noise = tf.keras.layers.Dense(7*7*128, name=\"fc_proj_noise\")\n",
    "        \n",
    "        reshape = tf.keras.layers.Reshape((7, 7, 128))\n",
    "\n",
    "        conv_t1 = tf.keras.layers.Conv2DTranspose(filters=64,\n",
    "                                                  kernel_size=(5,5),\n",
    "                                                  strides=(1,1),\n",
    "                                                  padding='same',\n",
    "                                                  name=\"conv_t_generator1\")\n",
    "        bn1 = tf.keras.layers.BatchNormalization()\n",
    "        act1 = tf.keras.layers.ReLU()\n",
    "\n",
    "        conv_t2 = tf.keras.layers.Conv2DTranspose(filters=32,\n",
    "                                                  kernel_size=(5,5),\n",
    "                                                  strides=(2,2),\n",
    "                                                  padding='same',\n",
    "                                                  name=\"conv_t_generator2\")\n",
    "        bn2 = tf.keras.layers.BatchNormalization()\n",
    "        act2 = tf.keras.layers.ReLU()\n",
    "\n",
    "        conv_t3 = tf.keras.layers.Conv2DTranspose(filters=1,\n",
    "                                                  kernel_size=(5,5),\n",
    "                                                  strides=(2,2),\n",
    "                                                  activation=tf.nn.tanh,\n",
    "                                                  padding='same',\n",
    "                                                  name=\"conv_t_generator3\")\n",
    "\n",
    "        self.layers = [reshape, conv_t1, bn1, act1, conv_t2, bn2, act2, conv_t3]\n",
    "        \n",
    "    def generate(self, rand_noise, class_vec):\n",
    "        x_noise = self.fc_proj_noise(rand_noise)\n",
    "        x_class = self.fc_embed_class(class_vec)\n",
    "        x = x_noise * x_class\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "class GeneratorMLP:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.fc_embed_class = tf.keras.layers.Dense(128, name=\"fc_embed_class\")\n",
    "        self.fc_proj_noise = tf.keras.layers.Dense(128, name=\"fc_generator1\")\n",
    "        \n",
    "        fc1 = tf.keras.layers.Dense(512, activation=tf.nn.relu, name=\"fc_generator2\")\n",
    "        fc2 = tf.keras.layers.Dense(28*28, activation=tf.nn.tanh, name=\"fc_generator3\")\n",
    "\n",
    "        reshape = tf.keras.layers.Reshape((28, 28, 1))\n",
    "\n",
    "        self.layers = [fc1, fc2, reshape]\n",
    "        \n",
    "    def generate(self, rand_noise, class_vec):\n",
    "        x_noise = self.fc_proj_noise(rand_noise)\n",
    "        x_class = self.fc_embed_class(class_vec)\n",
    "        x = x_noise * x_class\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator:\n",
    "        \n",
    "    def __init__(self):\n",
    "        #noise_layer = tf.keras.layers.GaussianNoise(stddev=0.05)\n",
    "        dropout1 = tf.keras.layers.Dropout(rate=0.3)\n",
    "        conv1 = tf.keras.layers.Conv2D(filters=32,\n",
    "                                       kernel_size=(5,5),\n",
    "                                       strides=(3,3),\n",
    "                                       activation=tf.nn.leaky_relu,\n",
    "                                       name=\"conv_discriminator1\")\n",
    "\n",
    "        dropout2 = tf.keras.layers.Dropout(rate=0.3)\n",
    "        conv2 = tf.keras.layers.Conv2D(filters=64,\n",
    "                                       kernel_size=(4,4),\n",
    "                                       strides=(2,2),\n",
    "                                       activation=tf.nn.leaky_relu,\n",
    "                                       name=\"conv_discriminator2\")\n",
    "        \n",
    "        dropout3 = tf.keras.layers.Dropout(rate=0.3)\n",
    "        conv3 = tf.keras.layers.Conv2D(filters=128,\n",
    "                                       kernel_size=(4,4),\n",
    "                                       strides=(2,2),\n",
    "                                       activation=tf.nn.leaky_relu,\n",
    "                                       name=\"conv_discriminator3\")\n",
    "        \n",
    "        flatten = tf.keras.layers.Flatten()\n",
    "        \n",
    "        # Discriminator\n",
    "        fc_discr = tf.keras.layers.Dense(1, name=\"fc_discriminator\")\n",
    "        # Classifier\n",
    "        fc_class = tf.keras.layers.Dense(nb_classes, name=\"fc_classifier\")\n",
    "        \n",
    "        self.layers_common = [noise_layer, dropout1, conv1, dropout2, conv2, dropout3, conv3, flatten]\n",
    "        self.layers_discr = [fc_discr]\n",
    "        self.layers_class = [fc_class] \n",
    "    \n",
    "    def discriminate(self, image):\n",
    "        x = image\n",
    "        for layer in self.layers_common:\n",
    "            x = layer(x)\n",
    "            \n",
    "        x_discr = x\n",
    "        x_class = x\n",
    "        for layer in self.layers_discr:\n",
    "            x_discr = layer(x_discr)\n",
    "        for layer in self.layers_class:\n",
    "            x_class = layer(x_class)\n",
    "            \n",
    "        return x_discr, x_class\n",
    "    \n",
    "class DiscriminatorMLP:\n",
    "        \n",
    "    def __init__(self):\n",
    "        noise_layer = tf.keras.layers.GaussianNoise(stddev=0.05)\n",
    "        flatten = tf.keras.layers.Flatten()\n",
    "        \n",
    "        dropout1 = tf.keras.layers.Dropout(rate=0.4)\n",
    "        fc1 = tf.keras.layers.Dense(128, activation=tf.nn.leaky_relu, name=\"fc_discriminator1\")\n",
    "                \n",
    "        dropout2 = tf.keras.layers.Dropout(rate=0.2)\n",
    "        fc2 = tf.keras.layers.Dense(64, activation=tf.nn.leaky_relu, name=\"fc_discriminator2\")\n",
    "\n",
    "        # Discriminator\n",
    "        fc_discr = tf.keras.layers.Dense(1, name=\"fc_discriminator2\")\n",
    "        # Classifier\n",
    "        fc_class = tf.keras.layers.Dense(nb_classes, name=\"fc_classifier2\")\n",
    "        \n",
    "        self.layers_common = [noise_layer, flatten, dropout1, fc1, dropout2, fc2]\n",
    "        self.layers_discr = [fc_discr]\n",
    "        self.layers_class = [fc_class]\n",
    "\n",
    "    def discriminate(self, image):\n",
    "        x = image\n",
    "        for layer in self.layers_common:\n",
    "            x = layer(x)\n",
    "            \n",
    "        x_discr = x\n",
    "        x_class = x\n",
    "        for layer in self.layers_discr:\n",
    "            x_discr = layer(x_discr)\n",
    "        for layer in self.layers_class:\n",
    "            x_class = layer(x_class)\n",
    "            \n",
    "        return x_discr, x_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACGAN:\n",
    "    \n",
    "    def __init__(self, original_im_shape, dim_noise, nb_classes):\n",
    "        self.original_im_shape = original_im_shape\n",
    "        self.dim_noise = dim_noise\n",
    "        self.nb_classes = nb_classes\n",
    "\n",
    "        with tf.variable_scope(\"AC-GAN\"):\n",
    "            self.generator = GeneratorMLP()\n",
    "            self.discriminator = DiscriminatorMLP()\n",
    "\n",
    "            # Data from mnist\n",
    "            self.original_image = tf.placeholder(tf.float32, (None, *(self.original_im_shape)), name=\"original_image\")\n",
    "            self.image_label = tf.placeholder(tf.int32, (None,), name=\"image_label\")\n",
    "            self.batch_size = tf.placeholder(tf.int64, None, name=\"batch_size\")\n",
    "            self.dataset = tf.data.Dataset.from_tensor_slices((self.original_image, self.image_label)).shuffle(10000).batch(self.batch_size).repeat()\n",
    "            self.iterator = self.dataset.make_initializable_iterator()\n",
    "\n",
    "            self.batch_images, self.batch_labels = self.iterator.get_next()\n",
    "            self.original_image_exp = tf.expand_dims(self.batch_images, -1)\n",
    "            self.batch_labels_oh = tf.one_hot(self.batch_labels, depth=self.nb_classes)\n",
    "            \n",
    "            # Sample and generate fake images\n",
    "            with tf.variable_scope(\"generator\"):\n",
    "                bs = tf.shape(self.original_image_exp)[0]\n",
    "                self.rand_noise = tf.random_normal((bs, self.dim_noise), name=\"rand_noise\")\n",
    "                self.rand_class = tf.random.uniform((bs,), maxval=self.nb_classes, dtype=tf.int32, name=\"rand_class\")\n",
    "                self.rand_class_oh = tf.one_hot(self.rand_class, depth=self.nb_classes, dtype=tf.float32, name=\"rand_class_oh\")\n",
    "                self.generated_images = self.generator.generate(self.rand_noise, self.rand_class_oh)\n",
    "\n",
    "            # Use discriminator\n",
    "            with tf.variable_scope(\"discriminator\"):\n",
    "                self.prob_true_real, self.class_real = self.discriminator.discriminate(self.original_image_exp)\n",
    "                self.prob_true_fake, self.class_fake = self.discriminator.discriminate(self.generated_images)\n",
    "\n",
    "            # Compute losses\n",
    "            self.loss_classif_discr = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.class_real,\n",
    "                                                                                             labels=self.batch_labels_oh) +\n",
    "                                                     tf.nn.sigmoid_cross_entropy_with_logits(logits=self.class_fake,\n",
    "                                                                                             labels=self.rand_class_oh))\n",
    "            self.loss_classif_gen = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.class_fake,\n",
    "                                                                                           labels=self.rand_class_oh))\n",
    "            \n",
    "            self.loss_generator = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.prob_true_fake,\n",
    "                                                                                         labels=tf.ones_like(self.prob_true_fake)))\n",
    "            self.loss_discriminator = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.prob_true_real,\n",
    "                                                                                             labels=.9 * tf.ones_like(self.prob_true_real)) +\n",
    "                                                     tf.nn.sigmoid_cross_entropy_with_logits(logits=self.prob_true_fake,\n",
    "                                                                                             labels=tf.zeros_like(self.prob_true_fake)))\n",
    "            \n",
    "            self.loss_discriminator += self.loss_classif_discr\n",
    "            self.loss_generator += self.loss_classif_gen\n",
    "            \n",
    "            # Separate trainable variables\n",
    "            self.generator_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"AC-GAN/generator\")\n",
    "            self.discriminator_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"AC-GAN/discriminator\")\n",
    "\n",
    "            # Optimization\n",
    "            self.learning_rate = tf.placeholder(tf.float32, None, name=\"learning_rate\")\n",
    "            self.optimizer_generator = tf.train.AdamOptimizer(self.learning_rate)\n",
    "            self.optimizer_discriminator = tf.train.AdamOptimizer(self.learning_rate)\n",
    "\n",
    "            self.generator_train_op = self.optimizer_generator.minimize(self.loss_generator, var_list=self.generator_variables)\n",
    "            self.discriminator_train_op = self.optimizer_discriminator.minimize(self.loss_discriminator, var_list=self.discriminator_variables)\n",
    "            \n",
    "            # Summaries   \n",
    "            tf.summary.scalar(\"loss_generator\", self.loss_generator)\n",
    "            tf.summary.scalar(\"loss_discriminator\", self.loss_discriminator)\n",
    "            tf.summary.scalar(\"loss_classif_discr\", self.loss_classif_discr)\n",
    "            tf.summary.scalar(\"loss_classif_gen\", self.loss_classif_gen)\n",
    "            tf.summary.image(\"generated_images\", (self.generated_images + 1) / 2, 16)\n",
    "            self.merged_summaries = tf.summary.merge_all()\n",
    "            \n",
    "            self.saver = tf.train.Saver()\n",
    "            \n",
    "    def train(self, X_train, y_train, batch_size, nb_steps, learning_rate, discriminator_steps, log_every, save_every, sess):\n",
    "        summary_writer = tf.summary.FileWriter(\"./tensorboard/\", sess.graph)\n",
    "\n",
    "        sess.run(self.iterator.initializer, feed_dict={self.original_image: X_train,\n",
    "                                                       self.image_label: y_train,\n",
    "                                                       self.batch_size: batch_size})\n",
    "        \n",
    "        for step in range(1, nb_steps + 1):\n",
    "            # Train discriminator\n",
    "            for k in range(discriminator_steps):\n",
    "                _ = sess.run(self.discriminator_train_op,\n",
    "                             feed_dict={self.learning_rate: learning_rate,\n",
    "                                        self.batch_size: batch_size})\n",
    "            # Train generator\n",
    "            _, summaries = sess.run([self.generator_train_op, self.merged_summaries],\n",
    "                                     feed_dict={self.learning_rate: learning_rate,\n",
    "                                                self.batch_size: batch_size})\n",
    "            \n",
    "            if step % log_every == 0:\n",
    "                print(\"Write summaries\")\n",
    "                summary_writer.add_summary(summaries, step)\n",
    "        \n",
    "            if step % save_every == 0:\n",
    "                print(\"Save model\")\n",
    "                self.saver.save(sess, \"./model/model.ckpt\")\n",
    "                \n",
    "    def restore(self, sess, ckpt_file):\n",
    "        self.saver.restore(sess, ckpt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_im_shape = (28, 28)\n",
    "dim_noise = 100\n",
    "nb_classes = 10\n",
    "\n",
    "batch_size = 128\n",
    "learning_rate = 4e-4\n",
    "\n",
    "discriminator_steps = 1\n",
    "\n",
    "nb_steps = 500000\n",
    "log_every = 2000\n",
    "save_every = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py:1419: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "acgan = ACGAN(original_im_shape, dim_noise, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write summaries\n",
      "Write summaries\n",
      "Write summaries\n",
      "Write summaries\n",
      "Write summaries\n",
      "Save model\n",
      "Write summaries\n",
      "Write summaries\n",
      "Write summaries\n",
      "Write summaries\n",
      "Write summaries\n",
      "Save model\n",
      "Write summaries\n",
      "Write summaries\n",
      "Write summaries\n",
      "Write summaries\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-bd7980d97ea2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m#acgan.restore(sess, \"./model/model.ckpt\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0macgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_every\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_every\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-8ba337cacb7a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X_train, y_train, batch_size, nb_steps, learning_rate, discriminator_steps, log_every, save_every, sess)\u001b[0m\n\u001b[1;32m     90\u001b[0m             _, summaries = sess.run([self.generator_train_op, self.merged_summaries],\n\u001b[1;32m     91\u001b[0m                                      feed_dict={self.learning_rate: learning_rate,\n\u001b[0;32m---> 92\u001b[0;31m                                                 self.batch_size: batch_size})\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlog_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #acgan.restore(sess, \"./model/model.ckpt\")\n",
    "    acgan.train(X_train, y_train, batch_size, nb_steps, learning_rate, discriminator_steps, log_every, save_every, sess)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
