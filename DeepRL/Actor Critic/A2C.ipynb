{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from skimage import transform\n",
    "from skimage.color import rgb2gray\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "from multiprocessing import Process, Pipe\n",
    "\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_screen(screen, sz_to_process):\n",
    "    print(screen.shape)\n",
    "    print(screen)\n",
    "    gray = rgb2gray(screen)\n",
    "    #cropped_screen = gray[8:-12,5:-12]  # For Space Invaders, TODO make it cleaner\n",
    "    cropped_screen = gray[25:-10,5:-5]  # For Pong, TODO make it cleaner\n",
    "    preprocessed_screen = transform.resize(cropped_screen, sz_to_process, mode='constant', anti_aliasing=True)\n",
    "    \n",
    "    return preprocessed_screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_observations(stacked_observations, observation, is_new_episode):\n",
    "    assert isinstance(stacked_observations, deque), \"stacked_observations has not type deque\"\n",
    "    sz_to_process = stacked_observations[0].shape  #TODO make it cleaner\n",
    "        \n",
    "    if is_new_episode:\n",
    "        for _ in range(len(stacked_observations)):\n",
    "            stacked_observations.append(observation)\n",
    "    else:\n",
    "        stacked_observations.append(observation)\n",
    "    \n",
    "    last_axis = len(stacked_observations[0].shape)\n",
    "    state = np.stack(stacked_observations, axis=last_axis)\n",
    "                    \n",
    "    return state, stacked_observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_target_values(batch_rewards, next_estimated_values, batch_dones, gamma):\n",
    "    nb_seq, len_seq = batch_rewards.shape\n",
    "    batch_target_values = np.zeros_like(batch_rewards, dtype=np.float)\n",
    "    cums = next_estimated_values\n",
    "        \n",
    "    for i in range(len_seq-1, -1, -1):\n",
    "        cums = np.where(batch_dones[:, i], batch_rewards[:, i], gamma * cums + batch_rewards[:, i])\n",
    "        \n",
    "        batch_target_values[:, i] = cums\n",
    "        \n",
    "    return batch_target_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvWrapper:\n",
    "    # Do not have to handle stacked frames externally\n",
    "    \n",
    "    def __init__(self, game_name, state_size):\n",
    "        self.env = gym.make(game_name)\n",
    "        self.stacked_observations = deque([np.zeros(state_size[:-1]) for _ in range(state_size[-1])],\n",
    "                                    maxlen=state_size[-1])\n",
    "        self.nb_actions = self.env.action_space.n\n",
    "        self.observation_shape = self.env.observation_space.shape\n",
    "        self.obs_is_image = self.observation_shape == (210, 160, 3)\n",
    "        \n",
    "    def step(self, action, render=False):\n",
    "        observation, reward, done, info = self.env.step(action)\n",
    "        if self.obs_is_image:\n",
    "            observation = preprocess_screen(observation, sz_to_process)\n",
    "        if render:\n",
    "            self.render()\n",
    "            \n",
    "        if done:\n",
    "            # Reset env and state\n",
    "            observation = self.env.reset()\n",
    "            if self.obs_is_image:\n",
    "                observation = preprocess_screen(observation, sz_to_process)\n",
    "\n",
    "            stacked_state, self.stacked_observations = stack_observations(self.stacked_observations, observation, True)\n",
    "        else:\n",
    "            stacked_state, self.stacked_observations = stack_observations(self.stacked_observations, observation, False)\n",
    "\n",
    "        return stacked_state, reward, done, info\n",
    "            \n",
    "    def reset(self):\n",
    "        observation = self.env.reset()\n",
    "        if self.obs_is_image:\n",
    "            observation = preprocess_screen(observation, sz_to_process)\n",
    "\n",
    "        stacked_state, self.stacked_observations = stack_observations(self.stacked_observations, observation, True)\n",
    "        \n",
    "        return stacked_state\n",
    "        \n",
    "    def render(self):\n",
    "        self.env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker(env_remote, worker_remote, env):\n",
    "    env_remote.close()\n",
    "    while True:\n",
    "        cmd, data = worker_remote.recv()\n",
    "\n",
    "        if cmd == 'step':\n",
    "            stacked_input, reward, done, info = env.step(data)\n",
    "            worker_remote.send((stacked_input, reward, done, info))\n",
    "\n",
    "        elif cmd == 'reset':\n",
    "            stacked_input = env.reset()\n",
    "            worker_remote.send(stacked_input)\n",
    "            \n",
    "        elif cmd == 'close':\n",
    "            worker_remote.close()\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError         \n",
    "\n",
    "class VecEnvWrapper:\n",
    "    def __init__(self, env_list):\n",
    "        self.env_remotes, self.worker_remotes = zip(*[Pipe() for _ in range(len(env_list))])\n",
    "        self.processes = [Process(target=worker, args=(e_remote, w_remote, env))\n",
    "                            for (e_remote, w_remote, env) in zip(self.env_remotes, self.worker_remotes, env_list)]\n",
    "\n",
    "        for p in self.processes:\n",
    "            p.daemon = True\n",
    "            p.start()\n",
    "        for wr in self.worker_remotes:\n",
    "            wr.close()\n",
    "            \n",
    "    def step(self, actions):   \n",
    "        for r, a in zip(self.env_remotes, actions):\n",
    "            r.send(('step', a))\n",
    "        \n",
    "        step_outputs = [r.recv() for r in self.env_remotes]\n",
    "\n",
    "        stacked_inputs, rewards, dones, infos = zip(*step_outputs)\n",
    "\n",
    "        return stacked_inputs, rewards, dones, infos\n",
    "\n",
    "    def reset(self):\n",
    "        for r in self.env_remotes:\n",
    "            r.send(('reset', None))\n",
    "        return [r.recv() for r in self.env_remotes]\n",
    "    \n",
    "    def close(self):\n",
    "        for r in self.env_remotes:\n",
    "            r.send(('close', None))\n",
    "        for r in self.env_remotes:\n",
    "            r.close()\n",
    "        for p in self.processes:\n",
    "            p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticGraphImages:\n",
    "    \n",
    "    def __init__(self, state_size, nb_actions, learning_rate, scope_name):\n",
    "        self.state_size = state_size\n",
    "        self.nb_actions = nb_actions\n",
    "        self.learning_rate = learning_rate\n",
    "        self.scope_name = scope_name\n",
    "        \n",
    "        with tf.variable_scope(self.scope_name):\n",
    "            self.state = tf.placeholder(tf.float32, (None, *(self.state_size)), name=\"state\")\n",
    "            self.action = tf.placeholder(tf.uint8, (None,), name=\"action\")\n",
    "            self.action_OH = tf.one_hot(self.action, self.nb_actions, name=\"action_OH\")\n",
    "\n",
    " \n",
    "            self.target_value = tf.placeholder(tf.float32, (None,), name=\"target_value\")\n",
    "            self.advantage = tf.placeholder(tf.float32, (None,), name=\"advantage\")\n",
    "        \n",
    "            initializer = tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "            device = '/device:GPU:0' if tf.test.is_gpu_available() else '/device:CPU:0'\n",
    "            with tf.device(device):\n",
    "                # Neural net\n",
    "                conv1_F = tf.Variable(initializer((7, 7, 4, 8)))\n",
    "                self.conv1 = tf.nn.conv2d(input=self.state,\n",
    "                                          filter=conv1_F,\n",
    "                                          strides=(1,4,4,1),\n",
    "                                          padding=\"VALID\",\n",
    "                                          name=\"conv1\")\n",
    "                self.conv1_act = tf.nn.relu(self.conv1)\n",
    "\n",
    "                conv2_F = tf.Variable(initializer((5, 5, 8, 16)))\n",
    "                self.conv2 = tf.nn.conv2d(input=self.conv1_act,\n",
    "                                          filter=conv2_F,\n",
    "                                          strides=(1,2,2,1),\n",
    "                                          padding=\"VALID\",\n",
    "                                          name=\"conv2\")\n",
    "                self.conv2_act = tf.nn.relu(self.conv2)\n",
    "            \n",
    "            \n",
    "                conv3_F = tf.Variable(initializer((3, 3, 16, 16)))\n",
    "                self.conv3 = tf.nn.conv2d(input=self.conv2_act,\n",
    "                                          filter=conv3_F,\n",
    "                                          strides=(1,2,2,1),\n",
    "                                          padding=\"VALID\",\n",
    "                                          name=\"conv3\")\n",
    "                self.conv3_act = tf.nn.relu(self.conv3)\n",
    "            \n",
    "                self.flatten = tf.keras.layers.Flatten()(self.conv3_act)\n",
    "\n",
    "                # Actor part\n",
    "                self.fc_actions = tf.keras.layers.Dense(units=512,\n",
    "                                                        activation=tf.nn.relu,\n",
    "                                                        kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                                        name=\"fc_action\")(self.flatten)\n",
    "\n",
    "                self.prob_actions = tf.keras.layers.Dense(units=self.nb_actions,\n",
    "                                                          activation=tf.nn.softmax,\n",
    "                                                          kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                                          name=\"action_distribution\")(self.fc_actions)\n",
    "\n",
    "                # Critic part\n",
    "                self.fc_value = tf.keras.layers.Dense(\n",
    "                                                units=512,\n",
    "                                                activation=tf.nn.relu,\n",
    "                                                kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                                name=\"fc_value\")(self.flatten)\n",
    "\n",
    "                self.value = tf.keras.layers.Dense(units=1,\n",
    "                                                   activation=None,\n",
    "                                                   kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                                   name=\"value\")(self.fc_value)\n",
    "            \n",
    "                # Losses\n",
    "                # Actor loss\n",
    "                self.log_prob_actions = tf.math.log(self.prob_actions)\n",
    "\n",
    "                self.log_prob_chosen_action = tf.reduce_sum(self.log_prob_actions * self.action_OH, axis=1)\n",
    "                self.actor_loss = - tf.reduce_mean(self.log_prob_chosen_action * self.advantage)\n",
    "\n",
    "                # Critic loss\n",
    "                self.critic_loss = tf.reduce_mean(tf.square(self.target_value - self.value))\n",
    "\n",
    "                # Entropy: sum(p(x) * -log(p(x)))\n",
    "                self.entropy = tf.reduce_sum(tf.multiply(self.prob_actions, - self.log_prob_actions))\n",
    "\n",
    "                # TODO put coeffs as parameters\n",
    "                self.loss = 0.5 * self.critic_loss + self.actor_loss - 0.01 * self.entropy\n",
    "\n",
    "                self.optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "                self.train_op = self.optimizer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticGraphVectors:\n",
    "    \n",
    "    def __init__(self, state_size, nb_actions, learning_rate, max_grad_norm=None, scope_name=\"ACNet\"):\n",
    "        self.state_size = state_size\n",
    "        self.nb_actions = nb_actions\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.scope_name = scope_name\n",
    "        \n",
    "        with tf.variable_scope(self.scope_name):\n",
    "            self.state = tf.placeholder(tf.float32, (None, *(self.state_size)), name=\"state\")\n",
    "            self.action = tf.placeholder(tf.uint8, (None,), name=\"action\")\n",
    "            self.action_OH = tf.one_hot(self.action, self.nb_actions, name=\"action_OH\")\n",
    "\n",
    " \n",
    "            self.target_value = tf.placeholder(tf.float32, (None,), name=\"target_value\")\n",
    "            self.advantage = tf.placeholder(tf.float32, (None,), name=\"advantage\")\n",
    "        \n",
    "            initializer = tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "            device = '/device:GPU:0' if tf.test.is_gpu_available() else '/device:CPU:0'\n",
    "            with tf.device(device):\n",
    "                # Neural net\n",
    "                self.flatten_state = tf.keras.layers.Flatten()(self.state)\n",
    "                \n",
    "                # Actor part\n",
    "                self.fc_actions = tf.keras.layers.Dense(units=32,\n",
    "                                                        activation=tf.nn.relu,\n",
    "                                                        kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                                        name=\"fc_action\")(self.flatten_state)\n",
    "\n",
    "                self.prob_actions = tf.keras.layers.Dense(units=self.nb_actions,\n",
    "                                                          activation=tf.nn.softmax,\n",
    "                                                          kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                                          name=\"action_distribution\")(self.fc_actions)\n",
    "\n",
    "                # Critic part\n",
    "                self.fc_value = tf.keras.layers.Dense(units=32,\n",
    "                                                      activation=tf.nn.relu,\n",
    "                                                      kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                                      name=\"fc_value\")(self.flatten_state)\n",
    "\n",
    "                self.value = tf.keras.layers.Dense(units=1,\n",
    "                                                   activation=None,\n",
    "                                                   kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                                   name=\"value\")(self.fc_value)\n",
    "        \n",
    "        with tf.variable_scope(\"Losses\"):\n",
    "            # Actor loss\n",
    "            self.log_prob_actions = tf.math.log(self.prob_actions)\n",
    "\n",
    "            self.log_prob_chosen_action = tf.reduce_sum(self.log_prob_actions * self.action_OH, axis=1)\n",
    "            self.actor_loss = - tf.reduce_mean(self.log_prob_chosen_action * self.advantage)\n",
    "\n",
    "            # Critic loss\n",
    "            self.critic_loss = tf.reduce_mean(tf.square(self.target_value - self.value))\n",
    "\n",
    "            # Entropy: sum(p(x) * -log(p(x)))\n",
    "            self.entropy = tf.reduce_sum(tf.multiply(self.prob_actions, - self.log_prob_actions))\n",
    "\n",
    "            # TODO put coeffs as parameters\n",
    "            self.loss = 0.5 * self.critic_loss + self.actor_loss - 0.001 * self.entropy\n",
    "\n",
    "            # Optimization\n",
    "            #self.optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "            self.optimizer = tf.train.RMSPropOptimizer(self.learning_rate, decay=0.9)\n",
    "                \n",
    "            #params = tf.trainable_variables(self.scope_name)\n",
    "            grads_vars = self.optimizer.compute_gradients(self.loss)\n",
    "            grads = [gv[0] for gv in grads_vars]\n",
    "            params = [gv[1] for gv in grads_vars]\n",
    "            if max_grad_norm is not None:\n",
    "                # Clip the gradients\n",
    "                grads, grad_norm = tf.clip_by_global_norm(grads, max_grad_norm)\n",
    "            grads = list(zip(grads, params))\n",
    "\n",
    "            # Summaries\n",
    "            tf.summary.scalar('actor_loss', self.actor_loss)\n",
    "            tf.summary.scalar('critic_loss', self.critic_loss)\n",
    "            tf.summary.scalar('entropy', self.entropy)\n",
    "            tf.summary.scalar('loss', self.loss)\n",
    "            self.merged_summaries = tf.summary.merge_all()\n",
    "\n",
    "            #self.train_op = self.optimizer.minimize(self.loss)\n",
    "            self.train_op = self.optimizer.apply_gradients(grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticTrainer:\n",
    "    def __init__(self, graph, vec_env, nb_env, eval_env, gamma, state_size, ckpt_file, session):\n",
    "        self.graph = graph\n",
    "        self.vec_env = vec_env\n",
    "        self.nb_env = nb_env\n",
    "        self.eval_env = eval_env\n",
    "        self.nb_actions = self.eval_env.nb_actions\n",
    "        self.gamma = gamma\n",
    "        self.state_size = state_size\n",
    "        self.ckpt_file = ckpt_file\n",
    "        self.session = session\n",
    "        self.saver = tf.train.Saver()\n",
    "        self.summary_writer = tf.summary.FileWriter(\"./tensorboard/\", self.session.graph)\n",
    "\n",
    "    def choose_actions(self, input_states):\n",
    "        prob_actions = self.session.run(self.graph.prob_actions,\n",
    "                                        feed_dict={self.graph.state: input_states})\n",
    "\n",
    "        actions = [np.random.choice(np.arange(self.nb_actions), p=p) for p in prob_actions]\n",
    "\n",
    "        return actions\n",
    "    \n",
    "    def get_values(self, input_states):\n",
    "        values = self.session.run(self.graph.value,\n",
    "                                  feed_dict={self.graph.state: input_states})\n",
    "\n",
    "        return values.flatten()\n",
    "        \n",
    "    def choose_actions_and_get_values(self, input_states):\n",
    "        prob_actions, values = self.session.run([self.graph.prob_actions, self.graph.value],\n",
    "                                                feed_dict={self.graph.state: input_states})\n",
    "\n",
    "        actions = [np.random.choice(np.arange(self.nb_actions), p=p) for p in prob_actions]\n",
    "\n",
    "        return actions, values.flatten()\n",
    "        \n",
    "    def run_episode(self, env_to_run, max_step, render):\n",
    "\n",
    "        # Reset state\n",
    "        state = env_to_run.reset()        \n",
    "        total_reward = 0\n",
    "        \n",
    "        for step in range(max_step):\n",
    "            state = np.expand_dims(state, axis=0)\n",
    "            action = self.choose_actions(state)[0]\n",
    "\n",
    "            # Apply action to env and get next state, reward, and done bool\n",
    "            state, reward, done, _ = env_to_run.step(action, render)\n",
    "            total_reward += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        return total_reward\n",
    "    \n",
    "    def evaluate_agent(self, env_to_run, max_step, n_trials):\n",
    "        rewards = []\n",
    "        for _ in range(n_trials):\n",
    "            rewards.append(self.run_episode(env_to_run, max_step, False))\n",
    "        avg_reward = np.mean(rewards)\n",
    "        std_reward = np.std(rewards)\n",
    "        \n",
    "        return avg_reward, std_reward\n",
    "        \n",
    "    def train_on_batch(self, states, actions, target_values, advantages, iteration):\n",
    "        summaries, _ = self.session.run([self.graph.merged_summaries, self.graph.train_op],\n",
    "                                         feed_dict={self.graph.state: states,\n",
    "                                                    self.graph.action: actions,\n",
    "                                                    self.graph.target_value: target_values,\n",
    "                                                    self.graph.advantage: advantages})\n",
    "        if iteration % 250 == 0:\n",
    "            self.summary_writer.add_summary(summaries, iteration)\n",
    "         \n",
    "    def play_and_learn(self, n_iterations, steps_per_iteration,\n",
    "                       evaluate_every, save_every,\n",
    "                       ckpt_file):\n",
    "\n",
    "        # Reset states\n",
    "        states = self.vec_env.reset()       \n",
    "\n",
    "        states_all_env = np.empty((self.nb_env, steps_per_iteration, *self.state_size), dtype=np.float)\n",
    "        actions_all_env = np.empty((self.nb_env, steps_per_iteration), dtype=np.int)\n",
    "        rewards_all_env = np.empty((self.nb_env, steps_per_iteration), dtype=np.float)\n",
    "        values_all_env = np.empty((self.nb_env, steps_per_iteration), dtype=np.float)\n",
    "        dones_all_env = np.empty((self.nb_env, steps_per_iteration), dtype=np.bool)\n",
    "        \n",
    "        T = time.time()\n",
    "        for iteration in range(1, n_iterations):\n",
    "\n",
    "            for step in range(steps_per_iteration):\n",
    "                actions, values = self.choose_actions_and_get_values(states)\n",
    "                next_states, rewards, dones, _ = self.vec_env.step(actions)\n",
    "\n",
    "                clipped_rewards = np.clip(rewards, -1, 1)\n",
    "                                \n",
    "                states_all_env[:, step] = states\n",
    "                actions_all_env[:, step] = actions\n",
    "                rewards_all_env[:, step] = clipped_rewards\n",
    "                values_all_env[:, step] = values\n",
    "                dones_all_env[:, step] = dones\n",
    "                \n",
    "                states = next_states\n",
    "            \n",
    "            # Estimated values for the future\n",
    "            next_estimated_values = self.get_values(states)\n",
    "            next_estimated_values = np.where(dones, 0, next_estimated_values)\n",
    "\n",
    "            target_values_all_env = compute_target_values(np.array(rewards_all_env),\n",
    "                                                          next_estimated_values,\n",
    "                                                          np.array(dones_all_env),\n",
    "                                                          self.gamma)\n",
    "            \n",
    "            advantages_all_env = target_values_all_env - values_all_env\n",
    "            \n",
    "            # Concatenate the experiences from the different envionments\n",
    "            batch_states = np.concatenate(states_all_env)\n",
    "            batch_actions = np.concatenate(actions_all_env)\n",
    "            batch_target_values = np.concatenate(target_values_all_env)\n",
    "            batch_advantages = np.concatenate(advantages_all_env)\n",
    "            \n",
    "            self.train_on_batch(batch_states, batch_actions, batch_target_values, batch_advantages, iteration)\n",
    "            \n",
    "            if iteration % evaluate_every == 0:\n",
    "                avg_reward, std_reward = self.evaluate_agent(self.eval_env, 10000, 10)\n",
    "                print(\"Average reward over 10 trials: %f (+- %f)\" % (avg_reward, std_reward))\n",
    "                print(\"Time to play %i iterations: %s\" %(iteration, str(datetime.timedelta(seconds=time.time() - T))))\n",
    "                \n",
    "            if iteration % save_every == 0:\n",
    "                self.saver.save(self.session, self.ckpt_file)\n",
    "                print(\"Saved model after %i iterations.\" % iteration)\n",
    "                \n",
    "    def restore(self, ckpt=None):\n",
    "        if ckpt is None:\n",
    "            ckpt = self.ckpt_file\n",
    "        self.saver.restore(self.session, ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "BrokenPipeError",
     "evalue": "[Errno 32] Broken pipe",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-138-8e8653196aa1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvec_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-3dcfb0764467>\u001b[0m in \u001b[0;36mclose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_remotes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'close'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_remotes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/multiprocessing/connection.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_writable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrecv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_send_bytes\u001b[0;34m(self, buf)\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0;31m# Also note we want to avoid sending a 0-length buffer separately,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m             \u001b[0;31m# to avoid \"broken pipe\" errors if the other end closed the pipe.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheader\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_send\u001b[0;34m(self, buf, write)\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0mremaining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m             \u001b[0mremaining\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe"
     ]
    }
   ],
   "source": [
    "vec_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-163:\n",
      "Process Process-164:\n",
      "Process Process-158:\n",
      "Process Process-160:\n",
      "Process Process-165:\n",
      "Process Process-162:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Process Process-157:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Process Process-161:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Process Process-156:\n",
      "Process Process-155:\n",
      "Process Process-150:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Process Process-152:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Process Process-151:\n",
      "Traceback (most recent call last):\n",
      "Process Process-154:\n",
      "Process Process-153:\n",
      "Process Process-159:\n",
      "  File \"<ipython-input-6-3dcfb0764467>\", line 8, in worker\n",
      "    worker_remote.send((stacked_input, reward, done, info))\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 206, in send\n",
      "    self._send_bytes(ForkingPickler.dumps(obj))\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-6-3dcfb0764467>\", line 4, in worker\n",
      "    cmd, data = worker_remote.recv()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-6-3dcfb0764467>\", line 4, in worker\n",
      "    cmd, data = worker_remote.recv()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"<ipython-input-6-3dcfb0764467>\", line 4, in worker\n",
      "    cmd, data = worker_remote.recv()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"<ipython-input-6-3dcfb0764467>\", line 4, in worker\n",
      "    cmd, data = worker_remote.recv()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/reduction.py\", line 50, in dumps\n",
      "    cls(buf, protocol).dump(obj)\n",
      "  File \"<ipython-input-6-3dcfb0764467>\", line 4, in worker\n",
      "    cmd, data = worker_remote.recv()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"<ipython-input-6-3dcfb0764467>\", line 4, in worker\n",
      "    cmd, data = worker_remote.recv()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/reduction.py\", line 38, in __init__\n",
      "    super().__init__(*args)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-6-3dcfb0764467>\", line 4, in worker\n",
      "    cmd, data = worker_remote.recv()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"<ipython-input-6-3dcfb0764467>\", line 4, in worker\n",
      "    cmd, data = worker_remote.recv()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-6-3dcfb0764467>\", line 4, in worker\n",
      "    cmd, data = worker_remote.recv()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"<ipython-input-6-3dcfb0764467>\", line 4, in worker\n",
      "    cmd, data = worker_remote.recv()\n",
      "  File \"<ipython-input-6-3dcfb0764467>\", line 4, in worker\n",
      "    cmd, data = worker_remote.recv()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"<ipython-input-6-3dcfb0764467>\", line 4, in worker\n",
      "    cmd, data = worker_remote.recv()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"<ipython-input-6-3dcfb0764467>\", line 8, in worker\n",
      "    worker_remote.send((stacked_input, reward, done, info))\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"<ipython-input-6-3dcfb0764467>\", line 4, in worker\n",
      "    cmd, data = worker_remote.recv()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"<ipython-input-6-3dcfb0764467>\", line 4, in worker\n",
      "    cmd, data = worker_remote.recv()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 206, in send\n",
      "    self._send_bytes(ForkingPickler.dumps(obj))\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "sz_to_process = (4,) #(110,84)\n",
    "stack_size = 2\n",
    "state_size = (*sz_to_process, stack_size)\n",
    "\n",
    "# Create the environments\n",
    "game_name = 'CartPole-v0' #'LunarLander-v2'  #'Pong-v0'  #'SpaceInvaders-v0'\n",
    "nb_env = 16\n",
    "env_list = [EnvWrapper(game_name, state_size) for _ in range(nb_env)]\n",
    "eval_env = EnvWrapper(game_name, state_size)\n",
    "for e in env_list:\n",
    "    e.env._max_episode_steps = 2500\n",
    "eval_env.env._max_episode_steps = 2500\n",
    "\n",
    "\n",
    "vec_env = VecEnvWrapper(env_list)\n",
    "\n",
    "nb_actions = eval_env.nb_actions\n",
    "\n",
    "gamma = 0.95\n",
    "\n",
    "n_iterations = int(5e6)\n",
    "steps_per_iteration = 32\n",
    "learning_rate = 7e-4 #0.00075\n",
    "max_grad_norm = 0.5\n",
    "\n",
    "evaluate_every = 250\n",
    "save_every = 2500\n",
    "\n",
    "ckpt_file = \"./models/model_cartpole.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward over 10 trials: 65.500000 (+- 35.878266)\n",
      "Time to play 250 iterations: 0:00:22.670544\n",
      "Average reward over 10 trials: 154.500000 (+- 64.527901)\n",
      "Time to play 500 iterations: 0:00:46.551461\n",
      "Average reward over 10 trials: 199.700000 (+- 149.970030)\n",
      "Time to play 750 iterations: 0:01:10.621818\n",
      "Average reward over 10 trials: 205.000000 (+- 62.486799)\n",
      "Time to play 1000 iterations: 0:01:34.049691\n",
      "Average reward over 10 trials: 500.800000 (+- 333.120939)\n",
      "Time to play 1250 iterations: 0:01:58.933450\n",
      "Average reward over 10 trials: 254.100000 (+- 114.835056)\n",
      "Time to play 1500 iterations: 0:02:22.607178\n",
      "Average reward over 10 trials: 365.200000 (+- 236.246397)\n",
      "Time to play 1750 iterations: 0:02:46.941789\n",
      "Average reward over 10 trials: 156.700000 (+- 89.213284)\n",
      "Time to play 2000 iterations: 0:03:10.042778\n",
      "Average reward over 10 trials: 222.000000 (+- 52.074946)\n",
      "Time to play 2250 iterations: 0:03:34.070725\n",
      "Average reward over 10 trials: 181.300000 (+- 42.666263)\n",
      "Time to play 2500 iterations: 0:03:57.015371\n",
      "Saved model after 2500 iterations.\n",
      "Average reward over 10 trials: 165.600000 (+- 26.337046)\n",
      "Time to play 2750 iterations: 0:04:21.113674\n",
      "Average reward over 10 trials: 152.300000 (+- 78.913940)\n",
      "Time to play 3000 iterations: 0:04:45.328748\n",
      "Average reward over 10 trials: 163.900000 (+- 20.612860)\n",
      "Time to play 3250 iterations: 0:05:09.337345\n",
      "Average reward over 10 trials: 178.700000 (+- 66.721885)\n",
      "Time to play 3500 iterations: 0:05:32.991955\n",
      "Average reward over 10 trials: 229.600000 (+- 40.358890)\n",
      "Time to play 3750 iterations: 0:05:56.690629\n",
      "Average reward over 10 trials: 211.600000 (+- 43.669669)\n",
      "Time to play 4000 iterations: 0:06:20.881089\n",
      "Average reward over 10 trials: 251.500000 (+- 60.724377)\n",
      "Time to play 4250 iterations: 0:06:45.246809\n",
      "Average reward over 10 trials: 155.000000 (+- 26.385602)\n",
      "Time to play 4500 iterations: 0:07:08.424128\n",
      "Average reward over 10 trials: 765.500000 (+- 499.473373)\n",
      "Time to play 4750 iterations: 0:07:35.628333\n",
      "Average reward over 10 trials: 518.000000 (+- 154.863165)\n",
      "Time to play 5000 iterations: 0:08:00.783094\n",
      "Saved model after 5000 iterations.\n",
      "Average reward over 10 trials: 416.600000 (+- 170.570924)\n",
      "Time to play 5250 iterations: 0:08:25.233557\n",
      "Average reward over 10 trials: 299.900000 (+- 83.093261)\n",
      "Time to play 5500 iterations: 0:08:49.406136\n",
      "Average reward over 10 trials: 243.200000 (+- 38.584453)\n",
      "Time to play 5750 iterations: 0:09:14.990328\n",
      "Average reward over 10 trials: 222.400000 (+- 44.421166)\n",
      "Time to play 6000 iterations: 0:09:39.500849\n",
      "Average reward over 10 trials: 299.900000 (+- 115.822666)\n",
      "Time to play 6250 iterations: 0:10:03.274672\n",
      "Average reward over 10 trials: 258.900000 (+- 44.787163)\n",
      "Time to play 6500 iterations: 0:10:26.584129\n",
      "Average reward over 10 trials: 199.900000 (+- 106.696251)\n",
      "Time to play 6750 iterations: 0:10:51.460372\n",
      "Average reward over 10 trials: 214.500000 (+- 63.449586)\n",
      "Time to play 7000 iterations: 0:11:14.655192\n",
      "Average reward over 10 trials: 306.500000 (+- 99.036609)\n",
      "Time to play 7250 iterations: 0:11:38.599575\n",
      "Average reward over 10 trials: 303.300000 (+- 212.130172)\n",
      "Time to play 7500 iterations: 0:12:03.188207\n",
      "Saved model after 7500 iterations.\n",
      "Average reward over 10 trials: 256.500000 (+- 63.619572)\n",
      "Time to play 7750 iterations: 0:12:28.465356\n",
      "Average reward over 10 trials: 346.100000 (+- 142.342861)\n",
      "Time to play 8000 iterations: 0:12:55.143046\n",
      "Average reward over 10 trials: 201.200000 (+- 74.512818)\n",
      "Time to play 8250 iterations: 0:13:19.751778\n",
      "Average reward over 10 trials: 332.300000 (+- 127.022872)\n",
      "Time to play 8500 iterations: 0:13:44.484682\n",
      "Average reward over 10 trials: 305.900000 (+- 53.262463)\n",
      "Time to play 8750 iterations: 0:14:10.453397\n",
      "Average reward over 10 trials: 224.100000 (+- 33.628708)\n",
      "Time to play 9000 iterations: 0:14:34.064323\n",
      "Average reward over 10 trials: 311.200000 (+- 184.011847)\n",
      "Time to play 9250 iterations: 0:14:58.038892\n",
      "Average reward over 10 trials: 183.700000 (+- 65.487480)\n",
      "Time to play 9500 iterations: 0:15:20.984943\n",
      "Average reward over 10 trials: 243.300000 (+- 73.507891)\n",
      "Time to play 9750 iterations: 0:15:44.793948\n",
      "Average reward over 10 trials: 388.300000 (+- 231.080960)\n",
      "Time to play 10000 iterations: 0:16:09.547891\n",
      "Saved model after 10000 iterations.\n",
      "Average reward over 10 trials: 612.600000 (+- 277.725476)\n",
      "Time to play 10250 iterations: 0:16:37.042440\n",
      "Average reward over 10 trials: 330.400000 (+- 142.698423)\n",
      "Time to play 10500 iterations: 0:17:01.114326\n",
      "Average reward over 10 trials: 336.000000 (+- 161.862905)\n",
      "Time to play 10750 iterations: 0:17:25.928738\n",
      "Average reward over 10 trials: 629.200000 (+- 299.541249)\n",
      "Time to play 11000 iterations: 0:17:52.083589\n",
      "Average reward over 10 trials: 277.800000 (+- 73.492585)\n",
      "Time to play 11250 iterations: 0:18:16.649417\n",
      "Average reward over 10 trials: 269.900000 (+- 88.810416)\n",
      "Time to play 11500 iterations: 0:18:41.266559\n",
      "Average reward over 10 trials: 226.100000 (+- 67.818065)\n",
      "Time to play 11750 iterations: 0:19:04.770323\n",
      "Average reward over 10 trials: 515.900000 (+- 211.105874)\n",
      "Time to play 12000 iterations: 0:19:29.903754\n",
      "Average reward over 10 trials: 558.800000 (+- 135.968232)\n",
      "Time to play 12250 iterations: 0:19:55.726769\n",
      "Average reward over 10 trials: 206.500000 (+- 67.956236)\n",
      "Time to play 12500 iterations: 0:20:20.460601\n",
      "Saved model after 12500 iterations.\n",
      "Average reward over 10 trials: 223.000000 (+- 104.266965)\n",
      "Time to play 12750 iterations: 0:20:44.127155\n",
      "Average reward over 10 trials: 403.100000 (+- 198.962032)\n",
      "Time to play 13000 iterations: 0:21:10.691031\n",
      "Average reward over 10 trials: 1086.300000 (+- 792.212856)\n",
      "Time to play 13250 iterations: 0:21:39.228873\n",
      "Average reward over 10 trials: 1377.100000 (+- 754.738690)\n",
      "Time to play 13500 iterations: 0:22:10.392675\n",
      "Average reward over 10 trials: 185.900000 (+- 79.994312)\n",
      "Time to play 13750 iterations: 0:22:35.607399\n",
      "Average reward over 10 trials: 330.500000 (+- 141.996655)\n",
      "Time to play 14000 iterations: 0:22:59.697076\n",
      "Average reward over 10 trials: 496.700000 (+- 171.688118)\n",
      "Time to play 14250 iterations: 0:23:24.829076\n",
      "Average reward over 10 trials: 1123.700000 (+- 720.576027)\n",
      "Time to play 14500 iterations: 0:23:54.370360\n",
      "Average reward over 10 trials: 677.300000 (+- 318.576537)\n",
      "Time to play 14750 iterations: 0:24:21.159694\n",
      "Average reward over 10 trials: 375.900000 (+- 244.091151)\n",
      "Time to play 15000 iterations: 0:24:46.234378\n",
      "Saved model after 15000 iterations.\n",
      "Average reward over 10 trials: 437.100000 (+- 261.352042)\n",
      "Time to play 15250 iterations: 0:25:11.427439\n",
      "Average reward over 10 trials: 227.000000 (+- 85.379154)\n",
      "Time to play 15500 iterations: 0:25:35.263842\n",
      "Average reward over 10 trials: 383.800000 (+- 223.226701)\n",
      "Time to play 15750 iterations: 0:25:59.876509\n",
      "Average reward over 10 trials: 412.400000 (+- 218.748806)\n",
      "Time to play 16000 iterations: 0:26:24.422515\n",
      "Average reward over 10 trials: 238.000000 (+- 86.294843)\n",
      "Time to play 16250 iterations: 0:26:47.736640\n",
      "Average reward over 10 trials: 1522.200000 (+- 907.761841)\n",
      "Time to play 16500 iterations: 0:27:19.764881\n",
      "Average reward over 10 trials: 673.000000 (+- 430.324993)\n",
      "Time to play 16750 iterations: 0:27:45.977151\n",
      "Average reward over 10 trials: 1519.700000 (+- 807.923641)\n",
      "Time to play 17000 iterations: 0:28:17.318630\n",
      "Average reward over 10 trials: 249.600000 (+- 108.365308)\n",
      "Time to play 17250 iterations: 0:28:41.144719\n",
      "Average reward over 10 trials: 285.600000 (+- 106.123701)\n",
      "Time to play 17500 iterations: 0:29:05.678656\n",
      "Saved model after 17500 iterations.\n",
      "Average reward over 10 trials: 533.300000 (+- 181.171217)\n",
      "Time to play 17750 iterations: 0:29:31.286221\n",
      "Average reward over 10 trials: 499.300000 (+- 305.766594)\n",
      "Time to play 18000 iterations: 0:29:56.750659\n",
      "Average reward over 10 trials: 762.300000 (+- 670.107909)\n",
      "Time to play 18250 iterations: 0:30:23.656871\n",
      "Average reward over 10 trials: 249.000000 (+- 76.604177)\n",
      "Time to play 18500 iterations: 0:30:47.432863\n",
      "Average reward over 10 trials: 200.500000 (+- 59.846888)\n",
      "Time to play 18750 iterations: 0:31:10.963648\n",
      "Average reward over 10 trials: 480.900000 (+- 314.232860)\n",
      "Time to play 19000 iterations: 0:31:36.192075\n",
      "Average reward over 10 trials: 284.100000 (+- 140.542129)\n",
      "Time to play 19250 iterations: 0:32:00.254889\n",
      "Average reward over 10 trials: 356.700000 (+- 167.413888)\n",
      "Time to play 19500 iterations: 0:32:24.764572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward over 10 trials: 412.000000 (+- 285.918170)\n",
      "Time to play 19750 iterations: 0:32:50.773338\n",
      "Average reward over 10 trials: 301.700000 (+- 122.293949)\n",
      "Time to play 20000 iterations: 0:33:14.998080\n",
      "Saved model after 20000 iterations.\n",
      "Average reward over 10 trials: 336.200000 (+- 182.283186)\n",
      "Time to play 20250 iterations: 0:33:39.057260\n",
      "Average reward over 10 trials: 315.900000 (+- 139.325841)\n",
      "Time to play 20500 iterations: 0:34:03.206326\n",
      "Average reward over 10 trials: 298.800000 (+- 105.363941)\n",
      "Time to play 20750 iterations: 0:34:27.930250\n",
      "Average reward over 10 trials: 265.300000 (+- 80.992654)\n",
      "Time to play 21000 iterations: 0:34:54.310464\n",
      "Average reward over 10 trials: 473.800000 (+- 211.576842)\n",
      "Time to play 21250 iterations: 0:35:19.599437\n",
      "Average reward over 10 trials: 819.500000 (+- 398.514303)\n",
      "Time to play 21500 iterations: 0:35:47.082050\n",
      "Average reward over 10 trials: 460.900000 (+- 246.737289)\n",
      "Time to play 21750 iterations: 0:36:11.938902\n",
      "Average reward over 10 trials: 566.400000 (+- 236.011525)\n",
      "Time to play 22000 iterations: 0:36:37.995204\n",
      "Average reward over 10 trials: 334.200000 (+- 190.064621)\n",
      "Time to play 22250 iterations: 0:37:02.596474\n",
      "Average reward over 10 trials: 464.100000 (+- 292.616285)\n",
      "Time to play 22500 iterations: 0:37:27.902373\n",
      "Saved model after 22500 iterations.\n",
      "Average reward over 10 trials: 799.000000 (+- 629.075353)\n",
      "Time to play 22750 iterations: 0:37:55.165084\n",
      "Average reward over 10 trials: 469.200000 (+- 181.423152)\n",
      "Time to play 23000 iterations: 0:38:20.673509\n",
      "Average reward over 10 trials: 467.700000 (+- 340.492599)\n",
      "Time to play 23250 iterations: 0:38:46.155792\n",
      "Average reward over 10 trials: 653.800000 (+- 568.142553)\n",
      "Time to play 23500 iterations: 0:39:13.336381\n",
      "Average reward over 10 trials: 515.700000 (+- 323.207379)\n",
      "Time to play 23750 iterations: 0:39:38.897678\n",
      "Average reward over 10 trials: 394.400000 (+- 154.437172)\n",
      "Time to play 24000 iterations: 0:40:03.926599\n",
      "Average reward over 10 trials: 274.100000 (+- 102.652277)\n",
      "Time to play 24250 iterations: 0:40:28.532889\n",
      "Average reward over 10 trials: 353.200000 (+- 239.958246)\n",
      "Time to play 24500 iterations: 0:40:52.454415\n",
      "Average reward over 10 trials: 302.500000 (+- 122.306378)\n",
      "Time to play 24750 iterations: 0:41:16.231094\n",
      "Average reward over 10 trials: 320.100000 (+- 174.568296)\n",
      "Time to play 25000 iterations: 0:41:40.061390\n",
      "Saved model after 25000 iterations.\n",
      "Average reward over 10 trials: 344.600000 (+- 141.682885)\n",
      "Time to play 25250 iterations: 0:42:03.883482\n",
      "Average reward over 10 trials: 457.700000 (+- 177.525238)\n",
      "Time to play 25500 iterations: 0:42:28.321228\n",
      "Average reward over 10 trials: 584.900000 (+- 559.578136)\n",
      "Time to play 25750 iterations: 0:42:54.412195\n",
      "Average reward over 10 trials: 390.700000 (+- 179.976693)\n",
      "Time to play 26000 iterations: 0:43:18.757175\n",
      "Average reward over 10 trials: 233.500000 (+- 135.660053)\n",
      "Time to play 26250 iterations: 0:43:41.794441\n",
      "Average reward over 10 trials: 290.200000 (+- 75.594709)\n",
      "Time to play 26500 iterations: 0:44:05.248647\n",
      "Average reward over 10 trials: 330.600000 (+- 119.971830)\n",
      "Time to play 26750 iterations: 0:44:29.089566\n",
      "Average reward over 10 trials: 280.200000 (+- 146.215457)\n",
      "Time to play 27000 iterations: 0:44:52.521096\n",
      "Average reward over 10 trials: 445.500000 (+- 341.887481)\n",
      "Time to play 27250 iterations: 0:45:17.403929\n",
      "Average reward over 10 trials: 416.600000 (+- 219.950995)\n",
      "Time to play 27500 iterations: 0:45:41.577513\n",
      "Saved model after 27500 iterations.\n",
      "Average reward over 10 trials: 301.200000 (+- 130.796636)\n",
      "Time to play 27750 iterations: 0:46:05.238218\n",
      "Average reward over 10 trials: 191.400000 (+- 53.656686)\n",
      "Time to play 28000 iterations: 0:46:28.114853\n",
      "Average reward over 10 trials: 374.700000 (+- 318.745055)\n",
      "Time to play 28250 iterations: 0:46:52.708417\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-120-c56b82ffb3d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m     actrainer.play_and_learn(n_iterations, steps_per_iteration,\n\u001b[1;32m     17\u001b[0m                              \u001b[0mevaluate_every\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_every\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                              ckpt_file)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-113-e9e9b658f3cf>\u001b[0m in \u001b[0;36mplay_and_learn\u001b[0;34m(self, n_iterations, steps_per_iteration, evaluate_every, save_every, ckpt_file)\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps_per_iteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose_actions_and_get_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                 \u001b[0mnext_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvec_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                 \u001b[0mclipped_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-3dcfb0764467>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mstep_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_remotes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mstacked_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mstep_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-3dcfb0764467>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mstep_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_remotes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mstacked_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mstep_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/multiprocessing/connection.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetbuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m         \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"!i\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmaxsize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mremaining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    acnet = ActorCriticGraphVectors(state_size, nb_actions, learning_rate, max_grad_norm, \"ACNet\")  # TODO Put lr and grad norm in trainer\n",
    "\n",
    "    actrainer = ActorCriticTrainer(acnet,\n",
    "                                   vec_env=vec_env, nb_env=nb_env,\n",
    "                                   eval_env=eval_env,\n",
    "                                   gamma=gamma,\n",
    "                                   state_size=state_size,\n",
    "                                   ckpt_file=ckpt_file,\n",
    "                                   session=sess)\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #actrainer.restore(sess, ckpt=\"./models/model_pong.ckpt\")\n",
    "    actrainer.play_and_learn(n_iterations, steps_per_iteration,\n",
    "                             evaluate_every, save_every,\n",
    "                             ckpt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/model_cartpole.ckpt\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    acnet = ActorCriticGraphVectors(state_size, nb_actions, learning_rate, max_grad_norm, \"ACNet\")  # TODO Put lr and grad norm in trainer\n",
    "\n",
    "    actrainer = ActorCriticTrainer(acnet,\n",
    "                                   vec_env=vec_env, nb_env=nb_env,\n",
    "                                   eval_env=eval_env,\n",
    "                                   gamma=gamma,\n",
    "                                   state_size=state_size,\n",
    "                                   ckpt_file=ckpt_file,\n",
    "                                   session=sess)\n",
    "    \n",
    "    actrainer.restore()\n",
    "    actrainer.run_episode(eval_env, 10000, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_env.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
