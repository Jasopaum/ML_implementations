{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from skimage import transform\n",
    "from skimage.color import rgb2gray\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "from multiprocessing import Process, Pipe\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_screen(screen, sz_to_process):\n",
    "    gray = rgb2gray(screen)\n",
    "    cropped_screen = gray[8:-12,5:-12]  # For Space Invaders, TODO make it cleaner\n",
    "    \n",
    "    preprocessed_screen = transform.resize(cropped_screen, sz_to_process, mode='constant', anti_aliasing=True)\n",
    "    \n",
    "    return preprocessed_screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_frames(stacked_frames, last_2_screens, is_new_episode):\n",
    "    assert isinstance(stacked_frames, deque), \"stacked_frames has not type deque\"\n",
    "    sz_to_process = stacked_frames[0].shape\n",
    "    \n",
    "    max_screen = np.maximum(last_2_screens[0], last_2_screens[1])\n",
    "\n",
    "    frame = preprocess_screen(max_screen, sz_to_process)\n",
    "    \n",
    "    if is_new_episode:\n",
    "        for _ in range(len(stacked_frames)):\n",
    "            stacked_frames.append(frame)\n",
    "    else:\n",
    "        stacked_frames.append(frame)\n",
    "        \n",
    "    input_state = np.stack(stacked_frames, axis=2)\n",
    "                    \n",
    "    return input_state, stacked_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_target_values(batch_rewards, next_estimated_values, batch_dones, gamma):\n",
    "    nb_seq, len_seq = batch_rewards.shape\n",
    "    batch_target_values = np.zeros_like(batch_rewards, dtype=np.float)\n",
    "    cums = next_estimated_values\n",
    "        \n",
    "    for i in range(len_seq-1, -1, -1):\n",
    "        cums = np.where(batch_dones[:, i], batch_rewards[:, i], gamma * cums + batch_rewards[:, i])\n",
    "        \n",
    "        batch_target_values[:, i] = cums\n",
    "        \n",
    "    return batch_target_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvWrapper:\n",
    "    # Do not have to handle stacked frames externally\n",
    "    \n",
    "    def __init__(self, game_name, state_size, frame_skipping):\n",
    "        self.env = gym.make(game_name)\n",
    "        self.stacked_frames = deque([np.zeros(state_size[:-1]) for _ in range(state_size[-1])],\n",
    "                                    maxlen=state_size[-1])\n",
    "        self.nb_actions = self.env.action_space.n\n",
    "        self.frame_skipping = frame_skipping\n",
    "        self.last_2_screens = deque(maxlen=2)\n",
    "        \n",
    "    def step(self, action, render=False):\n",
    "        for _ in range(self.frame_skipping):\n",
    "            screen, reward, done, info = self.env.step(action)\n",
    "            self.last_2_screens.append(screen)\n",
    "            if render:\n",
    "                self.render()\n",
    "            if done:\n",
    "                break\n",
    "        if done:\n",
    "            # Reset env and state\n",
    "            screen = self.env.reset()\n",
    "            for _ in range(2):\n",
    "                self.last_2_screens.append(screen)\n",
    "            stacked_state, self.stacked_frames = stack_frames(self.stacked_frames, self.last_2_screens, True)\n",
    "        else:\n",
    "            stacked_state, self.stacked_frames = stack_frames(self.stacked_frames, self.last_2_screens, False)\n",
    "\n",
    "        return stacked_state, reward, done, info\n",
    "            \n",
    "    def reset(self):\n",
    "        screen = self.env.reset()\n",
    "        for _ in range(2):\n",
    "            self.last_2_screens.append(screen)\n",
    "        stacked_state, self.stacked_frames = stack_frames(self.stacked_frames, self.last_2_screens, True)\n",
    "        \n",
    "        return stacked_state\n",
    "        \n",
    "    def render(self):\n",
    "        self.env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker(env_remote, worker_remote, env):\n",
    "\n",
    "    while True:\n",
    "        cmd, data = worker_remote.recv()\n",
    "\n",
    "        if cmd == 'step':\n",
    "            stacked_input, reward, done, info = env.step(data)\n",
    "            worker_remote.send((stacked_input, reward, done, info))\n",
    "\n",
    "        elif cmd == 'reset':\n",
    "            stacked_input = env.reset()\n",
    "            worker_remote.send(stacked_input)\n",
    "            \n",
    "        elif cmd == 'close':\n",
    "            env_remote.close()\n",
    "            worker_remote.close()\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError         \n",
    "\n",
    "class VecEnvWrapper:\n",
    "    def __init__(self, env_list):\n",
    "        self.env_remotes, self.worker_remotes = zip(*[Pipe() for _ in range(len(env_list))])\n",
    "        self.processes = [Process(target=worker, args=(e_remote, w_remote, env))\n",
    "                            for (e_remote, w_remote, env) in zip(self.env_remotes, self.worker_remotes, env_list)]\n",
    "\n",
    "        for p in self.processes:\n",
    "            p.daemon = True\n",
    "            p.start()\n",
    "\n",
    "    def step(self, actions):\n",
    "        for r, a in zip(self.env_remotes, actions):\n",
    "            r.send(('step', a))\n",
    "\n",
    "        step_outputs = [r.recv() for r in self.env_remotes]\n",
    "\n",
    "        stacked_inputs, rewards, dones, infos = zip(*step_outputs)\n",
    "\n",
    "        return stacked_inputs, rewards, dones, infos\n",
    "\n",
    "    def reset(self):\n",
    "        for r in self.env_remotes:\n",
    "            r.send(('reset', None))\n",
    "        return [r.recv() for r in self.env_remotes]\n",
    "    \n",
    "    def close(self):\n",
    "        for r in self.env_remotes:\n",
    "            r.send(('close', None))\n",
    "        for er, wr in zip(self.env_remotes, self.worker_remotes):\n",
    "            er.close()\n",
    "            wr.close()\n",
    "        for p in self.processes:\n",
    "            p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticGraph:\n",
    "    \n",
    "    def __init__(self, state_size, nb_actions, learning_rate, scope_name):\n",
    "        self.state_size = state_size\n",
    "        self.nb_actions = nb_actions\n",
    "        self.learning_rate = learning_rate\n",
    "        self.scope_name = scope_name\n",
    "        \n",
    "        with tf.variable_scope(self.scope_name):\n",
    "            self.state = tf.placeholder(tf.float32, (None, *(self.state_size)), name=\"state\")\n",
    "            self.action = tf.placeholder(tf.uint8, (None,), name=\"action\")\n",
    "            self.action_OH = tf.one_hot(self.action, self.nb_actions, name=\"action_OH\")\n",
    "\n",
    " \n",
    "            self.target_value = tf.placeholder(tf.float32, (None,), name=\"target_value\")\n",
    "            self.advantage = tf.placeholder(tf.float32, (None,), name=\"advantage\")\n",
    "                        \n",
    "            # Neural net\n",
    "            self.conv1 = tf.layers.conv2d(inputs=self.state,\n",
    "                                          filters=32,\n",
    "                                          kernel_size=(8,8),\n",
    "                                          strides=(4,4),\n",
    "                                          padding=\"valid\",\n",
    "                                          activation=tf.nn.relu,\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                          name=\"conv1\")\n",
    "            \n",
    "            self.conv2 = tf.layers.conv2d(inputs=self.conv1,\n",
    "                                          filters=64,\n",
    "                                          kernel_size=(4,4),\n",
    "                                          strides=(2,2),\n",
    "                                          padding=\"valid\",\n",
    "                                          activation=tf.nn.relu,\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                          name=\"conv2\")\n",
    "            \n",
    "            self.conv3 = tf.layers.conv2d(inputs=self.conv2,\n",
    "                                          filters=64,\n",
    "                                          kernel_size=(3,3),\n",
    "                                          strides=(2,2),\n",
    "                                          padding=\"valid\",\n",
    "                                          activation=tf.nn.relu,\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                          name=\"conv3\")\n",
    "            \n",
    "            self.flatten = tf.layers.flatten(self.conv3)\n",
    "            \n",
    "            # Actor part\n",
    "            self.fc_actions = tf.layers.dense(self.flatten,\n",
    "                                              units=512,\n",
    "                                              activation=tf.nn.relu,\n",
    "                                              kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                              name=\"fc_action\")\n",
    "            \n",
    "            self.prob_actions = tf.layers.dense(self.fc_actions,\n",
    "                                                units=self.nb_actions,\n",
    "                                                activation=tf.nn.softmax,\n",
    "                                                kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                                name=\"action_distribution\")\n",
    "            \n",
    "            # Critic part\n",
    "            self.fc_value = tf.layers.dense(self.flatten,\n",
    "                                            units=512,\n",
    "                                            activation=tf.nn.relu,\n",
    "                                            kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                            name=\"fc_value\")\n",
    "            \n",
    "            self.value = tf.layers.dense(self.fc_value,\n",
    "                                         units=1,\n",
    "                                         activation=None,\n",
    "                                         kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                         name=\"value\")\n",
    "            \n",
    "            # Losses\n",
    "            # Actor loss\n",
    "            self.log_prob_actions = tf.math.log(self.prob_actions)\n",
    "            \n",
    "            self.log_prob_chosen_action = tf.reduce_sum(self.log_prob_actions * self.action_OH, axis=1)\n",
    "            self.actor_loss = - tf.reduce_mean(self.log_prob_chosen_action * self.advantage)\n",
    "            \n",
    "            # Critic loss\n",
    "            self.critic_loss = tf.reduce_mean(tf.square(self.target_value - self.value))\n",
    "\n",
    "            # Entropy: sum(p(x) * -log(p(x)))\n",
    "            self.entropy = tf.reduce_sum(tf.multiply(self.prob_actions, - self.log_prob_actions))\n",
    "\n",
    "            # TODO put coeffs as parameters\n",
    "            self.loss = 0.5 * self.critic_loss + self.actor_loss - 0.01 * self.entropy\n",
    "\n",
    "            self.optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "            self.train_op = self.optimizer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticTrainer:\n",
    "    def __init__(self, graph, vec_env, nb_env, eval_env, gamma, state_size, ckpt_file):\n",
    "        self.graph = graph\n",
    "        self.vec_env = vec_env\n",
    "        self.nb_env = nb_env\n",
    "        self.eval_env = eval_env\n",
    "        self.nb_actions = self.eval_env.nb_actions\n",
    "        self.gamma = gamma\n",
    "        self.state_size = state_size\n",
    "        self.ckpt_file = ckpt_file\n",
    "        self.saver = tf.train.Saver()\n",
    "\n",
    "    def choose_actions(self, input_states, session):\n",
    "        prob_actions = session.run(self.graph.prob_actions,\n",
    "                                    feed_dict={self.graph.state: input_states})\n",
    "\n",
    "        actions = [np.random.choice(np.arange(self.nb_actions), p=p) for p in prob_actions]\n",
    "\n",
    "        return actions\n",
    "    \n",
    "    def get_values(self, input_states, session):\n",
    "        values = session.run(self.graph.value,\n",
    "                             feed_dict={self.graph.state: input_states})\n",
    "\n",
    "        return values.flatten()\n",
    "        \n",
    "    def choose_actions_and_get_values(self, input_states, session):\n",
    "        prob_actions, values = session.run([self.graph.prob_actions, self.graph.value],\n",
    "                                           feed_dict={self.graph.state: input_states})\n",
    "\n",
    "        actions = [np.random.choice(np.arange(self.nb_actions), p=p) for p in prob_actions]\n",
    "\n",
    "        return actions, values.flatten()\n",
    "        \n",
    "    def run_episode(self, env_to_run, max_step, render, session):\n",
    "\n",
    "        # Reset state\n",
    "        state = env_to_run.reset()        \n",
    "        total_reward = 0\n",
    "        \n",
    "        for step in range(max_step):\n",
    "            state = np.expand_dims(state, axis=0)\n",
    "            action = self.choose_actions(state, session)\n",
    "\n",
    "            # Apply action to env and get next state, reward, and done bool\n",
    "            state, reward, done, _ = env_to_run.step(action, render)\n",
    "            total_reward += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        print(\"Reward on episode: %f\" % total_reward)\n",
    "        \n",
    "    def train_on_batch(self, states, actions, target_values, advantages, session):\n",
    "        loss, _ = session.run([self.graph.loss, self.graph.train_op],\n",
    "                              feed_dict={self.graph.state: states,\n",
    "                                         self.graph.action: actions,\n",
    "                                         self.graph.target_value: target_values,\n",
    "                                         self.graph.advantage: advantages})\n",
    "        \n",
    "        return loss\n",
    "         \n",
    "    def play_and_learn(self, n_iterations, steps_per_iteration,\n",
    "                       evaluate_every, save_every,\n",
    "                       ckpt_file, session):\n",
    "\n",
    "        # Reset states\n",
    "        states = self.vec_env.reset()       \n",
    "\n",
    "        states_all_env = np.empty((self.nb_env, steps_per_iteration, *self.state_size), dtype=np.float)\n",
    "        actions_all_env = np.empty((self.nb_env, steps_per_iteration), dtype=np.int)\n",
    "        rewards_all_env = np.empty((self.nb_env, steps_per_iteration), dtype=np.float)\n",
    "        values_all_env = np.empty((self.nb_env, steps_per_iteration), dtype=np.float)\n",
    "        dones_all_env = np.empty((self.nb_env, steps_per_iteration), dtype=np.bool)\n",
    "        \n",
    "        T = time.time()\n",
    "        for iteration in range(1, n_iterations):\n",
    "\n",
    "            for step in range(steps_per_iteration):\n",
    "                actions, values = self.choose_actions_and_get_values(states, session)\n",
    "                next_states, rewards, dones, _ = self.vec_env.step(actions)\n",
    "                \n",
    "                clipped_rewards = np.clip(rewards, -1, 1)\n",
    "                                \n",
    "                states_all_env[:, step] = states\n",
    "                actions_all_env[:, step] = actions\n",
    "                rewards_all_env[:, step] = clipped_rewards\n",
    "                values_all_env[:, step] = values\n",
    "                dones_all_env[:, step] = dones\n",
    "                \n",
    "                states = next_states\n",
    "                    \n",
    "            # Estimated values for the future\n",
    "            next_estimated_values = self.get_values(states, session)\n",
    "            next_estimated_values = np.where(dones, 0, next_estimated_values)\n",
    "\n",
    "            target_values_all_env = compute_target_values(np.array(rewards_all_env),\n",
    "                                                          next_estimated_values,\n",
    "                                                          np.array(dones_all_env),\n",
    "                                                          self.gamma)\n",
    "            \n",
    "            advantages_all_env = target_values_all_env - values_all_env\n",
    "            \n",
    "            # Concatenate the experiences from the different envionments\n",
    "            batch_states = np.concatenate(states_all_env)\n",
    "            batch_actions = np.concatenate(actions_all_env)\n",
    "            batch_target_values = np.concatenate(target_values_all_env)\n",
    "            batch_advantages = np.concatenate(advantages_all_env)\n",
    "\n",
    "            loss = self.train_on_batch(batch_states, batch_actions, batch_target_values, batch_advantages, session)\n",
    "                \n",
    "            if iteration % evaluate_every == 0:\n",
    "                self.run_episode(self.eval_env, 10000, False, session)\n",
    "                print(\"Time to play %i iterations: %f\" %(iteration, time.time() - T))\n",
    "                \n",
    "            if iteration % save_every == 0:\n",
    "                self.saver.save(session, self.ckpt_file)\n",
    "                print(\"Saved model after %i iterations.\" % iteration)\n",
    "                \n",
    "    def restore(self, session):\n",
    "        self.saver.restore(session, self.ckpt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vec_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz_to_process = (110,84)\n",
    "stack_size = 4\n",
    "state_size = (*sz_to_process, stack_size)\n",
    "\n",
    "frame_skipping = 2\n",
    "\n",
    "# Create the environments\n",
    "nb_env = 8\n",
    "env_list = [EnvWrapper('SpaceInvaders-v0', state_size, frame_skipping) for _ in range(nb_env)]\n",
    "eval_env = EnvWrapper('SpaceInvaders-v0', state_size, frame_skipping)\n",
    "\n",
    "vec_env = VecEnvWrapper(env_list)\n",
    "\n",
    "nb_actions = eval_env.nb_actions\n",
    "\n",
    "gamma = 0.95\n",
    "\n",
    "n_iterations = int(5e6)\n",
    "steps_per_iteration = 4\n",
    "learning_rate = 0.00025\n",
    "\n",
    "evaluate_every = 100\n",
    "save_every = 20000\n",
    "\n",
    "ckpt_file = \"./models/model.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    acnet = ActorCriticGraph(state_size, nb_actions, learning_rate, \"ACNet\")\n",
    "    \n",
    "    actrainer = ActorCriticTrainer(acnet,\n",
    "                                   vec_env=vec_env, nb_env=nb_env,\n",
    "                                   eval_env=eval_env,\n",
    "                                   gamma=gamma,\n",
    "                                   state_size=state_size,\n",
    "                                   ckpt_file=ckpt_file)\n",
    "\n",
    "    # Setup TensorBoard\n",
    "    #writer = tf.summary.FileWriter(\"./tensorboard/\", sess.graph)\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #actrainer.restore(sess)\n",
    "    actrainer.play_and_learn(n_iterations, steps_per_iteration,\n",
    "                             evaluate_every, save_every,\n",
    "                             ckpt_file, sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    acnet = ActorCriticGraph(state_size, nb_actions, learning_rate, \"ACNet\")\n",
    "    \n",
    "    actrainer = ActorCriticTrainer(acnet,\n",
    "                                   env=env,\n",
    "                                   eval_env=eval_env,\n",
    "                                   gamma=gamma,\n",
    "                                   state_size=state_size,\n",
    "                                   ckpt_file=ckpt_file)\n",
    "    \n",
    "    actrainer.restore(sess)\n",
    "    actrainer.run_episode(eval_env, 10000, True, sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
