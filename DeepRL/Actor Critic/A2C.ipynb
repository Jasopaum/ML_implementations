{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from skimage import transform\n",
    "from skimage.color import rgb2gray\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "from multiprocessing import Process, Pipe\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_screen(screen, sz_to_process):\n",
    "    print(screen.shape)\n",
    "    print(screen)\n",
    "    gray = rgb2gray(screen)\n",
    "    #cropped_screen = gray[8:-12,5:-12]  # For Space Invaders, TODO make it cleaner\n",
    "    cropped_screen = gray[25:-10,5:-5]  # For Pong, TODO make it cleaner\n",
    "    preprocessed_screen = transform.resize(cropped_screen, sz_to_process, mode='constant', anti_aliasing=True)\n",
    "    \n",
    "    return preprocessed_screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_observations(stacked_observations, observation, is_new_episode):\n",
    "    assert isinstance(stacked_observations, deque), \"stacked_observations has not type deque\"\n",
    "    sz_to_process = stacked_observations[0].shape  #TODO make it cleaner\n",
    "        \n",
    "    if is_new_episode:\n",
    "        for _ in range(len(stacked_observations)):\n",
    "            stacked_observations.append(observation)\n",
    "    else:\n",
    "        stacked_observations.append(observation)\n",
    "    \n",
    "    last_axis = len(stacked_observations[0].shape)\n",
    "    state = np.stack(stacked_observations, axis=last_axis)\n",
    "                    \n",
    "    return state, stacked_observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_target_values(batch_rewards, next_estimated_values, batch_dones, gamma):\n",
    "    nb_seq, len_seq = batch_rewards.shape\n",
    "    batch_target_values = np.zeros_like(batch_rewards, dtype=np.float)\n",
    "    cums = next_estimated_values\n",
    "        \n",
    "    for i in range(len_seq-1, -1, -1):\n",
    "        cums = np.where(batch_dones[:, i], batch_rewards[:, i], gamma * cums + batch_rewards[:, i])\n",
    "        \n",
    "        batch_target_values[:, i] = cums\n",
    "        \n",
    "    return batch_target_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvWrapper:\n",
    "    # Do not have to handle stacked frames externally\n",
    "    \n",
    "    def __init__(self, game_name, state_size):\n",
    "        self.env = gym.make(game_name)\n",
    "        self.stacked_observations = deque([np.zeros(state_size[:-1]) for _ in range(state_size[-1])],\n",
    "                                    maxlen=state_size[-1])\n",
    "        self.nb_actions = self.env.action_space.n\n",
    "        self.observation_shape = self.env.observation_space.shape\n",
    "        self.obs_is_image = self.observation_shape == (210, 160, 3)\n",
    "        \n",
    "    def step(self, action, render=False):\n",
    "        observation, reward, done, info = self.env.step(action)\n",
    "        if self.obs_is_image:\n",
    "            observation = preprocess_screen(observation, sz_to_process)\n",
    "        if render:\n",
    "            self.render()\n",
    "            \n",
    "        if done:\n",
    "            # Reset env and state\n",
    "            observation = self.env.reset()\n",
    "            if self.obs_is_image:\n",
    "                observation = preprocess_screen(observation, sz_to_process)\n",
    "\n",
    "            stacked_state, self.stacked_observations = stack_observations(self.stacked_observations, observation, True)\n",
    "        else:\n",
    "            stacked_state, self.stacked_observations = stack_observations(self.stacked_observations, observation, False)\n",
    "\n",
    "        return stacked_state, reward, done, info\n",
    "            \n",
    "    def reset(self):\n",
    "        observation = self.env.reset()\n",
    "        if self.obs_is_image:\n",
    "            observation = preprocess_screen(observation, sz_to_process)\n",
    "\n",
    "        stacked_state, self.stacked_observations = stack_observations(self.stacked_observations, observation, True)\n",
    "        \n",
    "        return stacked_state\n",
    "        \n",
    "    def render(self):\n",
    "        self.env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker(env_remote, worker_remote, env):\n",
    "    env_remote.close()\n",
    "    while True:\n",
    "        cmd, data = worker_remote.recv()\n",
    "\n",
    "        if cmd == 'step':\n",
    "            stacked_input, reward, done, info = env.step(data)\n",
    "            worker_remote.send((stacked_input, reward, done, info))\n",
    "\n",
    "        elif cmd == 'reset':\n",
    "            stacked_input = env.reset()\n",
    "            worker_remote.send(stacked_input)\n",
    "            \n",
    "        elif cmd == 'close':\n",
    "            env_remote.close()\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError         \n",
    "\n",
    "class VecEnvWrapper:\n",
    "    def __init__(self, env_list):\n",
    "        self.env_remotes, self.worker_remotes = zip(*[Pipe() for _ in range(len(env_list))])\n",
    "        self.processes = [Process(target=worker, args=(e_remote, w_remote, env))\n",
    "                            for (e_remote, w_remote, env) in zip(self.env_remotes, self.worker_remotes, env_list)]\n",
    "\n",
    "        for p in self.processes:\n",
    "            p.daemon = True\n",
    "            p.start()\n",
    "        for wr in self.worker_remotes:\n",
    "            wr.close()\n",
    "            \n",
    "    def step(self, actions):   \n",
    "        for r, a in zip(self.env_remotes, actions):\n",
    "            r.send(('step', a))\n",
    "        \n",
    "        step_outputs = [r.recv() for r in self.env_remotes]\n",
    "\n",
    "        stacked_inputs, rewards, dones, infos = zip(*step_outputs)\n",
    "\n",
    "        return stacked_inputs, rewards, dones, infos\n",
    "\n",
    "    def reset(self):\n",
    "        for r in self.env_remotes:\n",
    "            r.send(('reset', None))\n",
    "        return [r.recv() for r in self.env_remotes]\n",
    "    \n",
    "    def close(self):\n",
    "        for r in self.env_remotes:\n",
    "            r.send(('close', None))\n",
    "        for r in self.env_remotes:\n",
    "            r.close()\n",
    "        for p in self.processes:\n",
    "            p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticGraphImages:\n",
    "    \n",
    "    def __init__(self, state_size, nb_actions, learning_rate, scope_name):\n",
    "        self.state_size = state_size\n",
    "        self.nb_actions = nb_actions\n",
    "        self.learning_rate = learning_rate\n",
    "        self.scope_name = scope_name\n",
    "        \n",
    "        with tf.variable_scope(self.scope_name):\n",
    "            self.state = tf.placeholder(tf.float32, (None, *(self.state_size)), name=\"state\")\n",
    "            self.action = tf.placeholder(tf.uint8, (None,), name=\"action\")\n",
    "            self.action_OH = tf.one_hot(self.action, self.nb_actions, name=\"action_OH\")\n",
    "\n",
    " \n",
    "            self.target_value = tf.placeholder(tf.float32, (None,), name=\"target_value\")\n",
    "            self.advantage = tf.placeholder(tf.float32, (None,), name=\"advantage\")\n",
    "        \n",
    "            initializer = tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "            device = '/device:GPU:0' if tf.test.is_gpu_available() else '/device:CPU:0'\n",
    "            with tf.device(device):\n",
    "                # Neural net\n",
    "                conv1_F = tf.Variable(initializer((7, 7, 4, 8)))\n",
    "                self.conv1 = tf.nn.conv2d(input=self.state,\n",
    "                                          filter=conv1_F,\n",
    "                                          strides=(1,4,4,1),\n",
    "                                          padding=\"VALID\",\n",
    "                                          name=\"conv1\")\n",
    "                self.conv1_act = tf.nn.relu(self.conv1)\n",
    "\n",
    "                conv2_F = tf.Variable(initializer((5, 5, 8, 16)))\n",
    "                self.conv2 = tf.nn.conv2d(input=self.conv1_act,\n",
    "                                          filter=conv2_F,\n",
    "                                          strides=(1,2,2,1),\n",
    "                                          padding=\"VALID\",\n",
    "                                          name=\"conv2\")\n",
    "                self.conv2_act = tf.nn.relu(self.conv2)\n",
    "            \n",
    "            \n",
    "                conv3_F = tf.Variable(initializer((3, 3, 16, 16)))\n",
    "                self.conv3 = tf.nn.conv2d(input=self.conv2_act,\n",
    "                                          filter=conv3_F,\n",
    "                                          strides=(1,2,2,1),\n",
    "                                          padding=\"VALID\",\n",
    "                                          name=\"conv3\")\n",
    "                self.conv3_act = tf.nn.relu(self.conv3)\n",
    "            \n",
    "                self.flatten = tf.keras.layers.Flatten()(self.conv3_act)\n",
    "\n",
    "                # Actor part\n",
    "                self.fc_actions = tf.keras.layers.Dense(units=512,\n",
    "                                                        activation=tf.nn.relu,\n",
    "                                                        kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                                        name=\"fc_action\")(self.flatten)\n",
    "\n",
    "                self.prob_actions = tf.keras.layers.Dense(units=self.nb_actions,\n",
    "                                                          activation=tf.nn.softmax,\n",
    "                                                          kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                                          name=\"action_distribution\")(self.fc_actions)\n",
    "\n",
    "                # Critic part\n",
    "                self.fc_value = tf.keras.layers.Dense(\n",
    "                                                units=512,\n",
    "                                                activation=tf.nn.relu,\n",
    "                                                kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                                name=\"fc_value\")(self.flatten)\n",
    "\n",
    "                self.value = tf.keras.layers.Dense(units=1,\n",
    "                                                   activation=None,\n",
    "                                                   kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                                   name=\"value\")(self.fc_value)\n",
    "            \n",
    "                # Losses\n",
    "                # Actor loss\n",
    "                self.log_prob_actions = tf.math.log(self.prob_actions)\n",
    "\n",
    "                self.log_prob_chosen_action = tf.reduce_sum(self.log_prob_actions * self.action_OH, axis=1)\n",
    "                self.actor_loss = - tf.reduce_mean(self.log_prob_chosen_action * self.advantage)\n",
    "\n",
    "                # Critic loss\n",
    "                self.critic_loss = tf.reduce_mean(tf.square(self.target_value - self.value))\n",
    "\n",
    "                # Entropy: sum(p(x) * -log(p(x)))\n",
    "                self.entropy = tf.reduce_sum(tf.multiply(self.prob_actions, - self.log_prob_actions))\n",
    "\n",
    "                # TODO put coeffs as parameters\n",
    "                self.loss = 0.5 * self.critic_loss + self.actor_loss - 0.01 * self.entropy\n",
    "\n",
    "                self.optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "                self.train_op = self.optimizer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticGraphVectors:\n",
    "    \n",
    "    def __init__(self, state_size, nb_actions, learning_rate, max_grad_norm=None, scope_name=\"ACNet\"):\n",
    "        self.state_size = state_size\n",
    "        self.nb_actions = nb_actions\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.scope_name = scope_name\n",
    "        \n",
    "        with tf.variable_scope(self.scope_name):\n",
    "            self.state = tf.placeholder(tf.float32, (None, *(self.state_size)), name=\"state\")\n",
    "            self.action = tf.placeholder(tf.uint8, (None,), name=\"action\")\n",
    "            self.action_OH = tf.one_hot(self.action, self.nb_actions, name=\"action_OH\")\n",
    "\n",
    " \n",
    "            self.target_value = tf.placeholder(tf.float32, (None,), name=\"target_value\")\n",
    "            self.advantage = tf.placeholder(tf.float32, (None,), name=\"advantage\")\n",
    "        \n",
    "            initializer = tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "            device = '/device:GPU:0' if tf.test.is_gpu_available() else '/device:CPU:0'\n",
    "            with tf.device(device):\n",
    "                # Neural net\n",
    "                self.flatten_state = tf.keras.layers.Flatten()(self.state)\n",
    "                \n",
    "                # Actor part\n",
    "                self.fc_actions = tf.keras.layers.Dense(units=10,\n",
    "                                                        activation=tf.nn.relu,\n",
    "                                                        kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                                        name=\"fc_action\")(self.flatten_state)\n",
    "\n",
    "                self.prob_actions = tf.keras.layers.Dense(units=self.nb_actions,\n",
    "                                                          activation=tf.nn.softmax,\n",
    "                                                          kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                                          name=\"action_distribution\")(self.fc_actions)\n",
    "\n",
    "                # Critic part\n",
    "                self.fc_value = tf.keras.layers.Dense(units=10,\n",
    "                                                      activation=tf.nn.relu,\n",
    "                                                      kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                                      name=\"fc_value\")(self.flatten_state)\n",
    "\n",
    "                self.value = tf.keras.layers.Dense(units=1,\n",
    "                                                   activation=None,\n",
    "                                                   kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                                   name=\"value\")(self.fc_value)\n",
    "            \n",
    "                # Losses\n",
    "                # Actor loss\n",
    "                self.log_prob_actions = tf.math.log(self.prob_actions)\n",
    "\n",
    "                self.log_prob_chosen_action = tf.reduce_sum(self.log_prob_actions * self.action_OH, axis=1)\n",
    "                self.actor_loss = - tf.reduce_mean(self.log_prob_chosen_action * self.advantage)\n",
    "\n",
    "                # Critic loss\n",
    "                self.critic_loss = tf.reduce_mean(tf.square(self.target_value - self.value))\n",
    "\n",
    "                # Entropy: sum(p(x) * -log(p(x)))\n",
    "                self.entropy = tf.reduce_sum(tf.multiply(self.prob_actions, - self.log_prob_actions))\n",
    "\n",
    "                # TODO put coeffs as parameters\n",
    "                self.loss = 0.5 * self.critic_loss + self.actor_loss - 0.01 * self.entropy\n",
    "                \n",
    "                # Clip norm\n",
    "                params = tf.trainable_variables(self.scope_name)\n",
    "                grads = tf.gradients(self.loss, params)\n",
    "                if max_grad_norm is not None:\n",
    "                    # Clip the gradients (normalize)\n",
    "                    grads, grad_norm = tf.clip_by_global_norm(grads, max_grad_norm)\n",
    "                grads = list(zip(grads, params))\n",
    "        \n",
    "                self.optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "                #self.train_op = self.optimizer.minimize(self.loss)\n",
    "                self.train_op = self.optimizer.apply_gradients(grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticTrainer:\n",
    "    def __init__(self, graph, vec_env, nb_env, eval_env, gamma, state_size, ckpt_file):\n",
    "        self.graph = graph\n",
    "        self.vec_env = vec_env\n",
    "        self.nb_env = nb_env\n",
    "        self.eval_env = eval_env\n",
    "        self.nb_actions = self.eval_env.nb_actions\n",
    "        self.gamma = gamma\n",
    "        self.state_size = state_size\n",
    "        self.ckpt_file = ckpt_file\n",
    "        self.saver = tf.train.Saver()\n",
    "\n",
    "    def choose_actions(self, input_states, session):\n",
    "        prob_actions = session.run(self.graph.prob_actions,\n",
    "                                    feed_dict={self.graph.state: input_states})\n",
    "\n",
    "        actions = [np.random.choice(np.arange(self.nb_actions), p=p) for p in prob_actions]\n",
    "\n",
    "        return actions\n",
    "    \n",
    "    def get_values(self, input_states, session):\n",
    "        values = session.run(self.graph.value,\n",
    "                             feed_dict={self.graph.state: input_states})\n",
    "\n",
    "        return values.flatten()\n",
    "        \n",
    "    def choose_actions_and_get_values(self, input_states, session):\n",
    "        prob_actions, values = session.run([self.graph.prob_actions, self.graph.value],\n",
    "                                           feed_dict={self.graph.state: input_states})\n",
    "\n",
    "        actions = [np.random.choice(np.arange(self.nb_actions), p=p) for p in prob_actions]\n",
    "\n",
    "        return actions, values.flatten()\n",
    "        \n",
    "    def run_episode(self, env_to_run, max_step, render, session):\n",
    "\n",
    "        # Reset state\n",
    "        state = env_to_run.reset()        \n",
    "        total_reward = 0\n",
    "        \n",
    "        for step in range(max_step):\n",
    "            state = np.expand_dims(state, axis=0)\n",
    "            action = self.choose_actions(state, session)[0]\n",
    "\n",
    "            # Apply action to env and get next state, reward, and done bool\n",
    "            state, reward, done, _ = env_to_run.step(action, render)\n",
    "            total_reward += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        print(\"Reward on episode: %f\" % total_reward)\n",
    "        \n",
    "    def train_on_batch(self, states, actions, target_values, advantages, session):\n",
    "        loss, _ = session.run([self.graph.loss, self.graph.train_op],\n",
    "                              feed_dict={self.graph.state: states,\n",
    "                                         self.graph.action: actions,\n",
    "                                         self.graph.target_value: target_values,\n",
    "                                         self.graph.advantage: advantages})\n",
    "        \n",
    "        return loss\n",
    "         \n",
    "    def play_and_learn(self, n_iterations, steps_per_iteration,\n",
    "                       evaluate_every, save_every,\n",
    "                       ckpt_file, session):\n",
    "\n",
    "        # Reset states\n",
    "        states = self.vec_env.reset()       \n",
    "\n",
    "        states_all_env = np.empty((self.nb_env, steps_per_iteration, *self.state_size), dtype=np.float)\n",
    "        actions_all_env = np.empty((self.nb_env, steps_per_iteration), dtype=np.int)\n",
    "        rewards_all_env = np.empty((self.nb_env, steps_per_iteration), dtype=np.float)\n",
    "        values_all_env = np.empty((self.nb_env, steps_per_iteration), dtype=np.float)\n",
    "        dones_all_env = np.empty((self.nb_env, steps_per_iteration), dtype=np.bool)\n",
    "        \n",
    "        T = time.time()\n",
    "        for iteration in range(1, n_iterations):\n",
    "\n",
    "            for step in range(steps_per_iteration):\n",
    "                actions, values = self.choose_actions_and_get_values(states, session)\n",
    "                next_states, rewards, dones, _ = self.vec_env.step(actions)\n",
    "\n",
    "                clipped_rewards = np.clip(rewards, -1, 1)\n",
    "                                \n",
    "                states_all_env[:, step] = states\n",
    "                actions_all_env[:, step] = actions\n",
    "                rewards_all_env[:, step] = clipped_rewards\n",
    "                values_all_env[:, step] = values\n",
    "                dones_all_env[:, step] = dones\n",
    "                \n",
    "                states = next_states\n",
    "            \n",
    "            # Estimated values for the future\n",
    "            next_estimated_values = self.get_values(states, session)\n",
    "            next_estimated_values = np.where(dones, 0, next_estimated_values)\n",
    "\n",
    "            target_values_all_env = compute_target_values(np.array(rewards_all_env),\n",
    "                                                          next_estimated_values,\n",
    "                                                          np.array(dones_all_env),\n",
    "                                                          self.gamma)\n",
    "            \n",
    "            advantages_all_env = target_values_all_env - values_all_env\n",
    "            \n",
    "            # Concatenate the experiences from the different envionments\n",
    "            batch_states = np.concatenate(states_all_env)\n",
    "            batch_actions = np.concatenate(actions_all_env)\n",
    "            batch_target_values = np.concatenate(target_values_all_env)\n",
    "            batch_advantages = np.concatenate(advantages_all_env)\n",
    "            \n",
    "            loss = self.train_on_batch(batch_states, batch_actions, batch_target_values, batch_advantages, session)\n",
    "            \n",
    "            if iteration % evaluate_every == 0:\n",
    "                self.run_episode(self.eval_env, 10000, False, session)\n",
    "                print(\"Time to play %i iterations: %f\" %(iteration, time.time() - T))\n",
    "                \n",
    "            if iteration % save_every == 0:\n",
    "                self.saver.save(session, self.ckpt_file)\n",
    "                print(\"Saved model after %i iterations.\" % iteration)\n",
    "                \n",
    "    def restore(self, session, ckpt=None):\n",
    "        if ckpt is None:\n",
    "            ckpt = self.ckpt_file\n",
    "        self.saver.restore(session, ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "BrokenPipeError",
     "evalue": "[Errno 32] Broken pipe",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-8e8653196aa1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvec_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-48-86eb9b795e55>\u001b[0m in \u001b[0;36mclose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_remotes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'close'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_remotes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.7.2_2/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_writable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrecv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.7.2_2/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_send_bytes\u001b[0;34m(self, buf)\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0;31m# Also note we want to avoid sending a 0-length buffer separately,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m             \u001b[0;31m# to avoid \"broken pipe\" errors if the other end closed the pipe.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheader\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.7.2_2/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_send\u001b[0;34m(self, buf, write)\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0mremaining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m             \u001b[0mremaining\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe"
     ]
    }
   ],
   "source": [
    "vec_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz_to_process = (4,) #(110,84)\n",
    "stack_size = 4\n",
    "state_size = (*sz_to_process, stack_size)\n",
    "\n",
    "# Create the environments\n",
    "game_name = 'CartPole-v0' #'LunarLander-v2'  #'Pong-v0'  #'SpaceInvaders-v0'\n",
    "nb_env = 16\n",
    "env_list = [EnvWrapper(game_name, state_size) for _ in range(nb_env)]\n",
    "eval_env = EnvWrapper(game_name, state_size)\n",
    "for e in env_list:\n",
    "    e.env._max_episode_steps = 2500\n",
    "eval_env.env._max_episode_steps = 2500\n",
    "\n",
    "\n",
    "vec_env = VecEnvWrapper(env_list)\n",
    "\n",
    "nb_actions = eval_env.nb_actions\n",
    "\n",
    "gamma = 0.99\n",
    "\n",
    "n_iterations = int(5e6)\n",
    "steps_per_iteration = 4\n",
    "learning_rate = 0.01 #0.00075\n",
    "max_grad_norm = 0.5\n",
    "\n",
    "evaluate_every = 5000\n",
    "save_every = 50000\n",
    "\n",
    "ckpt_file = \"./models/model_cartpole.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward on episode: 180.000000\n",
      "Time to play 5000 iterations: 78.180474\n",
      "Reward on episode: 17.000000\n",
      "Time to play 10000 iterations: 155.509469\n",
      "Reward on episode: 131.000000\n",
      "Time to play 15000 iterations: 233.698744\n",
      "Reward on episode: 222.000000\n",
      "Time to play 20000 iterations: 312.848570\n",
      "Reward on episode: 60.000000\n",
      "Time to play 25000 iterations: 388.877827\n",
      "Reward on episode: 12.000000\n",
      "Time to play 30000 iterations: 474.449958\n",
      "Reward on episode: 20.000000\n",
      "Time to play 35000 iterations: 550.608641\n",
      "Reward on episode: 73.000000\n",
      "Time to play 40000 iterations: 628.217841\n",
      "Reward on episode: 28.000000\n",
      "Time to play 45000 iterations: 705.565625\n",
      "Reward on episode: 237.000000\n",
      "Time to play 50000 iterations: 780.526319\n",
      "Saved model after 50000 iterations.\n",
      "Reward on episode: 48.000000\n",
      "Time to play 55000 iterations: 855.541730\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    acnet = ActorCriticGraphVectors(state_size, nb_actions, learning_rate, max_grad_norm, \"ACNet\")  # TODO Put lr and grad norm in trainer\n",
    "\n",
    "    actrainer = ActorCriticTrainer(acnet,\n",
    "                                   vec_env=vec_env, nb_env=nb_env,\n",
    "                                   eval_env=eval_env,\n",
    "                                   gamma=gamma,\n",
    "                                   state_size=state_size,\n",
    "                                   ckpt_file=ckpt_file)\n",
    "\n",
    "    # Setup TensorBoard\n",
    "    #writer = tf.summary.FileWriter(\"./tensorboard/\", sess.graph)\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #actrainer.restore(sess, ckpt=\"./models/model_pong.ckpt\")\n",
    "    actrainer.play_and_learn(n_iterations, steps_per_iteration,\n",
    "                             evaluate_every, save_every,\n",
    "                             ckpt_file, sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/model_cartpole.ckpt\n",
      "Reward on episode: 43.000000\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    acnet = ActorCriticGraphVectors(state_size, nb_actions, learning_rate, \"ACNet\")\n",
    "    \n",
    "    actrainer = ActorCriticTrainer(acnet,\n",
    "                                   vec_env=vec_env, nb_env=nb_env,\n",
    "                                   eval_env=eval_env,\n",
    "                                   gamma=gamma,\n",
    "                                   state_size=state_size,\n",
    "                                   ckpt_file=ckpt_file)\n",
    "    \n",
    "    actrainer.restore(sess)\n",
    "    actrainer.run_episode(eval_env, 10000, True, sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
