{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from skimage import transform\n",
    "from skimage.color import rgb2gray\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "from multiprocessing import Process, Pipe\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_screen(screen, sz_to_process):\n",
    "    gray = rgb2gray(screen)\n",
    "    cropped_screen = gray[8:-12,5:-12]  # For Space Invaders, TODO make it cleaner\n",
    "    \n",
    "    preprocessed_screen = transform.resize(cropped_screen, sz_to_process)\n",
    "    \n",
    "    return preprocessed_screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_frames(stacked_frames, last_2_screens, is_new_episode):\n",
    "    assert isinstance(stacked_frames, deque), \"stacked_frames has not type deque\"\n",
    "    sz_to_process = stacked_frames[0].shape\n",
    "    \n",
    "    max_screen = np.maximum(last_2_screens[0], last_2_screens[1])\n",
    "\n",
    "    frame = preprocess_screen(max_screen, sz_to_process)\n",
    "    \n",
    "    if is_new_episode:\n",
    "        for _ in range(len(stacked_frames)):\n",
    "            stacked_frames.append(frame)\n",
    "    else:\n",
    "        stacked_frames.append(frame)\n",
    "        \n",
    "    input_state = np.stack(stacked_frames, axis=2)\n",
    "                    \n",
    "    return input_state, stacked_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_target_values(rewards, next_estimated_value, is_done, gamma):\n",
    "    target_values = np.zeros_like(rewards, dtype=np.float)\n",
    "    cum = next_estimated_value\n",
    "    end_idx = len(rewards) - 1\n",
    "    \n",
    "    rev_rewards = reversed(rewards)\n",
    "    rev_is_done = reversed(is_done)\n",
    "    \n",
    "    for i, (r, done) in enumerate(zip(rev_rewards, rev_is_done)):\n",
    "        if done:\n",
    "            cum = r\n",
    "        else:\n",
    "            cum = gamma * cum + r\n",
    "        target_values[end_idx - i] = cum\n",
    "            \n",
    "    return target_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_target_values(batch_rewards, next_estimated_values, batch_dones, self.gamma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvWrapper:\n",
    "    # Do not have to handle stacked frames externally\n",
    "    \n",
    "    def __init__(self, game_name, state_size, frame_skipping):\n",
    "        self.env = gym.make(game_name)\n",
    "        self.stacked_frames = deque([np.zeros(state_size[:-1]) for _ in range(state_size[-1])],\n",
    "                                    maxlen=state_size[-1])\n",
    "        self.nb_actions = self.env.action_space.n\n",
    "        self.frame_skipping = frame_skipping\n",
    "        self.last_2_screens = deque(maxlen=2)\n",
    "        \n",
    "    def step(self, action, render=False):\n",
    "        for _ in range(self.frame_skipping):\n",
    "            screen, reward, done, info = self.env.step(action)\n",
    "            self.last_2_screens.append(screen)\n",
    "            if render:\n",
    "                self.render()\n",
    "            if done:\n",
    "                break\n",
    "        if done:\n",
    "            # Reset env and state\n",
    "            screen = self.env.reset()\n",
    "            for _ in range(2):\n",
    "                self.last_2_screens.append(screen)\n",
    "            stacked_state, self.stacked_frames = stack_frames(self.stacked_frames, self.last_2_screens, True)\n",
    "        else:\n",
    "            stacked_state, self.stacked_frames = stack_frames(self.stacked_frames, self.last_2_screens, False)\n",
    "\n",
    "        return stacked_state, reward, done, info\n",
    "            \n",
    "    def reset(self):\n",
    "        screen = self.env.reset()\n",
    "        for _ in range(2):\n",
    "            self.last_2_screens.append(screen)\n",
    "        stacked_state, self.stacked_frames = stack_frames(self.stacked_frames, self.last_2_screens, True)\n",
    "        \n",
    "        return stacked_state\n",
    "        \n",
    "    def render(self):\n",
    "        self.env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker(env_remote, worker_remote, env):\n",
    "\n",
    "    while True:\n",
    "        cmd, data = worker_remote.recv()\n",
    "\n",
    "        if cmd == 'step':\n",
    "            stacked_input, reward, done, info = env.step(data)\n",
    "            worker_remote.send((stacked_input, reward, done, info))\n",
    "\n",
    "        elif cmd == 'reset':\n",
    "            stacked_input = env.reset()\n",
    "            worker_remote.send(stacked_input)\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError         \n",
    "\n",
    "class VecEnvWrapper:\n",
    "    def __init__(self, env_list):\n",
    "        self.env_remotes, self.worker_remotes = zip(*[Pipe() for _ in range(len(env_list))])\n",
    "        self.processes = [Process(target=worker, args=(e_remote, w_remote, env))\n",
    "                            for (e_remote, w_remote, env) in zip(self.env_remotes, self.worker_remotes, env_list)]\n",
    "\n",
    "        for p in self.processes:\n",
    "            p.daemon = True\n",
    "            p.start()\n",
    "\n",
    "    def step(self, actions):\n",
    "        for r, a in zip(self.env_remotes, actions):\n",
    "            r.send(('step', a))\n",
    "\n",
    "        step_outputs = [r.recv() for r in self.env_remotes]\n",
    "\n",
    "        stacked_inputs, rewards, dones, infos = zip(*step_outputs)\n",
    "\n",
    "        return stacked_inputs, rewards, dones, infos\n",
    "\n",
    "    def reset(self):\n",
    "        for r in self.env_remotes:\n",
    "            r.send(('reset', None))\n",
    "        return [r.recv() for r in self.env_remotes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24274587631225586\n",
      "(5, 110, 84, 4)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "sz_to_process = (110,84)\n",
    "stack_size = 4\n",
    "state_size = (*sz_to_process, stack_size)\n",
    "n_env = 5\n",
    "\n",
    "env_list = [EnvWrapper('SpaceInvaders-v0', state_size, 1) for _ in range(n_env)]\n",
    "vec_env = VecEnvWrapper(env_list)\n",
    "\n",
    "a = vec_env.reset()\n",
    "T = time.time()\n",
    "for _ in range(10):\n",
    "    b = vec_env.step(np.zeros(n_env, dtype=np.int))\n",
    "print(time.time() - T)\n",
    "print(np.array(b[0]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticGraph:\n",
    "    \n",
    "    def __init__(self, state_size, nb_actions, learning_rate, scope_name):\n",
    "        self.state_size = state_size\n",
    "        self.nb_actions = nb_actions\n",
    "        self.learning_rate = learning_rate\n",
    "        self.scope_name = scope_name\n",
    "        \n",
    "        with tf.variable_scope(self.scope_name):\n",
    "            self.state = tf.placeholder(tf.float32, (None, *(self.state_size)), name=\"state\")\n",
    "            self.action = tf.placeholder(tf.uint8, (None,), name=\"action\")\n",
    "            self.action_OH = tf.one_hot(self.action, self.nb_actions, name=\"action_OH\")\n",
    "\n",
    " \n",
    "            self.target_value = tf.placeholder(tf.float32, (None,), name=\"target_value\")\n",
    "            self.advantage = tf.placeholder(tf.float32, (None,), name=\"advantage\")\n",
    "                        \n",
    "            # Neural net\n",
    "            self.conv1 = tf.layers.conv2d(inputs=self.state,\n",
    "                                          filters=32,\n",
    "                                          kernel_size=(8,8),\n",
    "                                          strides=(4,4),\n",
    "                                          padding=\"valid\",\n",
    "                                          activation=tf.nn.relu,\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                          name=\"conv1\")\n",
    "            \n",
    "            self.conv2 = tf.layers.conv2d(inputs=self.conv1,\n",
    "                                          filters=64,\n",
    "                                          kernel_size=(4,4),\n",
    "                                          strides=(2,2),\n",
    "                                          padding=\"valid\",\n",
    "                                          activation=tf.nn.relu,\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                          name=\"conv2\")\n",
    "            \n",
    "            self.conv3 = tf.layers.conv2d(inputs=self.conv2,\n",
    "                                          filters=64,\n",
    "                                          kernel_size=(3,3),\n",
    "                                          strides=(2,2),\n",
    "                                          padding=\"valid\",\n",
    "                                          activation=tf.nn.relu,\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                          name=\"conv3\")\n",
    "            \n",
    "            self.flatten = tf.layers.flatten(self.conv3)\n",
    "            \n",
    "            # Actor part\n",
    "            self.fc_actions = tf.layers.dense(self.flatten,\n",
    "                                              units=512,\n",
    "                                              activation=tf.nn.relu,\n",
    "                                              kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                              name=\"fc_action\")\n",
    "            \n",
    "            self.prob_actions = tf.layers.dense(self.fc_actions,\n",
    "                                                units=self.nb_actions,\n",
    "                                                activation=tf.nn.softmax,\n",
    "                                                kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                                name=\"action_distribution\")\n",
    "            \n",
    "            # Critic part\n",
    "            self.fc_value = tf.layers.dense(self.flatten,\n",
    "                                            units=512,\n",
    "                                            activation=tf.nn.relu,\n",
    "                                            kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                            name=\"fc_value\")\n",
    "            \n",
    "            self.value = tf.layers.dense(self.fc_value,\n",
    "                                         units=1,\n",
    "                                         activation=None,\n",
    "                                         kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                         name=\"value\")\n",
    "            \n",
    "            # Losses\n",
    "            # Actor loss\n",
    "            self.log_prob_actions = tf.math.log(self.prob_actions)\n",
    "            \n",
    "            self.log_prob_chosen_action = tf.reduce_sum(self.log_prob_actions * self.action_OH, axis=1)\n",
    "            self.actor_loss = - tf.reduce_mean(self.log_prob_chosen_action * self.advantage)\n",
    "            \n",
    "            # Critic loss\n",
    "            self.critic_loss = tf.reduce_mean(tf.square(self.target_value - self.value))\n",
    "\n",
    "            # Entropy: sum(p(x) * -log(p(x)))\n",
    "            self.entropy = tf.reduce_sum(tf.multiply(self.prob_actions, - self.log_prob_actions))\n",
    "\n",
    "            # TODO put coeffs as parameters\n",
    "            self.loss = 0.5 * self.critic_loss + self.actor_loss - 0.01 * self.entropy\n",
    "\n",
    "            self.optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "            self.train_op = self.optimizer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticTrainer:\n",
    "    def __init__(self, graph, vec_env, eval_env, gamma, state_size, ckpt_file):\n",
    "        self.graph = graph\n",
    "        #self.env = env\n",
    "        self.vec_env = vec_env\n",
    "        self.eval_env = eval_env\n",
    "        self.nb_actions = self.eval_env.nb_actions\n",
    "        self.gamma = gamma\n",
    "        self.state_size = state_size\n",
    "        self.ckpt_file = ckpt_file\n",
    "        self.saver = tf.train.Saver()\n",
    "\n",
    "    def choose_actions(self, input_states, session):\n",
    "        prob_actions = session.run(self.graph.prob_actions,\n",
    "                                    feed_dict={self.graph.state: input_states})\n",
    "\n",
    "        actions = [np.random.choice(np.arange(self.nb_actions), p=p) for p in prob_actions]\n",
    "\n",
    "        return actions\n",
    "    \n",
    "    def get_values(self, input_states, session):\n",
    "        values = session.run(self.graph.value,\n",
    "                             feed_dict={self.graph.state: input_states})\n",
    "\n",
    "        #return value.flatten()[0]\n",
    "        return values\n",
    "        \n",
    "    def choose_actions_and_get_values(self, input_states, session):\n",
    "        prob_actions, values = session.run([self.graph.prob_actions, self.graph.value],\n",
    "                                           feed_dict={self.graph.state: input_states})\n",
    "\n",
    "        actions = [np.random.choice(np.arange(self.nb_actions), p=p) for p in prob_actions]\n",
    "\n",
    "        return actions, values\n",
    "        \n",
    "    def run_episode(self, env_to_run, max_step, render, session):\n",
    "\n",
    "        # Reset state\n",
    "        state = env_to_run.reset()        \n",
    "        total_reward = 0\n",
    "        \n",
    "        for step in range(max_step):\n",
    "            action = self.choose_action(state, session)\n",
    "\n",
    "            # Apply action to env and get next state, reward, and done bool\n",
    "            state, reward, done, _ = env_to_run.step(action, render)\n",
    "            total_reward += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        print(\"Reward on episode: %f\" % total_reward)\n",
    "        \n",
    "    def train_on_batch(self, states, actions, target_values, advantages, session):\n",
    "        loss, _ = session.run([self.graph.loss, self.graph.train_op],\n",
    "                              feed_dict={self.graph.state: states,\n",
    "                                         self.graph.action: actions,\n",
    "                                         self.graph.target_value: target_values,\n",
    "                                         self.graph.advantage: advantages})\n",
    "        \n",
    "        return loss\n",
    "         \n",
    "    def play_and_learn(self, n_iterations, steps_per_iteration,\n",
    "                       evaluate_every, save_every,\n",
    "                       ckpt_file, session):\n",
    "\n",
    "        # Reset states\n",
    "        #state = self.env.reset()       \n",
    "        states = self.vec_env.reset()       \n",
    "\n",
    "        for iteration in range(1, n_iterations):\n",
    "            \n",
    "            batch_states, batch_actions, batch_rewards, batch_values, batch_dones = [], [], [], [], []\n",
    "            #is_done = np.zeros(steps_per_iteration, dtype=bool)\n",
    "            \n",
    "            for step in range(steps_per_iteration):\n",
    "                #state = np.expand_dims(input_states, axis=0)\n",
    "                #action, value = self.choose_action_and_get_val(state, session)\n",
    "                #next_state, reward, done, _ = self.env.step(action)\n",
    "                actions, values = self.choose_actions_and_get_values(states, session)\n",
    "                next_states, rewards, dones, _ = self.vec_env.step(actions)\n",
    "                \n",
    "                clipped_rewards = np.clip(rewards, -1, 1)\n",
    "                                \n",
    "                batch_states.append(states)\n",
    "                batch_actions.append(actions)\n",
    "                batch_rewards.append(clipped_rewards)\n",
    "                batch_values.append(values)\n",
    "                batch_dones.append(dones)\n",
    "                \n",
    "                states = next_states\n",
    "                    \n",
    "                #if done:\n",
    "                #    is_done[step] = True\n",
    "                    \n",
    "            # Target values\n",
    "            next_estimated_values = self.get_values(states, session)\n",
    "            next_estimated_values = np.where(dones, 0, next_estimated_value)\n",
    "\n",
    "            #target_values = compute_target_values(rewards, next_estimated_value, is_done, self.gamma)\n",
    "            target_values = compute_target_values(batch_rewards, next_estimated_values, batch_dones, self.gamma)\n",
    "            \n",
    "            advantages = target_values - values\n",
    "                                    \n",
    "            loss = self.train_on_batch(states, actions, target_values, advantages, session)\n",
    "                \n",
    "            if iteration % evaluate_every == 0:\n",
    "                self.run_episode(self.eval_env, 10000, False, session)\n",
    "                \n",
    "            if iteration % save_every == 0:\n",
    "                self.saver.save(session, self.ckpt_file)\n",
    "                print(\"Saved model after %i iterations.\" % iteration)\n",
    "                \n",
    "    def restore(self, session):\n",
    "        self.saver.restore(session, self.ckpt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz_to_process = (110,84)\n",
    "stack_size = 4\n",
    "state_size = (*sz_to_process, stack_size)\n",
    "\n",
    "frame_skipping = 2\n",
    "\n",
    "# Create the environments\n",
    "#env = EnvWrapper('SpaceInvaders-v0', state_size, frame_skipping, sz_to_process)\n",
    "nb_env = 8\n",
    "env_list = [EnvWrapper('SpaceInvaders-v0', state_size, frame_skipping) for _ in range(nb_env)]\n",
    "eval_env = EnvWrapper('SpaceInvaders-v0', state_size, frame_skipping)\n",
    "\n",
    "vec_env = VecEnvWrapper(env_list)\n",
    "\n",
    "nb_actions = eval_env.nb_actions\n",
    "\n",
    "gamma = 0.95\n",
    "\n",
    "n_iterations = int(5e6)\n",
    "steps_per_iteration = 32\n",
    "learning_rate = 0.00025\n",
    "\n",
    "evaluate_every = 5000\n",
    "save_every = 20000\n",
    "\n",
    "ckpt_file = \"./models/model.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 8)\n",
      "[False False False False False False False False]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'is_done' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-3e2a80322d80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m     actrainer.play_and_learn(n_iterations, steps_per_iteration,\n\u001b[1;32m     20\u001b[0m                              \u001b[0mevaluate_every\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_every\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                              ckpt_file, sess)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-89-b52c09c53bc7>\u001b[0m in \u001b[0;36mplay_and_learn\u001b[0;34m(self, n_iterations, steps_per_iteration, evaluate_every, save_every, ckpt_file, session)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_estimated_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdones\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0mtarget_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_target_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_estimated_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_done\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0madvantages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_values\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'is_done' is not defined"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    acnet = ActorCriticGraph(state_size, nb_actions, learning_rate, \"ACNet\")\n",
    "    \n",
    "    actrainer = ActorCriticTrainer(acnet,\n",
    "                                   #env=env,\n",
    "                                   vec_env=vec_env,\n",
    "                                   eval_env=eval_env,\n",
    "                                   gamma=gamma,\n",
    "                                   state_size=state_size,\n",
    "                                   ckpt_file=ckpt_file)\n",
    "\n",
    "    # Setup TensorBoard\n",
    "    #writer = tf.summary.FileWriter(\"./tensorboard/\", sess.graph)\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #actrainer.restore(sess)\n",
    "    actrainer.play_and_learn(n_iterations, steps_per_iteration,\n",
    "                             evaluate_every, save_every,\n",
    "                             ckpt_file, sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    acnet = ActorCriticGraph(state_size, nb_actions, learning_rate, \"ACNet\")\n",
    "    \n",
    "    actrainer = ActorCriticTrainer(acnet,\n",
    "                                   env=env,\n",
    "                                   eval_env=eval_env,\n",
    "                                   gamma=gamma,\n",
    "                                   state_size=state_size,\n",
    "                                   ckpt_file=ckpt_file)\n",
    "    \n",
    "    actrainer.restore(sess)\n",
    "    actrainer.run_episode(eval_env, 10000, True, sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
