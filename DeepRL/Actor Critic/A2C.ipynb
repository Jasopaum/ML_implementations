{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from skimage import transform\n",
    "from skimage.color import rgb2gray\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "from multiprocessing import Process, Pool\n",
    "from multiprocessing import sharedctypes\n",
    "\n",
    "from threading import Thread\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_screen(screen, sz_to_process):\n",
    "    gray = rgb2gray(screen)\n",
    "    cropped_screen = gray[8:-12,5:-12]  # For Space Invaders, TODO make it cleaner\n",
    "    \n",
    "    preprocessed_screen = transform.resize(cropped_screen, sz_to_process)\n",
    "    \n",
    "    return preprocessed_screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_frames(stacked_frames, screen, is_new_episode):\n",
    "    assert isinstance(stacked_frames, deque), \"stacked_frames has not type deque\"\n",
    "    sz_to_process = stacked_frames[0].shape\n",
    "    frame = preprocess_screen(screen, sz_to_process)\n",
    "    # Not tested yet:\n",
    "    frame = np.maximum(frame, stacked_frames[-1])\n",
    "    if is_new_episode:\n",
    "        for _ in range(len(stacked_frames)):\n",
    "            stacked_frames.append(frame)\n",
    "\n",
    "    else:\n",
    "        stacked_frames.append(frame)\n",
    "        \n",
    "    input_state = np.stack(stacked_frames, axis=2)\n",
    "    \n",
    "    return input_state, stacked_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_target_values(rewards, next_estimated_value, is_done, gamma):\n",
    "    target_values = np.zeros_like(rewards, dtype=np.float)\n",
    "    cum = next_estimated_value\n",
    "    end_idx = len(rewards) - 1\n",
    "    \n",
    "    rev_rewards = reversed(rewards)\n",
    "    rev_is_done = reversed(is_done)\n",
    "    \n",
    "    for i, (r, done) in enumerate(zip(rev_rewards, rev_is_done)):\n",
    "        if done:\n",
    "            cum = r\n",
    "        else:\n",
    "            cum = gamma * cum + r\n",
    "        target_values[end_idx - i] = cum\n",
    "            \n",
    "    return target_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvWrapper:\n",
    "    # Do not have to handle stacked frames externally\n",
    "    \n",
    "    def __init__(self, game_name, state_size, frame_skipping):\n",
    "        self.env = gym.make(game_name)\n",
    "        self.stacked_frames = deque([np.zeros(state_size[:-1], dtype=np.float) for _ in range(state_size[-1])],\n",
    "                                     maxlen=state_size[-1])\n",
    "        self.nb_actions = self.env.action_space.n\n",
    "        self.frame_skipping = frame_skipping\n",
    "        \n",
    "    def step(self, action, render=False):\n",
    "        for _ in range(self.frame_skipping):\n",
    "            screen, reward, done, info = self.env.step(action)\n",
    "            if render:\n",
    "                self.render()\n",
    "            if done:\n",
    "                break\n",
    "        if done:\n",
    "            # Reset env and state\n",
    "            screen = self.env.reset()\n",
    "            stacked_input, self.stacked_frames = stack_frames(self.stacked_frames, screen, True)\n",
    "        else:\n",
    "            stacked_input, self.stacked_frames = stack_frames(self.stacked_frames, screen, False)\n",
    "\n",
    "        return stacked_input, reward, done, info\n",
    "            \n",
    "    def reset(self):\n",
    "        screen = self.env.reset()\n",
    "        stacked_input, self.stacked_frames = stack_frames(self.stacked_frames, screen, True)\n",
    "        \n",
    "        return stacked_input\n",
    "        \n",
    "    def render(self):\n",
    "        self.env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticGraph:\n",
    "    \n",
    "    def __init__(self, state_size, nb_actions, learning_rate, scope_name):\n",
    "        self.state_size = state_size\n",
    "        self.nb_actions = nb_actions\n",
    "        self.learning_rate = learning_rate\n",
    "        self.scope_name = scope_name\n",
    "        \n",
    "        with tf.variable_scope(self.scope_name):\n",
    "            self.state = tf.placeholder(tf.float32, (None, *(self.state_size)), name=\"state\")\n",
    "            self.action = tf.placeholder(tf.uint8, (None,), name=\"action\")\n",
    "            self.action_OH = tf.one_hot(self.action, self.nb_actions, name=\"action_OH\")\n",
    "\n",
    " \n",
    "            self.target_value = tf.placeholder(tf.float32, (None,), name=\"target_value\")\n",
    "            self.advantage = tf.placeholder(tf.float32, (None,), name=\"advantage\")\n",
    "                        \n",
    "            # Neural net\n",
    "            self.conv1 = tf.layers.conv2d(inputs=self.state,\n",
    "                                          filters=32,\n",
    "                                          kernel_size=(8,8),\n",
    "                                          strides=(4,4),\n",
    "                                          padding=\"valid\",\n",
    "                                          activation=tf.nn.relu,\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                          name=\"conv1\")\n",
    "            \n",
    "            self.conv2 = tf.layers.conv2d(inputs=self.conv1,\n",
    "                                          filters=64,\n",
    "                                          kernel_size=(4,4),\n",
    "                                          strides=(2,2),\n",
    "                                          padding=\"valid\",\n",
    "                                          activation=tf.nn.relu,\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                          name=\"conv2\")\n",
    "            \n",
    "            self.conv3 = tf.layers.conv2d(inputs=self.conv2,\n",
    "                                          filters=64,\n",
    "                                          kernel_size=(3,3),\n",
    "                                          strides=(2,2),\n",
    "                                          padding=\"valid\",\n",
    "                                          activation=tf.nn.relu,\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                          name=\"conv3\")\n",
    "            \n",
    "            self.flatten = tf.layers.flatten(self.conv3)\n",
    "            \n",
    "            # Actor part\n",
    "            self.fc_actions = tf.layers.dense(self.flatten,\n",
    "                                              units=512,\n",
    "                                              activation=tf.nn.relu,\n",
    "                                              kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                              name=\"fc_action\")\n",
    "            \n",
    "            self.prob_actions = tf.layers.dense(self.fc_actions,\n",
    "                                                units=self.nb_actions,\n",
    "                                                activation=tf.nn.softmax,\n",
    "                                                kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                                name=\"action_distribution\")\n",
    "            \n",
    "            # Critic part\n",
    "            self.fc_value = tf.layers.dense(self.flatten,\n",
    "                                            units=512,\n",
    "                                            activation=tf.nn.relu,\n",
    "                                            kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                            name=\"fc_value\")\n",
    "            \n",
    "            self.value = tf.layers.dense(self.fc_value,\n",
    "                                         units=1,\n",
    "                                         activation=None,\n",
    "                                         kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                         name=\"value\")\n",
    "            \n",
    "            # Losses\n",
    "            # Actor loss\n",
    "            self.log_prob_actions = tf.math.log(self.prob_actions)\n",
    "            \n",
    "            self.log_prob_chosen_action = tf.reduce_sum(self.log_prob_actions * self.action_OH, axis=1)\n",
    "            self.actor_loss = - tf.reduce_mean(self.log_prob_chosen_action * self.advantage)\n",
    "            \n",
    "            # Critic loss\n",
    "            self.critic_loss = tf.reduce_mean(tf.square(self.target_value - self.value))\n",
    "\n",
    "            # Entropy: sum(p(x) * -log(p(x)))\n",
    "            self.entropy = tf.reduce_sum(tf.multiply(self.prob_actions, - self.log_prob_actions))\n",
    "\n",
    "            # TODO put coeffs as parameters\n",
    "            self.loss = 0.5 * self.critic_loss + self.actor_loss - 0.01 * self.entropy\n",
    "\n",
    "            self.optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "            self.train_op = self.optimizer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticTrainer:\n",
    "    def __init__(self, graph, env, eval_env, gamma, state_size, ckpt_file):\n",
    "        self.graph = graph\n",
    "        self.env = env\n",
    "        self.nb_actions = self.env.nb_actions\n",
    "        #self.env_list = env_list\n",
    "        self.eval_env = eval_env\n",
    "        #self.nb_actions = self.env_list[0].nb_actions\n",
    "        self.gamma = gamma\n",
    "        self.state_size = state_size\n",
    "        self.ckpt_file = ckpt_file\n",
    "        self.saver = tf.train.Saver()\n",
    "\n",
    "    def choose_action(self, input_state, session):\n",
    "        prob_actions = session.run(self.graph.prob_actions,\n",
    "                                    feed_dict={self.graph.state: np.expand_dims(input_state, axis=0)})\n",
    "\n",
    "        action = np.random.choice(np.arange(nb_actions),\n",
    "                                  p=np.ravel(prob_actions))\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def get_val(self, input_state, session):\n",
    "        value = session.run(self.graph.value,\n",
    "                            feed_dict={self.graph.state: np.expand_dims(input_state, axis=0)})\n",
    "\n",
    "        return value.flatten()[0]\n",
    "        \n",
    "    def choose_action_and_get_val(self, input_state, session):\n",
    "        prob_actions, value = session.run([self.graph.prob_actions, self.graph.value],\n",
    "                                           feed_dict={self.graph.state: np.expand_dims(input_state, axis=0)})\n",
    "\n",
    "        action = np.random.choice(np.arange(nb_actions),\n",
    "                                  p=np.ravel(prob_actions))\n",
    "\n",
    "        return action, value.flatten()[0]\n",
    "        \n",
    "    def run_episode(self, env_to_run, max_step, render, session):\n",
    "\n",
    "        # Reset state\n",
    "        state = env_to_run.reset()        \n",
    "        total_reward = 0\n",
    "        \n",
    "        for step in range(max_step):\n",
    "            action = self.choose_action(state, session)\n",
    "\n",
    "            # Apply action to env and get next state, reward, and done bool\n",
    "            state, reward, done, _ = env_to_run.step(action, render)\n",
    "            total_reward += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        print(\"Reward on episode: %f\" % total_reward)\n",
    "        \n",
    "    def train_on_batch(self, states, actions, target_values, advantages, session):\n",
    "        loss, _ = session.run([self.graph.loss, self.graph.train_op],\n",
    "                              feed_dict={self.graph.state: states,\n",
    "                                         self.graph.action: actions,\n",
    "                                         self.graph.target_value: target_values,\n",
    "                                         self.graph.advantage: advantages})\n",
    "        \n",
    "        return loss\n",
    "         \n",
    "    def play_and_learn(self, n_iterations, steps_per_iteration,\n",
    "                       evaluate_every, save_every,\n",
    "                       ckpt_file, session):\n",
    "\n",
    "        # Reset states\n",
    "        state = self.env.reset()       \n",
    "\n",
    "        for iteration in range(1, n_iterations):\n",
    "            \n",
    "            states, actions, rewards, values = [], [], [], []\n",
    "            is_done = np.zeros(steps_per_iteration, dtype=bool)\n",
    "            \n",
    "            for step in range(steps_per_iteration):\n",
    "                action, value = self.choose_action_and_get_val(state, session)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "\n",
    "                clipped_reward = np.clip(reward, -1, 1)\n",
    "                \n",
    "                states.append(state)\n",
    "                actions.append(action)\n",
    "                rewards.append(clipped_reward)\n",
    "                values.append(value)\n",
    "                \n",
    "                state = next_state\n",
    "\n",
    "                if done:\n",
    "                    is_done[step] = True\n",
    "                    \n",
    "            # Target values\n",
    "            next_estimated_value = 0 if done else self.get_val(state, session)\n",
    "            target_values = compute_target_values(rewards, next_estimated_value, is_done, self.gamma)\n",
    "            \n",
    "            advantages = target_values - values\n",
    "                                    \n",
    "            loss = self.train_on_batch(states, actions, target_values, advantages, session)\n",
    "                \n",
    "            if iteration % evaluate_every == 0:\n",
    "                self.run_episode(self.eval_env, 10000, False, session)\n",
    "                \n",
    "            if iteration % save_every == 0:\n",
    "                self.saver.save(session, self.ckpt_file)\n",
    "                print(\"Saved model after %i iterations.\" % iteration)\n",
    "                \n",
    "    def restore(self, session):\n",
    "        self.saver.restore(session, self.ckpt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz_to_process = (110,84)\n",
    "stack_size = 4\n",
    "state_size = (*sz_to_process, stack_size)\n",
    "\n",
    "frame_skipping = 2\n",
    "\n",
    "# Create the environments\n",
    "env = EnvWrapper('SpaceInvaders-v0', state_size, frame_skipping)\n",
    "eval_env = EnvWrapper('SpaceInvaders-v0', state_size, frame_skipping)\n",
    "\n",
    "nb_actions = env.nb_actions\n",
    "\n",
    "gamma = 0.95\n",
    "\n",
    "n_iterations = int(5e6)\n",
    "steps_per_iteration = 8\n",
    "learning_rate = 0.00025\n",
    "\n",
    "evaluate_every = 2500\n",
    "save_every = 10000\n",
    "\n",
    "ckpt_file = \"./models/model.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/model.ckpt\n",
      "Reward on episode: 35.000000\n",
      "Reward on episode: 30.000000\n",
      "Reward on episode: 155.000000\n",
      "Reward on episode: 70.000000\n",
      "Saved model after 10000 iterations.\n",
      "Reward on episode: 90.000000\n",
      "Reward on episode: 15.000000\n",
      "Reward on episode: 60.000000\n",
      "Reward on episode: 50.000000\n",
      "Saved model after 20000 iterations.\n",
      "Reward on episode: 120.000000\n",
      "Reward on episode: 50.000000\n",
      "Reward on episode: 80.000000\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    acnet = ActorCriticGraph(state_size, nb_actions, learning_rate, \"ACNet\")\n",
    "    \n",
    "    actrainer = ActorCriticTrainer(acnet,\n",
    "                                   env=env,\n",
    "                                   eval_env=eval_env,\n",
    "                                   gamma=gamma,\n",
    "                                   state_size=state_size,\n",
    "                                   ckpt_file=ckpt_file)\n",
    "\n",
    "    # Setup TensorBoard\n",
    "    #writer = tf.summary.FileWriter(\"./tensorboard/\", sess.graph)\n",
    "    \n",
    "    #sess.run(tf.global_variables_initializer())\n",
    "    actrainer.restore(sess)\n",
    "    actrainer.play_and_learn(n_iterations, steps_per_iteration,\n",
    "                             evaluate_every, save_every,\n",
    "                             ckpt_file, sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/model.ckpt\n",
      "Reward on episode: 155.000000\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    acnet = ActorCriticGraph(state_size, nb_actions, learning_rate, \"ACNet\")\n",
    "    \n",
    "    actrainer = ActorCriticTrainer(acnet,\n",
    "                                   env=env,\n",
    "                                   eval_env=eval_env,\n",
    "                                   gamma=gamma,\n",
    "                                   state_size=state_size,\n",
    "                                   ckpt_file=ckpt_file)\n",
    "    \n",
    "    actrainer.restore(sess)\n",
    "    actrainer.run_episode(eval_env, 10000, True, sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
