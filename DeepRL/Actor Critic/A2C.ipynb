{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from skimage import transform\n",
    "from skimage.color import rgb2gray\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "from multiprocessing import Process, Pipe\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_screen(screen, sz_to_process):\n",
    "    print(screen.shape)\n",
    "    print(screen)\n",
    "    gray = rgb2gray(screen)\n",
    "    #cropped_screen = gray[8:-12,5:-12]  # For Space Invaders, TODO make it cleaner\n",
    "    cropped_screen = gray[25:-10,5:-5]  # For Pong, TODO make it cleaner\n",
    "    preprocessed_screen = transform.resize(cropped_screen, sz_to_process, mode='constant', anti_aliasing=True)\n",
    "    \n",
    "    return preprocessed_screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_observations(stacked_observations, observation, is_new_episode):\n",
    "    assert isinstance(stacked_observations, deque), \"stacked_observations has not type deque\"\n",
    "    sz_to_process = stacked_observations[0].shape  #TODO make it cleaner\n",
    "        \n",
    "    if is_new_episode:\n",
    "        for _ in range(len(stacked_observations)):\n",
    "            stacked_observations.append(observation)\n",
    "    else:\n",
    "        stacked_observations.append(observation)\n",
    "    \n",
    "    last_axis = len(stacked_observations[0].shape)\n",
    "    state = np.stack(stacked_observations, axis=last_axis)\n",
    "                    \n",
    "    return state, stacked_observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_target_values(batch_rewards, next_estimated_values, batch_dones, gamma):\n",
    "    nb_seq, len_seq = batch_rewards.shape\n",
    "    batch_target_values = np.zeros_like(batch_rewards, dtype=np.float)\n",
    "    cums = next_estimated_values\n",
    "        \n",
    "    for i in range(len_seq-1, -1, -1):\n",
    "        cums = np.where(batch_dones[:, i], batch_rewards[:, i], gamma * cums + batch_rewards[:, i])\n",
    "        \n",
    "        batch_target_values[:, i] = cums\n",
    "        \n",
    "    return batch_target_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvWrapper:\n",
    "    # Do not have to handle stacked frames externally\n",
    "    \n",
    "    def __init__(self, game_name, state_size):\n",
    "        self.env = gym.make(game_name)\n",
    "        self.stacked_observations = deque([np.zeros(state_size[:-1]) for _ in range(state_size[-1])],\n",
    "                                    maxlen=state_size[-1])\n",
    "        self.nb_actions = self.env.action_space.n\n",
    "        self.observation_shape = self.env.observation_space.shape\n",
    "        self.obs_is_image = self.observation_shape == (210, 160, 3)\n",
    "        \n",
    "    def step(self, action, render=False):\n",
    "        observation, reward, done, info = self.env.step(action)\n",
    "        if self.obs_is_image:\n",
    "            observation = preprocess_screen(observation, sz_to_process)\n",
    "        if render:\n",
    "            self.render()\n",
    "            \n",
    "        if done:\n",
    "            # Reset env and state\n",
    "            observation = self.env.reset()\n",
    "            if self.obs_is_image:\n",
    "                observation = preprocess_screen(observation, sz_to_process)\n",
    "\n",
    "            stacked_state, self.stacked_observations = stack_observations(self.stacked_observations, observation, True)\n",
    "        else:\n",
    "            stacked_state, self.stacked_observations = stack_observations(self.stacked_observations, observation, False)\n",
    "\n",
    "        return stacked_state, reward, done, info\n",
    "            \n",
    "    def reset(self):\n",
    "        observation = self.env.reset()\n",
    "        if self.obs_is_image:\n",
    "            observation = preprocess_screen(observation, sz_to_process)\n",
    "\n",
    "        stacked_state, self.stacked_observations = stack_observations(self.stacked_observations, observation, True)\n",
    "        \n",
    "        return stacked_state\n",
    "        \n",
    "    def render(self):\n",
    "        self.env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker(env_remote, worker_remote, env):\n",
    "    env_remote.close()\n",
    "    while True:\n",
    "        cmd, data = worker_remote.recv()\n",
    "\n",
    "        if cmd == 'step':\n",
    "            stacked_input, reward, done, info = env.step(data)\n",
    "            worker_remote.send((stacked_input, reward, done, info))\n",
    "\n",
    "        elif cmd == 'reset':\n",
    "            stacked_input = env.reset()\n",
    "            worker_remote.send(stacked_input)\n",
    "            \n",
    "        elif cmd == 'close':\n",
    "            env_remote.close()\n",
    "            worker_remote.close()\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError         \n",
    "\n",
    "class VecEnvWrapper:\n",
    "    def __init__(self, env_list):\n",
    "        self.env_remotes, self.worker_remotes = zip(*[Pipe() for _ in range(len(env_list))])\n",
    "        self.processes = [Process(target=worker, args=(e_remote, w_remote, env))\n",
    "                            for (e_remote, w_remote, env) in zip(self.env_remotes, self.worker_remotes, env_list)]\n",
    "\n",
    "        for p in self.processes:\n",
    "            p.daemon = True\n",
    "            p.start()\n",
    "        for wr in self.worker_remotes:\n",
    "            wr.close()\n",
    "            \n",
    "    def step(self, actions):   \n",
    "        for r, a in zip(self.env_remotes, actions):\n",
    "            r.send(('step', a))\n",
    "        \n",
    "        step_outputs = [r.recv() for r in self.env_remotes]\n",
    "\n",
    "        stacked_inputs, rewards, dones, infos = zip(*step_outputs)\n",
    "\n",
    "        return stacked_inputs, rewards, dones, infos\n",
    "\n",
    "    def reset(self):\n",
    "        for r in self.env_remotes:\n",
    "            r.send(('reset', None))\n",
    "        return [r.recv() for r in self.env_remotes]\n",
    "    \n",
    "    def close(self):\n",
    "        for r in self.env_remotes:\n",
    "            r.send(('close', None))\n",
    "        for r in self.env_remotes:\n",
    "            r.close()\n",
    "        for p in self.processes:\n",
    "            p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticGraphImages:\n",
    "    \n",
    "    def __init__(self, state_size, nb_actions, learning_rate, scope_name):\n",
    "        self.state_size = state_size\n",
    "        self.nb_actions = nb_actions\n",
    "        self.learning_rate = learning_rate\n",
    "        self.scope_name = scope_name\n",
    "        \n",
    "        with tf.variable_scope(self.scope_name):\n",
    "            self.state = tf.placeholder(tf.float32, (None, *(self.state_size)), name=\"state\")\n",
    "            self.action = tf.placeholder(tf.uint8, (None,), name=\"action\")\n",
    "            self.action_OH = tf.one_hot(self.action, self.nb_actions, name=\"action_OH\")\n",
    "\n",
    " \n",
    "            self.target_value = tf.placeholder(tf.float32, (None,), name=\"target_value\")\n",
    "            self.advantage = tf.placeholder(tf.float32, (None,), name=\"advantage\")\n",
    "        \n",
    "            initializer = tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "            with tf.device('/device:GPU:0'):\n",
    "                # Neural net\n",
    "                conv1_F = tf.Variable(initializer((7, 7, 4, 8)))\n",
    "                self.conv1 = tf.nn.conv2d(input=self.state,\n",
    "                                          filter=conv1_F,\n",
    "                                          strides=(1,4,4,1),\n",
    "                                          padding=\"VALID\",\n",
    "                                          name=\"conv1\")\n",
    "                self.conv1_act = tf.nn.relu(self.conv1)\n",
    "\n",
    "                conv2_F = tf.Variable(initializer((5, 5, 8, 16)))\n",
    "                self.conv2 = tf.nn.conv2d(input=self.conv1_act,\n",
    "                                          filter=conv2_F,\n",
    "                                          strides=(1,2,2,1),\n",
    "                                          padding=\"VALID\",\n",
    "                                          name=\"conv2\")\n",
    "                self.conv2_act = tf.nn.relu(self.conv2)\n",
    "            \n",
    "            \n",
    "                conv3_F = tf.Variable(initializer((3, 3, 16, 16)))\n",
    "                self.conv3 = tf.nn.conv2d(input=self.conv2_act,\n",
    "                                          filter=conv3_F,\n",
    "                                          strides=(1,2,2,1),\n",
    "                                          padding=\"VALID\",\n",
    "                                          name=\"conv3\")\n",
    "                self.conv3_act = tf.nn.relu(self.conv3)\n",
    "            \n",
    "                self.flatten = tf.keras.layers.Flatten()(self.conv3_act)\n",
    "\n",
    "                # Actor part\n",
    "                self.fc_actions = tf.keras.layers.Dense(units=512,\n",
    "                                                        activation=tf.nn.relu,\n",
    "                                                        kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                                        name=\"fc_action\")(self.flatten)\n",
    "\n",
    "                self.prob_actions = tf.keras.layers.Dense(units=self.nb_actions,\n",
    "                                                          activation=tf.nn.softmax,\n",
    "                                                          kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                                          name=\"action_distribution\")(self.fc_actions)\n",
    "\n",
    "                # Critic part\n",
    "                self.fc_value = tf.keras.layers.Dense(\n",
    "                                                units=512,\n",
    "                                                activation=tf.nn.relu,\n",
    "                                                kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                                name=\"fc_value\")(self.flatten)\n",
    "\n",
    "                self.value = tf.keras.layers.Dense(units=1,\n",
    "                                                   activation=None,\n",
    "                                                   kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                                   name=\"value\")(self.fc_value)\n",
    "            \n",
    "                # Losses\n",
    "                # Actor loss\n",
    "                self.log_prob_actions = tf.math.log(self.prob_actions)\n",
    "\n",
    "                self.log_prob_chosen_action = tf.reduce_sum(self.log_prob_actions * self.action_OH, axis=1)\n",
    "                self.actor_loss = - tf.reduce_mean(self.log_prob_chosen_action * self.advantage)\n",
    "\n",
    "                # Critic loss\n",
    "                self.critic_loss = tf.reduce_mean(tf.square(self.target_value - self.value))\n",
    "\n",
    "                # Entropy: sum(p(x) * -log(p(x)))\n",
    "                self.entropy = tf.reduce_sum(tf.multiply(self.prob_actions, - self.log_prob_actions))\n",
    "\n",
    "                # TODO put coeffs as parameters\n",
    "                self.loss = 0.5 * self.critic_loss + self.actor_loss - 0.01 * self.entropy\n",
    "\n",
    "                self.optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "                self.train_op = self.optimizer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticGraphVectors:\n",
    "    \n",
    "    def __init__(self, state_size, nb_actions, learning_rate, scope_name):\n",
    "        self.state_size = state_size\n",
    "        self.nb_actions = nb_actions\n",
    "        self.learning_rate = learning_rate\n",
    "        self.scope_name = scope_name\n",
    "        \n",
    "        with tf.variable_scope(self.scope_name):\n",
    "            self.state = tf.placeholder(tf.float32, (None, *(self.state_size)), name=\"state\")\n",
    "            self.action = tf.placeholder(tf.uint8, (None,), name=\"action\")\n",
    "            self.action_OH = tf.one_hot(self.action, self.nb_actions, name=\"action_OH\")\n",
    "\n",
    " \n",
    "            self.target_value = tf.placeholder(tf.float32, (None,), name=\"target_value\")\n",
    "            self.advantage = tf.placeholder(tf.float32, (None,), name=\"advantage\")\n",
    "        \n",
    "            initializer = tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "            with tf.device('/device:GPU:0'):\n",
    "                # Neural net\n",
    "                self.flatten_state = tf.keras.layers.Flatten()(self.state)\n",
    "                self.hidden_layer_1 = tf.keras.layers.Dense(units=64,\n",
    "                                                            activation=tf.nn.relu,\n",
    "                                                            kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                                            name=\"hidden_layer_1\")(self.flatten_state)\n",
    "                \n",
    "                # Actor part\n",
    "                self.fc_actions = tf.keras.layers.Dense(units=32,\n",
    "                                                        activation=tf.nn.relu,\n",
    "                                                        kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                                        name=\"fc_action\")(self.hidden_layer_1)\n",
    "\n",
    "                self.prob_actions = tf.keras.layers.Dense(units=self.nb_actions,\n",
    "                                                          activation=tf.nn.softmax,\n",
    "                                                          kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                                          name=\"action_distribution\")(self.fc_actions)\n",
    "\n",
    "                # Critic part\n",
    "                self.fc_value = tf.keras.layers.Dense(\n",
    "                                                units=32,\n",
    "                                                activation=tf.nn.relu,\n",
    "                                                kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                                name=\"fc_value\")(self.hidden_layer_1)\n",
    "\n",
    "                self.value = tf.keras.layers.Dense(units=1,\n",
    "                                                   activation=None,\n",
    "                                                   kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                                   name=\"value\")(self.fc_value)\n",
    "            \n",
    "                # Losses\n",
    "                # Actor loss\n",
    "                self.log_prob_actions = tf.math.log(self.prob_actions)\n",
    "\n",
    "                self.log_prob_chosen_action = tf.reduce_sum(self.log_prob_actions * self.action_OH, axis=1)\n",
    "                self.actor_loss = - tf.reduce_mean(self.log_prob_chosen_action * self.advantage)\n",
    "\n",
    "                # Critic loss\n",
    "                self.critic_loss = tf.reduce_mean(tf.square(self.target_value - self.value))\n",
    "\n",
    "                # Entropy: sum(p(x) * -log(p(x)))\n",
    "                self.entropy = tf.reduce_sum(tf.multiply(self.prob_actions, - self.log_prob_actions))\n",
    "\n",
    "                # TODO put coeffs as parameters\n",
    "                self.loss = 0.5 * self.critic_loss + self.actor_loss - 0.01 * self.entropy\n",
    "\n",
    "                self.optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "                self.train_op = self.optimizer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticTrainer:\n",
    "    def __init__(self, graph, vec_env, nb_env, eval_env, gamma, state_size, ckpt_file):\n",
    "        self.graph = graph\n",
    "        self.vec_env = vec_env\n",
    "        self.nb_env = nb_env\n",
    "        self.eval_env = eval_env\n",
    "        self.nb_actions = self.eval_env.nb_actions\n",
    "        self.gamma = gamma\n",
    "        self.state_size = state_size\n",
    "        self.ckpt_file = ckpt_file\n",
    "        self.saver = tf.train.Saver()\n",
    "\n",
    "    def choose_actions(self, input_states, session):\n",
    "        prob_actions = session.run(self.graph.prob_actions,\n",
    "                                    feed_dict={self.graph.state: input_states})\n",
    "\n",
    "        actions = [np.random.choice(np.arange(self.nb_actions), p=p) for p in prob_actions]\n",
    "\n",
    "        return actions\n",
    "    \n",
    "    def get_values(self, input_states, session):\n",
    "        values = session.run(self.graph.value,\n",
    "                             feed_dict={self.graph.state: input_states})\n",
    "\n",
    "        return values.flatten()\n",
    "        \n",
    "    def choose_actions_and_get_values(self, input_states, session):\n",
    "        prob_actions, values = session.run([self.graph.prob_actions, self.graph.value],\n",
    "                                           feed_dict={self.graph.state: input_states})\n",
    "\n",
    "        actions = [np.random.choice(np.arange(self.nb_actions), p=p) for p in prob_actions]\n",
    "\n",
    "        return actions, values.flatten()\n",
    "        \n",
    "    def run_episode(self, env_to_run, max_step, render, session):\n",
    "\n",
    "        # Reset state\n",
    "        state = env_to_run.reset()        \n",
    "        total_reward = 0\n",
    "        \n",
    "        for step in range(max_step):\n",
    "            state = np.expand_dims(state, axis=0)\n",
    "            action = self.choose_actions(state, session)[0]\n",
    "\n",
    "            # Apply action to env and get next state, reward, and done bool\n",
    "            state, reward, done, _ = env_to_run.step(action, render)\n",
    "            total_reward += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        print(\"Reward on episode: %f\" % total_reward)\n",
    "        \n",
    "    def train_on_batch(self, states, actions, target_values, advantages, session):\n",
    "        loss, _ = session.run([self.graph.loss, self.graph.train_op],\n",
    "                              feed_dict={self.graph.state: states,\n",
    "                                         self.graph.action: actions,\n",
    "                                         self.graph.target_value: target_values,\n",
    "                                         self.graph.advantage: advantages})\n",
    "        \n",
    "        return loss\n",
    "         \n",
    "    def play_and_learn(self, n_iterations, steps_per_iteration,\n",
    "                       evaluate_every, save_every,\n",
    "                       ckpt_file, session):\n",
    "\n",
    "        # Reset states\n",
    "        states = self.vec_env.reset()       \n",
    "\n",
    "        states_all_env = np.empty((self.nb_env, steps_per_iteration, *self.state_size), dtype=np.float)\n",
    "        actions_all_env = np.empty((self.nb_env, steps_per_iteration), dtype=np.int)\n",
    "        rewards_all_env = np.empty((self.nb_env, steps_per_iteration), dtype=np.float)\n",
    "        values_all_env = np.empty((self.nb_env, steps_per_iteration), dtype=np.float)\n",
    "        dones_all_env = np.empty((self.nb_env, steps_per_iteration), dtype=np.bool)\n",
    "        \n",
    "        T = time.time()\n",
    "        for iteration in range(1, n_iterations):\n",
    "\n",
    "            for step in range(steps_per_iteration):\n",
    "                actions, values = self.choose_actions_and_get_values(states, session)\n",
    "                next_states, rewards, dones, _ = self.vec_env.step(actions)\n",
    "\n",
    "                clipped_rewards = np.clip(rewards, -1, 1)\n",
    "                                \n",
    "                states_all_env[:, step] = states\n",
    "                actions_all_env[:, step] = actions\n",
    "                rewards_all_env[:, step] = clipped_rewards\n",
    "                values_all_env[:, step] = values\n",
    "                dones_all_env[:, step] = dones\n",
    "                \n",
    "                states = next_states\n",
    "            \n",
    "            # Estimated values for the future\n",
    "            next_estimated_values = self.get_values(states, session)\n",
    "            next_estimated_values = np.where(dones, 0, next_estimated_values)\n",
    "\n",
    "            target_values_all_env = compute_target_values(np.array(rewards_all_env),\n",
    "                                                          next_estimated_values,\n",
    "                                                          np.array(dones_all_env),\n",
    "                                                          self.gamma)\n",
    "            \n",
    "            advantages_all_env = target_values_all_env - values_all_env\n",
    "            \n",
    "            # Concatenate the experiences from the different envionments\n",
    "            batch_states = np.concatenate(states_all_env)\n",
    "            batch_actions = np.concatenate(actions_all_env)\n",
    "            batch_target_values = np.concatenate(target_values_all_env)\n",
    "            batch_advantages = np.concatenate(advantages_all_env)\n",
    "            \n",
    "            loss = self.train_on_batch(batch_states, batch_actions, batch_target_values, batch_advantages, session)\n",
    "            \n",
    "            if iteration % evaluate_every == 0:\n",
    "                self.run_episode(self.eval_env, 10000, False, session)\n",
    "                print(\"Time to play %i iterations: %f\" %(iteration, time.time() - T))\n",
    "                \n",
    "            if iteration % save_every == 0:\n",
    "                self.saver.save(session, self.ckpt_file)\n",
    "                print(\"Saved model after %i iterations.\" % iteration)\n",
    "                \n",
    "    def restore(self, session, ckpt=None):\n",
    "        if ckpt is None:\n",
    "            ckpt = self.ckpt_file\n",
    "        self.saver.restore(session, ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "BrokenPipeError",
     "evalue": "[Errno 32] Broken pipe",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-8e8653196aa1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvec_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-40-209bc49d07a7>\u001b[0m in \u001b[0;36mclose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_remotes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'close'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_remotes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworker_remotes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/multiprocessing/connection.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_writable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrecv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_send_bytes\u001b[0;34m(self, buf)\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0;31m# Also note we want to avoid sending a 0-length buffer separately,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m             \u001b[0;31m# to avoid \"broken pipe\" errors if the other end closed the pipe.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheader\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_send\u001b[0;34m(self, buf, write)\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0mremaining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m             \u001b[0mremaining\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe"
     ]
    }
   ],
   "source": [
    "vec_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-110:\n",
      "Process Process-111:\n",
      "Process Process-112:\n",
      "Process Process-109:\n",
      "Process Process-104:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "Process Process-98:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Process Process-107:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-40-209bc49d07a7>\", line 4, in worker\n",
      "    cmd, data = worker_remote.recv()\n",
      "Traceback (most recent call last):\n",
      "Process Process-105:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "Process Process-99:\n",
      "  File \"<ipython-input-40-209bc49d07a7>\", line 4, in worker\n",
      "    cmd, data = worker_remote.recv()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"<ipython-input-40-209bc49d07a7>\", line 4, in worker\n",
      "    cmd, data = worker_remote.recv()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "Process Process-108:\n",
      "Process Process-101:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Process Process-97:\n",
      "Process Process-103:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"<ipython-input-40-209bc49d07a7>\", line 4, in worker\n",
      "    cmd, data = worker_remote.recv()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Process Process-106:\n",
      "  File \"<ipython-input-40-209bc49d07a7>\", line 4, in worker\n",
      "    cmd, data = worker_remote.recv()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"<ipython-input-40-209bc49d07a7>\", line 4, in worker\n",
      "    cmd, data = worker_remote.recv()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Process Process-102:\n",
      "Process Process-100:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"<ipython-input-40-209bc49d07a7>\", line 4, in worker\n",
      "    cmd, data = worker_remote.recv()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-40-209bc49d07a7>\", line 4, in worker\n",
      "    cmd, data = worker_remote.recv()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"<ipython-input-40-209bc49d07a7>\", line 4, in worker\n",
      "    cmd, data = worker_remote.recv()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-40-209bc49d07a7>\", line 4, in worker\n",
      "    cmd, data = worker_remote.recv()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"<ipython-input-40-209bc49d07a7>\", line 4, in worker\n",
      "    cmd, data = worker_remote.recv()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"<ipython-input-40-209bc49d07a7>\", line 4, in worker\n",
      "    cmd, data = worker_remote.recv()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"<ipython-input-40-209bc49d07a7>\", line 4, in worker\n",
      "    cmd, data = worker_remote.recv()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "  File \"<ipython-input-40-209bc49d07a7>\", line 4, in worker\n",
      "    cmd, data = worker_remote.recv()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"<ipython-input-40-209bc49d07a7>\", line 4, in worker\n",
      "    cmd, data = worker_remote.recv()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"<ipython-input-40-209bc49d07a7>\", line 4, in worker\n",
      "    cmd, data = worker_remote.recv()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "sz_to_process = (8,) #(110,84)\n",
    "stack_size = 4\n",
    "state_size = (*sz_to_process, stack_size)\n",
    "\n",
    "# frame_skipping = 1\n",
    "# max_of_n_frames = 1\n",
    "\n",
    "# Create the environments\n",
    "game_name = 'LunarLander-v2'  #'Pong-v0'  #'SpaceInvaders-v0'\n",
    "nb_env = 16\n",
    "env_list = [EnvWrapper(game_name, state_size) for _ in range(nb_env)]\n",
    "eval_env = EnvWrapper(game_name, state_size)\n",
    "\n",
    "vec_env = VecEnvWrapper(env_list)\n",
    "\n",
    "nb_actions = eval_env.nb_actions\n",
    "\n",
    "gamma = 0.95\n",
    "\n",
    "n_iterations = int(5e6)\n",
    "steps_per_iteration = 4\n",
    "learning_rate = 0.00075\n",
    "\n",
    "evaluate_every = 5000\n",
    "save_every = 50000\n",
    "\n",
    "ckpt_file = \"./models/model_lunarlander.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward on episode: -284.362040\n",
      "Time to play 2500 iterations: 54.322519\n",
      "Reward on episode: -120.596071\n",
      "Time to play 5000 iterations: 108.726586\n",
      "Reward on episode: -338.225197\n",
      "Time to play 7500 iterations: 170.163993\n",
      "Reward on episode: -343.937804\n",
      "Time to play 10000 iterations: 224.141732\n",
      "Reward on episode: -94.109414\n",
      "Time to play 12500 iterations: 281.556568\n",
      "Reward on episode: -61.415565\n",
      "Time to play 15000 iterations: 337.038483\n",
      "Reward on episode: -250.171947\n",
      "Time to play 17500 iterations: 391.037705\n",
      "Reward on episode: -268.277346\n",
      "Time to play 20000 iterations: 444.348269\n",
      "Reward on episode: -127.458783\n",
      "Time to play 22500 iterations: 498.406257\n",
      "Reward on episode: -54.626617\n",
      "Time to play 25000 iterations: 552.012539\n",
      "Saved model after 25000 iterations.\n",
      "Reward on episode: -171.482295\n",
      "Time to play 27500 iterations: 607.025943\n",
      "Reward on episode: -105.035661\n",
      "Time to play 30000 iterations: 660.760997\n",
      "Reward on episode: -214.092052\n",
      "Time to play 32500 iterations: 717.473870\n",
      "Reward on episode: -217.638373\n",
      "Time to play 35000 iterations: 773.829436\n",
      "Reward on episode: -438.582179\n",
      "Time to play 37500 iterations: 827.612452\n",
      "Reward on episode: -407.892876\n",
      "Time to play 40000 iterations: 881.640870\n",
      "Reward on episode: -62.402310\n",
      "Time to play 42500 iterations: 939.216227\n",
      "Reward on episode: -252.908253\n",
      "Time to play 45000 iterations: 993.603511\n",
      "Reward on episode: -317.502367\n",
      "Time to play 47500 iterations: 1047.525464\n",
      "Reward on episode: -121.316555\n",
      "Time to play 50000 iterations: 1100.891464\n",
      "Saved model after 50000 iterations.\n",
      "Reward on episode: -44.743863\n",
      "Time to play 52500 iterations: 1154.669129\n",
      "Reward on episode: -408.739379\n",
      "Time to play 55000 iterations: 1209.810480\n",
      "Reward on episode: -369.057306\n",
      "Time to play 57500 iterations: 1266.562642\n",
      "Reward on episode: -307.916592\n",
      "Time to play 60000 iterations: 1324.455061\n",
      "Reward on episode: -178.170697\n",
      "Time to play 62500 iterations: 1379.281937\n",
      "Reward on episode: -208.285077\n",
      "Time to play 65000 iterations: 1435.207312\n",
      "Reward on episode: -141.651062\n",
      "Time to play 67500 iterations: 1487.088015\n",
      "Reward on episode: -242.970686\n",
      "Time to play 70000 iterations: 1539.795556\n",
      "Reward on episode: -63.983014\n",
      "Time to play 72500 iterations: 1593.514237\n",
      "Reward on episode: 24.741790\n",
      "Time to play 75000 iterations: 1646.306636\n",
      "Saved model after 75000 iterations.\n",
      "Reward on episode: -249.404975\n",
      "Time to play 77500 iterations: 1698.143293\n",
      "Reward on episode: -117.186371\n",
      "Time to play 80000 iterations: 1760.608413\n",
      "Reward on episode: -147.675730\n",
      "Time to play 82500 iterations: 1819.093598\n",
      "Reward on episode: -511.005628\n",
      "Time to play 85000 iterations: 1873.749519\n",
      "Reward on episode: -483.321887\n",
      "Time to play 87500 iterations: 1928.424401\n",
      "Reward on episode: -167.190150\n",
      "Time to play 90000 iterations: 1983.554135\n",
      "Reward on episode: -131.886283\n",
      "Time to play 92500 iterations: 2040.349706\n",
      "Reward on episode: -411.384690\n",
      "Time to play 95000 iterations: 2095.991469\n",
      "Reward on episode: 78.103303\n",
      "Time to play 97500 iterations: 2150.204166\n",
      "Reward on episode: -69.568174\n",
      "Time to play 100000 iterations: 2204.959927\n",
      "Saved model after 100000 iterations.\n",
      "Reward on episode: -12.102919\n",
      "Time to play 102500 iterations: 2258.810309\n",
      "Reward on episode: -28.706120\n",
      "Time to play 105000 iterations: 2312.598980\n",
      "Reward on episode: -337.865810\n",
      "Time to play 107500 iterations: 2367.192087\n",
      "Reward on episode: -221.358562\n",
      "Time to play 110000 iterations: 2421.993394\n",
      "Reward on episode: -332.686360\n",
      "Time to play 112500 iterations: 2475.955014\n",
      "Reward on episode: -424.099792\n",
      "Time to play 115000 iterations: 2530.672529\n",
      "Reward on episode: -387.999943\n",
      "Time to play 117500 iterations: 2585.237330\n",
      "Reward on episode: -161.331080\n",
      "Time to play 120000 iterations: 2641.878037\n",
      "Reward on episode: -96.546977\n",
      "Time to play 122500 iterations: 2694.795665\n",
      "Reward on episode: -298.352050\n",
      "Time to play 125000 iterations: 2750.055976\n",
      "Saved model after 125000 iterations.\n",
      "Reward on episode: -203.744024\n",
      "Time to play 127500 iterations: 2803.852332\n",
      "Reward on episode: -78.226205\n",
      "Time to play 130000 iterations: 2859.530770\n",
      "Reward on episode: -80.330037\n",
      "Time to play 132500 iterations: 2912.977520\n",
      "Reward on episode: -50.742734\n",
      "Time to play 135000 iterations: 2966.429885\n",
      "Reward on episode: -250.407017\n",
      "Time to play 137500 iterations: 3021.084247\n",
      "Reward on episode: -133.161516\n",
      "Time to play 140000 iterations: 3074.255939\n",
      "Reward on episode: -135.112112\n",
      "Time to play 142500 iterations: 3129.647853\n",
      "Reward on episode: -281.432327\n",
      "Time to play 145000 iterations: 3191.039440\n",
      "Reward on episode: -70.598052\n",
      "Time to play 147500 iterations: 3244.784278\n",
      "Reward on episode: -304.250292\n",
      "Time to play 150000 iterations: 3299.342817\n",
      "Saved model after 150000 iterations.\n",
      "Reward on episode: -25.144129\n",
      "Time to play 152500 iterations: 3353.612126\n",
      "Reward on episode: -182.838562\n",
      "Time to play 155000 iterations: 3406.754015\n",
      "Reward on episode: -236.527993\n",
      "Time to play 157500 iterations: 3460.349364\n",
      "Reward on episode: -6.271337\n",
      "Time to play 160000 iterations: 3515.800906\n",
      "Reward on episode: -345.351998\n",
      "Time to play 162500 iterations: 3571.944851\n",
      "Reward on episode: -263.476994\n",
      "Time to play 165000 iterations: 3627.340250\n",
      "Reward on episode: -44.466565\n",
      "Time to play 167500 iterations: 3681.624640\n",
      "Reward on episode: -304.893695\n",
      "Time to play 170000 iterations: 3736.308653\n",
      "Reward on episode: -279.110495\n",
      "Time to play 172500 iterations: 3791.802944\n",
      "Reward on episode: -191.580134\n",
      "Time to play 175000 iterations: 3849.341338\n",
      "Saved model after 175000 iterations.\n",
      "Reward on episode: -191.740115\n",
      "Time to play 177500 iterations: 3903.340774\n",
      "Reward on episode: -55.092517\n",
      "Time to play 180000 iterations: 3957.014189\n",
      "Reward on episode: -86.807405\n",
      "Time to play 182500 iterations: 4011.464970\n",
      "Reward on episode: -288.864459\n",
      "Time to play 185000 iterations: 4067.596223\n",
      "Reward on episode: -144.369219\n",
      "Time to play 187500 iterations: 4122.405572\n",
      "Reward on episode: -30.288158\n",
      "Time to play 190000 iterations: 4177.437753\n",
      "Reward on episode: 129.794256\n",
      "Time to play 192500 iterations: 4233.227155\n",
      "Reward on episode: -252.934308\n",
      "Time to play 195000 iterations: 4290.649539\n",
      "Reward on episode: -110.145885\n",
      "Time to play 197500 iterations: 4347.518294\n",
      "Reward on episode: -46.661570\n",
      "Time to play 200000 iterations: 4404.139563\n",
      "Saved model after 200000 iterations.\n",
      "Reward on episode: 3.834026\n",
      "Time to play 202500 iterations: 4459.455477\n",
      "Reward on episode: -317.521551\n",
      "Time to play 205000 iterations: 4513.570270\n",
      "Reward on episode: -307.646092\n",
      "Time to play 207500 iterations: 4568.088535\n",
      "Reward on episode: 155.908191\n",
      "Time to play 210000 iterations: 4623.764644\n",
      "Reward on episode: -43.671278\n",
      "Time to play 212500 iterations: 4681.470161\n",
      "Reward on episode: -14.334728\n",
      "Time to play 215000 iterations: 4739.648621\n",
      "Reward on episode: -99.455222\n",
      "Time to play 217500 iterations: 4793.667369\n",
      "Reward on episode: -2.601075\n",
      "Time to play 220000 iterations: 4846.685227\n",
      "Reward on episode: 158.258452\n",
      "Time to play 222500 iterations: 4902.553734\n",
      "Reward on episode: -231.386243\n",
      "Time to play 225000 iterations: 4955.723296\n",
      "Saved model after 225000 iterations.\n",
      "Reward on episode: -61.641848\n",
      "Time to play 227500 iterations: 5011.157737\n",
      "Reward on episode: -241.549470\n",
      "Time to play 230000 iterations: 5069.767337\n",
      "Reward on episode: -180.685823\n",
      "Time to play 232500 iterations: 5126.670122\n",
      "Reward on episode: 4.386178\n",
      "Time to play 235000 iterations: 5181.904523\n",
      "Reward on episode: -238.805373\n",
      "Time to play 237500 iterations: 5253.349238\n",
      "Reward on episode: -8.634032\n",
      "Time to play 240000 iterations: 5313.355226\n",
      "Reward on episode: -218.278302\n",
      "Time to play 242500 iterations: 5373.570503\n",
      "Reward on episode: -78.725841\n",
      "Time to play 245000 iterations: 5430.605429\n",
      "Reward on episode: -291.626010\n",
      "Time to play 247500 iterations: 5485.716944\n",
      "Reward on episode: -46.131875\n",
      "Time to play 250000 iterations: 5539.261170\n",
      "Saved model after 250000 iterations.\n",
      "Reward on episode: -159.456612\n",
      "Time to play 252500 iterations: 5596.132514\n",
      "Reward on episode: -201.856378\n",
      "Time to play 255000 iterations: 5650.458566\n",
      "Reward on episode: -147.091424\n",
      "Time to play 257500 iterations: 5703.862817\n",
      "Reward on episode: -15.540514\n",
      "Time to play 260000 iterations: 5758.833091\n",
      "Reward on episode: -117.647737\n",
      "Time to play 262500 iterations: 5814.756706\n",
      "Reward on episode: -284.447786\n",
      "Time to play 265000 iterations: 5869.448887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward on episode: -32.785145\n",
      "Time to play 267500 iterations: 5924.541357\n",
      "Reward on episode: -203.733595\n",
      "Time to play 270000 iterations: 5979.052675\n",
      "Reward on episode: -272.870485\n",
      "Time to play 272500 iterations: 6034.891077\n",
      "Reward on episode: -90.264700\n",
      "Time to play 275000 iterations: 6088.552458\n",
      "Saved model after 275000 iterations.\n",
      "Reward on episode: -344.833594\n",
      "Time to play 277500 iterations: 6144.551343\n",
      "Reward on episode: -305.823787\n",
      "Time to play 280000 iterations: 6200.592257\n",
      "Reward on episode: -43.874572\n",
      "Time to play 282500 iterations: 6254.541942\n",
      "Reward on episode: -225.502186\n",
      "Time to play 285000 iterations: 6312.983519\n",
      "Reward on episode: -302.971653\n",
      "Time to play 287500 iterations: 6368.260840\n",
      "Reward on episode: -210.692024\n",
      "Time to play 290000 iterations: 6421.083219\n",
      "Reward on episode: -172.065633\n",
      "Time to play 292500 iterations: 6476.083141\n",
      "Reward on episode: -27.307611\n",
      "Time to play 295000 iterations: 6531.079514\n",
      "Reward on episode: -53.741802\n",
      "Time to play 297500 iterations: 6586.338591\n",
      "Reward on episode: -259.275727\n",
      "Time to play 300000 iterations: 6640.323496\n",
      "Saved model after 300000 iterations.\n",
      "Reward on episode: -421.171547\n",
      "Time to play 302500 iterations: 6697.524244\n",
      "Reward on episode: -89.665120\n",
      "Time to play 305000 iterations: 6754.131305\n",
      "Reward on episode: -67.746534\n",
      "Time to play 307500 iterations: 6808.372131\n",
      "Reward on episode: -79.301860\n",
      "Time to play 310000 iterations: 6863.857861\n",
      "Reward on episode: -198.292247\n",
      "Time to play 312500 iterations: 6918.804122\n",
      "Reward on episode: -450.506358\n",
      "Time to play 315000 iterations: 6972.778813\n",
      "Reward on episode: -388.688048\n",
      "Time to play 317500 iterations: 7026.400139\n",
      "Reward on episode: -223.244563\n",
      "Time to play 320000 iterations: 7080.561516\n",
      "Reward on episode: -296.856705\n",
      "Time to play 322500 iterations: 7135.597913\n",
      "Reward on episode: -242.254210\n",
      "Time to play 325000 iterations: 7189.921803\n",
      "Saved model after 325000 iterations.\n",
      "Reward on episode: -227.486466\n",
      "Time to play 327500 iterations: 7245.042463\n",
      "Reward on episode: -295.585053\n",
      "Time to play 330000 iterations: 7299.012950\n",
      "Reward on episode: -129.231937\n",
      "Time to play 332500 iterations: 7355.165895\n",
      "Reward on episode: -49.932342\n",
      "Time to play 335000 iterations: 7413.057800\n",
      "Reward on episode: -251.697527\n",
      "Time to play 337500 iterations: 7470.119484\n",
      "Reward on episode: 22.041696\n",
      "Time to play 340000 iterations: 7527.090283\n",
      "Reward on episode: -447.015422\n",
      "Time to play 342500 iterations: 7585.676809\n",
      "Reward on episode: -56.048690\n",
      "Time to play 345000 iterations: 7638.846402\n",
      "Reward on episode: -256.345715\n",
      "Time to play 347500 iterations: 7694.878821\n",
      "Reward on episode: -318.218340\n",
      "Time to play 350000 iterations: 7752.187100\n",
      "Saved model after 350000 iterations.\n",
      "Reward on episode: -109.085578\n",
      "Time to play 352500 iterations: 7808.178001\n",
      "Reward on episode: -28.136239\n",
      "Time to play 355000 iterations: 7863.895065\n",
      "Reward on episode: -41.490029\n",
      "Time to play 357500 iterations: 7917.627011\n",
      "Reward on episode: -324.591156\n",
      "Time to play 360000 iterations: 7972.214488\n",
      "Reward on episode: -36.100558\n",
      "Time to play 362500 iterations: 8028.813344\n",
      "Reward on episode: -267.881133\n",
      "Time to play 365000 iterations: 8084.689865\n",
      "Reward on episode: -382.178430\n",
      "Time to play 367500 iterations: 8139.543131\n",
      "Reward on episode: -213.109366\n",
      "Time to play 370000 iterations: 8195.270439\n",
      "Reward on episode: -319.524674\n",
      "Time to play 372500 iterations: 8251.077646\n",
      "Reward on episode: -69.890840\n",
      "Time to play 375000 iterations: 8304.631260\n",
      "Saved model after 375000 iterations.\n",
      "Reward on episode: -49.104515\n",
      "Time to play 377500 iterations: 8358.724108\n",
      "Reward on episode: -46.726853\n",
      "Time to play 380000 iterations: 8414.919460\n",
      "Reward on episode: -97.366091\n",
      "Time to play 382500 iterations: 8470.426752\n",
      "Reward on episode: 25.178387\n",
      "Time to play 385000 iterations: 8524.047293\n",
      "Reward on episode: 53.159260\n",
      "Time to play 387500 iterations: 8577.993947\n",
      "Reward on episode: -243.212208\n",
      "Time to play 390000 iterations: 8633.119788\n",
      "Reward on episode: -340.090994\n",
      "Time to play 392500 iterations: 8689.781785\n",
      "Reward on episode: -28.893116\n",
      "Time to play 395000 iterations: 8744.323040\n",
      "Reward on episode: -29.421664\n",
      "Time to play 397500 iterations: 8799.817282\n",
      "Reward on episode: -97.626881\n",
      "Time to play 400000 iterations: 8855.414677\n",
      "Saved model after 400000 iterations.\n",
      "Reward on episode: -127.616796\n",
      "Time to play 402500 iterations: 8913.394211\n",
      "Reward on episode: -88.296146\n",
      "Time to play 405000 iterations: 8968.957191\n",
      "Reward on episode: -158.183309\n",
      "Time to play 407500 iterations: 9024.601419\n",
      "Reward on episode: -79.515677\n",
      "Time to play 410000 iterations: 9078.598974\n",
      "Reward on episode: -307.709973\n",
      "Time to play 412500 iterations: 9132.624833\n",
      "Reward on episode: 14.004963\n",
      "Time to play 415000 iterations: 9189.328420\n",
      "Reward on episode: -201.992406\n",
      "Time to play 417500 iterations: 9241.822084\n",
      "Reward on episode: 24.070480\n",
      "Time to play 420000 iterations: 9294.717191\n",
      "Reward on episode: -394.115358\n",
      "Time to play 422500 iterations: 9348.375245\n",
      "Reward on episode: -179.910803\n",
      "Time to play 425000 iterations: 9411.972425\n",
      "Saved model after 425000 iterations.\n",
      "Reward on episode: -263.095081\n",
      "Time to play 427500 iterations: 9470.400030\n",
      "Reward on episode: -362.134176\n",
      "Time to play 430000 iterations: 9523.474662\n",
      "Reward on episode: -54.422354\n",
      "Time to play 432500 iterations: 9577.096761\n",
      "Reward on episode: 3.266514\n",
      "Time to play 435000 iterations: 9631.338084\n",
      "Reward on episode: -233.606822\n",
      "Time to play 437500 iterations: 9685.395330\n",
      "Reward on episode: -64.241319\n",
      "Time to play 440000 iterations: 9739.943166\n",
      "Reward on episode: -197.166538\n",
      "Time to play 442500 iterations: 9792.622092\n",
      "Reward on episode: -350.749549\n",
      "Time to play 445000 iterations: 9846.723516\n",
      "Reward on episode: -160.738874\n",
      "Time to play 447500 iterations: 9899.961010\n",
      "Reward on episode: -292.614695\n",
      "Time to play 450000 iterations: 9952.384042\n",
      "Saved model after 450000 iterations.\n",
      "Reward on episode: -70.649025\n",
      "Time to play 452500 iterations: 10005.396042\n",
      "Reward on episode: -98.426097\n",
      "Time to play 455000 iterations: 10057.203191\n",
      "Reward on episode: -221.102067\n",
      "Time to play 457500 iterations: 10108.952769\n",
      "Reward on episode: -102.301771\n",
      "Time to play 460000 iterations: 10161.251682\n",
      "Reward on episode: -235.486306\n",
      "Time to play 462500 iterations: 10213.659034\n",
      "Reward on episode: -78.418742\n",
      "Time to play 465000 iterations: 10266.000576\n",
      "Reward on episode: 2.517557\n",
      "Time to play 467500 iterations: 10319.373771\n",
      "Reward on episode: -242.312156\n",
      "Time to play 470000 iterations: 10370.149163\n",
      "Reward on episode: -60.462603\n",
      "Time to play 472500 iterations: 10421.299998\n",
      "Reward on episode: -282.427003\n",
      "Time to play 475000 iterations: 10474.652737\n",
      "Saved model after 475000 iterations.\n",
      "Reward on episode: -16.129427\n",
      "Time to play 477500 iterations: 10526.508634\n",
      "Reward on episode: -56.953373\n",
      "Time to play 480000 iterations: 10577.652747\n",
      "Reward on episode: -87.813465\n",
      "Time to play 482500 iterations: 10629.771643\n",
      "Reward on episode: -10.836599\n",
      "Time to play 485000 iterations: 10681.149069\n",
      "Reward on episode: -123.827334\n",
      "Time to play 487500 iterations: 10733.708710\n",
      "Reward on episode: -205.003898\n",
      "Time to play 490000 iterations: 10785.832994\n",
      "Reward on episode: -149.079417\n",
      "Time to play 492500 iterations: 10837.135766\n",
      "Reward on episode: -122.870244\n",
      "Time to play 495000 iterations: 10888.564996\n",
      "Reward on episode: -178.008603\n",
      "Time to play 497500 iterations: 10939.952843\n",
      "Reward on episode: -150.080554\n",
      "Time to play 500000 iterations: 10992.451578\n",
      "Saved model after 500000 iterations.\n",
      "Reward on episode: -98.989775\n",
      "Time to play 502500 iterations: 11044.244420\n",
      "Reward on episode: -360.189731\n",
      "Time to play 505000 iterations: 11101.145431\n",
      "Reward on episode: -41.783294\n",
      "Time to play 507500 iterations: 11177.118562\n",
      "Reward on episode: -82.265269\n",
      "Time to play 510000 iterations: 11233.875890\n",
      "Reward on episode: -355.593407\n",
      "Time to play 512500 iterations: 11287.709322\n",
      "Reward on episode: -212.789316\n",
      "Time to play 515000 iterations: 11342.250340\n",
      "Reward on episode: -420.876918\n",
      "Time to play 517500 iterations: 11396.775473\n",
      "Reward on episode: -193.131788\n",
      "Time to play 520000 iterations: 11453.313161\n",
      "Reward on episode: -206.642757\n",
      "Time to play 522500 iterations: 11510.655139\n",
      "Reward on episode: -186.789313\n",
      "Time to play 525000 iterations: 11566.562794\n",
      "Saved model after 525000 iterations.\n",
      "Reward on episode: -195.487636\n",
      "Time to play 527500 iterations: 11622.023318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward on episode: -96.258920\n",
      "Time to play 530000 iterations: 11677.725391\n",
      "Reward on episode: -71.892035\n",
      "Time to play 532500 iterations: 11733.614536\n",
      "Reward on episode: -412.695071\n",
      "Time to play 535000 iterations: 11788.357924\n",
      "Reward on episode: 1.713603\n",
      "Time to play 537500 iterations: 11842.096157\n",
      "Reward on episode: -267.949020\n",
      "Time to play 540000 iterations: 11897.865550\n",
      "Reward on episode: 24.313489\n",
      "Time to play 542500 iterations: 11954.380976\n",
      "Reward on episode: 0.220830\n",
      "Time to play 545000 iterations: 12009.958537\n",
      "Reward on episode: -194.014902\n",
      "Time to play 547500 iterations: 12063.890448\n",
      "Reward on episode: -312.989603\n",
      "Time to play 550000 iterations: 12118.423801\n",
      "Saved model after 550000 iterations.\n",
      "Reward on episode: -215.009939\n",
      "Time to play 552500 iterations: 12173.137539\n",
      "Reward on episode: -193.541236\n",
      "Time to play 555000 iterations: 12229.582396\n",
      "Reward on episode: -399.491416\n",
      "Time to play 557500 iterations: 12284.151261\n",
      "Reward on episode: -171.753617\n",
      "Time to play 560000 iterations: 12339.138448\n",
      "Reward on episode: -239.614529\n",
      "Time to play 562500 iterations: 12394.041956\n",
      "Reward on episode: -328.077183\n",
      "Time to play 565000 iterations: 12449.872430\n",
      "Reward on episode: -143.470814\n",
      "Time to play 567500 iterations: 12503.993056\n",
      "Reward on episode: -372.319210\n",
      "Time to play 570000 iterations: 12558.641429\n",
      "Reward on episode: -63.591633\n",
      "Time to play 572500 iterations: 12614.320246\n",
      "Reward on episode: -56.093339\n",
      "Time to play 575000 iterations: 12669.501555\n",
      "Saved model after 575000 iterations.\n",
      "Reward on episode: -20.184044\n",
      "Time to play 577500 iterations: 12725.199134\n",
      "Reward on episode: 20.377375\n",
      "Time to play 580000 iterations: 12778.589580\n",
      "Reward on episode: -158.601470\n",
      "Time to play 582500 iterations: 12833.363140\n",
      "Reward on episode: -224.712729\n",
      "Time to play 585000 iterations: 12888.714110\n",
      "Reward on episode: -134.308539\n",
      "Time to play 587500 iterations: 12943.181887\n",
      "Reward on episode: -87.868892\n",
      "Time to play 590000 iterations: 12996.528100\n",
      "Reward on episode: -77.409356\n",
      "Time to play 592500 iterations: 13049.926551\n",
      "Reward on episode: -6.904437\n",
      "Time to play 595000 iterations: 13105.016761\n",
      "Reward on episode: -96.175770\n",
      "Time to play 597500 iterations: 13160.220066\n",
      "Reward on episode: -222.498192\n",
      "Time to play 600000 iterations: 13214.486699\n",
      "Saved model after 600000 iterations.\n",
      "Reward on episode: 1.728970\n",
      "Time to play 602500 iterations: 13268.876963\n",
      "Reward on episode: -332.471268\n",
      "Time to play 605000 iterations: 13323.130368\n",
      "Reward on episode: -236.806858\n",
      "Time to play 607500 iterations: 13377.028415\n",
      "Reward on episode: -329.821927\n",
      "Time to play 610000 iterations: 13431.372508\n",
      "Reward on episode: -204.143427\n",
      "Time to play 612500 iterations: 13483.202511\n",
      "Reward on episode: -58.186720\n",
      "Time to play 615000 iterations: 13534.695549\n",
      "Reward on episode: -36.878490\n",
      "Time to play 617500 iterations: 13588.335317\n",
      "Reward on episode: -319.782301\n",
      "Time to play 620000 iterations: 13642.011422\n",
      "Reward on episode: -390.900844\n",
      "Time to play 622500 iterations: 13696.089011\n",
      "Reward on episode: -126.849807\n",
      "Time to play 625000 iterations: 13751.735589\n",
      "Saved model after 625000 iterations.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-64f411a8f66e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m     actrainer.play_and_learn(n_iterations, steps_per_iteration,\n\u001b[1;32m     22\u001b[0m                              \u001b[0mevaluate_every\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_every\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                              ckpt_file, sess)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-43-1280a9161601>\u001b[0m in \u001b[0;36mplay_and_learn\u001b[0;34m(self, n_iterations, steps_per_iteration, evaluate_every, save_every, ckpt_file, session)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps_per_iteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                 \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose_actions_and_get_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m                 \u001b[0mnext_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvec_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-43-1280a9161601>\u001b[0m in \u001b[0;36mchoose_actions_and_get_values\u001b[0;34m(self, input_states, session)\u001b[0m\n\u001b[1;32m     29\u001b[0m                                            feed_dict={self.graph.state: input_states})\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprob_actions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-43-1280a9161601>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     29\u001b[0m                                            feed_dict={self.graph.state: input_states})\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprob_actions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/numpy/core/getlimits.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, dtype)\u001b[0m\n\u001b[1;32m    363\u001b[0m     \u001b[0m_finfo_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__new__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m             \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumeric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "tf_config = tf.ConfigProto(allow_soft_placement=False)\n",
    "tf_config.gpu_options.allow_growth = True\n",
    "\n",
    "with tf.Session(config=tf_config) as sess:\n",
    "    acnet = ActorCriticGraphVectors(state_size, nb_actions, learning_rate, \"ACNet\")\n",
    "\n",
    "    actrainer = ActorCriticTrainer(acnet,\n",
    "                                   vec_env=vec_env, nb_env=nb_env,\n",
    "                                   eval_env=eval_env,\n",
    "                                   gamma=gamma,\n",
    "                                   state_size=state_size,\n",
    "                                   ckpt_file=ckpt_file)\n",
    "\n",
    "    # Setup TensorBoard\n",
    "    #writer = tf.summary.FileWriter(\"./tensorboard/\", sess.graph)\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #actrainer.restore(sess, ckpt=\"./models/model_pong.ckpt\")\n",
    "    actrainer.play_and_learn(n_iterations, steps_per_iteration,\n",
    "                             evaluate_every, save_every,\n",
    "                             ckpt_file, sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    acnet = ActorCriticGraph(state_size, nb_actions, learning_rate, \"ACNet\")\n",
    "    \n",
    "    actrainer = ActorCriticTrainer(acnet,\n",
    "                                   env=env,\n",
    "                                   eval_env=eval_env,\n",
    "                                   gamma=gamma,\n",
    "                                   state_size=state_size,\n",
    "                                   ckpt_file=ckpt_file)\n",
    "    \n",
    "    actrainer.restore(sess)\n",
    "    actrainer.run_episode(eval_env, 10000, True, sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
