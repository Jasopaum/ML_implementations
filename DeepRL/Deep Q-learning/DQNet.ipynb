{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from skimage import transform\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_screen(screen, sz_to_process):\n",
    "    gray = rgb2gray(screen)\n",
    "    cropped_screen = gray[8:-12,5:-12]  # For Space Invaders, TODO make it cleaner\n",
    "    \n",
    "    preprocessed_screen = transform.resize(cropped_screen, sz_to_process, mode='constant', anti_aliasing=True)\n",
    "    \n",
    "    return preprocessed_screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_frames(stacked_frames, last_2_screens, is_new_episode):\n",
    "    assert isinstance(stacked_frames, deque), \"stacked_frames has not type deque\"\n",
    "    sz_to_process = stacked_frames[0].shape\n",
    "    \n",
    "    max_screen = np.maximum(last_2_screens[0], last_2_screens[1])\n",
    "\n",
    "    frame = preprocess_screen(max_screen, sz_to_process)\n",
    "    \n",
    "    if is_new_episode:\n",
    "        for _ in range(len(stacked_frames)):\n",
    "            stacked_frames.append(frame)\n",
    "    else:\n",
    "        stacked_frames.append(frame)\n",
    "        \n",
    "    input_state = np.stack(stacked_frames, axis=2)\n",
    "                    \n",
    "    return input_state, stacked_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvWrapper:\n",
    "    # Do not have to handle stacked frames externally\n",
    "    \n",
    "    def __init__(self, game_name, state_size, frame_skipping):\n",
    "        self.env = gym.make(game_name)\n",
    "        self.stacked_frames = deque([np.zeros(state_size[:-1]) for _ in range(state_size[-1])],\n",
    "                                    maxlen=state_size[-1])\n",
    "        self.nb_actions = self.env.action_space.n\n",
    "        self.frame_skipping = frame_skipping\n",
    "        self.last_2_screens = deque(maxlen=2)\n",
    "        \n",
    "    def step(self, action, render=False):\n",
    "        for _ in range(self.frame_skipping):\n",
    "            screen, reward, done, info = self.env.step(action)\n",
    "            self.last_2_screens.append(screen)\n",
    "            if render:\n",
    "                self.render()\n",
    "            if done:\n",
    "                break\n",
    "        if done:\n",
    "            # Reset env and state\n",
    "            screen = self.env.reset()\n",
    "            for _ in range(2):\n",
    "                self.last_2_screens.append(screen)\n",
    "            stacked_state, self.stacked_frames = stack_frames(self.stacked_frames, self.last_2_screens, True)\n",
    "        else:\n",
    "            stacked_state, self.stacked_frames = stack_frames(self.stacked_frames, self.last_2_screens, False)\n",
    "\n",
    "        return stacked_state, reward, done, info\n",
    "            \n",
    "    def reset(self):\n",
    "        screen = self.env.reset()\n",
    "        for _ in range(2):\n",
    "            self.last_2_screens.append(screen)\n",
    "        stacked_state, self.stacked_frames = stack_frames(self.stacked_frames, self.last_2_screens, True)\n",
    "        \n",
    "        return stacked_state\n",
    "        \n",
    "    def render(self):\n",
    "        self.env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumTree():\n",
    "        \n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.data = np.zeros(capacity, dtype=object)\n",
    "        self.tree = np.zeros(2 * capacity - 1)\n",
    "        self.pointer = 0\n",
    "                \n",
    "    def add(self, data, p):\n",
    "        idx_data = self.pointer\n",
    "        idx_tree = self.pointer + self.capacity - 1\n",
    "                \n",
    "        self.data[idx_data] = data\n",
    "\n",
    "        diff_p = p - self.tree[idx_tree]\n",
    "        self.tree[idx_tree] += diff_p\n",
    "        \n",
    "        while idx_tree != 0:\n",
    "            idx_tree = (idx_tree - 1) // 2\n",
    "            self.tree[idx_tree] += diff_p\n",
    "        \n",
    "        self.pointer = (self.pointer + 1) % self.capacity\n",
    "        \n",
    "    def update(self, idx_data, p):\n",
    "        idx_tree = idx_data + self.capacity - 1\n",
    "\n",
    "        diff_p = p - self.tree[idx_tree]\n",
    "        self.tree[idx_tree] += diff_p\n",
    "        \n",
    "        while idx_tree != 0:\n",
    "            idx_tree = (idx_tree - 1) // 2\n",
    "            self.tree[idx_tree] += diff_p\n",
    "            \n",
    "    def get(self, s):\n",
    "        cur_idx = 0\n",
    "        while cur_idx < self.capacity - 1:\n",
    "            left_idx = 2 * cur_idx + 1\n",
    "            right_idx = left_idx + 1\n",
    "            \n",
    "            if self.tree[left_idx] >= s:\n",
    "                cur_idx = left_idx\n",
    "            else:\n",
    "                cur_idx = right_idx\n",
    "                s -= self.tree[left_idx]\n",
    "                \n",
    "        idx_data = cur_idx - (self.capacity - 1)\n",
    "        return idx_data, self.data[idx_data], self.tree[cur_idx]\n",
    "    \n",
    "    def max_p_leaf(self):\n",
    "        return np.max(self.tree[-self.capacity:])\n",
    "    \n",
    "    def total(self):\n",
    "        return self.tree[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory():\n",
    "        \n",
    "    def __init__(self, max_size, PER_epsilon, PER_alpha, PER_beta, PER_beta_increment):\n",
    "        self.sumtree = SumTree(max_size)\n",
    "        \n",
    "        self.PER_epsilon = PER_epsilon\n",
    "        self.PER_alpha = PER_alpha\n",
    "        self.PER_beta = PER_beta\n",
    "\n",
    "        self.PER_beta_increment = PER_beta_increment\n",
    "        \n",
    "    def add(self, experience):\n",
    "        priority = self.sumtree.max_p_leaf()\n",
    "        if priority == 0:\n",
    "            priority = self.PER_epsilon\n",
    "        self.sumtree.add(experience, priority)\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        tot_sumtree = self.sumtree.total()\n",
    "        p_step = tot_sumtree / batch_size\n",
    "        \n",
    "        indices_data = np.empty(batch_size, dtype=np.int)\n",
    "        experiences = np.empty(batch_size, dtype=object)\n",
    "        is_weights = np.empty(batch_size, dtype=np.float)\n",
    "        \n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            # Sample value ar random in segment\n",
    "            a, b = i * p_step,  (i + 1) * p_step\n",
    "            value = np.random.uniform(a, b)\n",
    "            \n",
    "            # Get experience from the sumtree\n",
    "            idx_data, experience, priority = self.sumtree.get(value)\n",
    "            \n",
    "            # Compute IS weight: (1/a)^b = a^(-b) \n",
    "            is_w = (batch_size * (priority / tot_sumtree)) ** (-self.PER_beta)\n",
    "            \n",
    "            indices_data[i] = idx_data\n",
    "            experiences[i] = experience\n",
    "            is_weights[i] = is_w\n",
    "            \n",
    "        is_weights /= np.max(is_weights)\n",
    "        \n",
    "        # Anneal beta towards 1\n",
    "        self.PER_beta = min(self.PER_beta + self.PER_beta_increment, 1)\n",
    "\n",
    "        return indices_data, experiences, is_weights\n",
    "    \n",
    "    def update_priorities(self, indices_data, errors):\n",
    "        errors += self.PER_epsilon\n",
    "        errors = errors ** self.PER_alpha\n",
    "        for i, e in zip(indices_data, errors):\n",
    "            self.sumtree.update(i, e)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQGraph:\n",
    "    \n",
    "    def __init__(self, state_size, nb_actions, learning_rate, scope_name):\n",
    "        self.state_size = state_size\n",
    "        self.nb_actions = nb_actions\n",
    "        self.learning_rate = learning_rate\n",
    "        self.scope_name = scope_name\n",
    "        \n",
    "        with tf.variable_scope(self.scope_name):\n",
    "            self.inputs = tf.placeholder(tf.float32, (None, *(self.state_size)), name=\"inputs\")\n",
    "            self.action = tf.placeholder(tf.uint8, (None,), name=\"action\")\n",
    "            self.action_OH = tf.one_hot(self.action, self.nb_actions, name=\"action_OH\")\n",
    "            \n",
    "            self.target_Q = tf.placeholder(tf.float32, (None,), name=\"target_Q\")\n",
    "            \n",
    "            self.is_weight = tf.placeholder(tf.float32, (None,), name=\"is_weight\")\n",
    "            \n",
    "            # Neural net\n",
    "            self.conv1 = tf.layers.conv2d(inputs=self.inputs,\n",
    "                                          filters=32,\n",
    "                                          kernel_size=(8,8),\n",
    "                                          strides=(4,4),\n",
    "                                          padding=\"valid\",\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                          name=\"conv1\")\n",
    "            self.conv1_out = tf.nn.elu(self.conv1, name=\"conv1_out\")\n",
    "            \n",
    "            self.conv2 = tf.layers.conv2d(inputs=self.conv1_out,\n",
    "                                          filters=64,\n",
    "                                          kernel_size=(4,4),\n",
    "                                          strides=(2,2),\n",
    "                                          padding=\"valid\",\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                          name=\"conv2\")\n",
    "            self.conv2_out = tf.nn.elu(self.conv2, name=\"conv2_out\")            \n",
    "            \n",
    "            self.conv3 = tf.layers.conv2d(inputs=self.conv2_out,\n",
    "                                          filters=64,\n",
    "                                          kernel_size=(3,3),\n",
    "                                          strides=(2,2),\n",
    "                                          padding=\"valid\",\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                          name=\"conv3\")\n",
    "            self.conv3_out = tf.nn.elu(self.conv3, name=\"conv3_out\")\n",
    "            \n",
    "            self.flatten = tf.layers.flatten(self.conv3_out)\n",
    "            \n",
    "            # Dueling DQNet, we separate the network in 2 streams.\n",
    "            # One computing V(s)\n",
    "            self.fc_value = tf.layers.dense(self.flatten,\n",
    "                                            units=512,\n",
    "                                            activation=tf.nn.elu,\n",
    "                                            kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                            name=\"fc_value\")\n",
    "            \n",
    "            self.value = tf.layers.dense(self.fc_value,\n",
    "                                         units=1,\n",
    "                                         activation=None,\n",
    "                                         kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                         name=\"value\")\n",
    "            \n",
    "            # The other computing A(s,a)\n",
    "            self.fc_advantages = tf.layers.dense(self.flatten,\n",
    "                                                 units=512,\n",
    "                                                 activation=tf.nn.elu,\n",
    "                                                 kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                                 name=\"fc_advantages\")\n",
    "            \n",
    "            self.advantages = tf.layers.dense(self.fc_advantages,\n",
    "                                              units=self.nb_actions,\n",
    "                                              activation=None,\n",
    "                                              kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                              name=\"advantages\")\n",
    "            \n",
    "            self.output = self.value + (self.advantages - tf.reduce_mean(self.advantages, axis=1, keepdims=True))\n",
    "            \n",
    "            # Prediction of the Q value\n",
    "            self.pred_Q = tf.reduce_sum(tf.multiply(self.output, self.action_OH), 1)\n",
    "            \n",
    "            # Error for PER\n",
    "            self.errors = tf.abs(self.pred_Q - self.target_Q)\n",
    "            \n",
    "            self.loss = tf.reduce_mean(self.is_weight * tf.math.square(self.errors))\n",
    "            \n",
    "            self.optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "            self.optimizer = tf.contrib.estimator.clip_gradients_by_norm(self.optimizer, clip_norm=1.)\n",
    "            \n",
    "            #tf.summary.scalar(\"grad_norm\", self.optimizer.compute_gradients(self.loss))\n",
    "            \n",
    "            self.train_op = self.optimizer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQTrainer:\n",
    "    \n",
    "    def __init__(self,\n",
    "                 dqnet, dqtarget,\n",
    "                 env, eval_env,\n",
    "                 nb_actions, gamma, exp_memory,\n",
    "                 ckpt_file):\n",
    "        \n",
    "        self.dqnet = dqnet\n",
    "        self.dqtarget = dqtarget\n",
    "        self.env = env\n",
    "        self.eval_env = eval_env\n",
    "        self.nb_actions = nb_actions\n",
    "        self.gamma = gamma\n",
    "        self.exp_memory = exp_memory\n",
    "        self.ckpt_file = ckpt_file\n",
    "        self.saver = tf.train.Saver()\n",
    "        \n",
    "    def choose_action(self, input_state, epsilon, session):\n",
    "        # Epsilon-greedy strategy\n",
    "        if np.random.rand() > epsilon:\n",
    "            outputs = session.run(self.dqnet.output,\n",
    "                                  feed_dict={self.dqnet.inputs: np.expand_dims(input_state, axis=0)})\n",
    "            action = np.argmax(outputs)\n",
    "        else:\n",
    "            action = np.random.choice(np.arange(nb_actions))\n",
    "\n",
    "        return action\n",
    "\n",
    "\n",
    "    def run_episode(self, env_to_run, max_step, session, render=False, store_in_memory=True, epsilon=0):        \n",
    "        state = env_to_run.reset()\n",
    "\n",
    "        total_reward = 0\n",
    "\n",
    "        for step in range(max_step):\n",
    "            action = self.choose_action(state, epsilon, session)\n",
    "\n",
    "            # Apply action to env and get next state, reward, and done bool\n",
    "            next_state, reward, done, _ = env_to_run.step(action, render)\n",
    "            clipped_reward = np.clip(reward, -1, 1)\n",
    "\n",
    "            if store_in_memory:\n",
    "                self.exp_memory.add((state, action, clipped_reward, next_state, done))\n",
    "\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        print(\"Reward on episode: %f\" % total_reward)\n",
    " \n",
    "    def train_on_batch(self, batch_size, session):\n",
    "        \n",
    "        indices_data, experiences, is_weights = self.exp_memory.sample(batch_size)\n",
    "\n",
    "        states = [el[0] for el in experiences]\n",
    "        actions = [el[1] for el in experiences]\n",
    "        rewards = [el[2] for el in experiences]\n",
    "        next_states = [el[3] for el in experiences]\n",
    "        done = [el[4] for el in experiences]\n",
    "        \n",
    "        # Double DQN mechanism\n",
    "        # First run to compute Q values for next state with both networks\n",
    "        Q_vals_next = sess.run(self.dqnet.output,\n",
    "                               feed_dict={self.dqnet.inputs: next_states})\n",
    "        \n",
    "        Q_vals_next_target = sess.run(self.dqtarget.output,\n",
    "                                      feed_dict={self.dqtarget.inputs: next_states})\n",
    "        \n",
    "        # Compute target Q values (Bellman equation)\n",
    "        target_Qs = np.empty(batch_size)\n",
    "        for step in range(batch_size):\n",
    "            if done[step]:\n",
    "                target_Qs[step] = rewards[step]\n",
    "            else:\n",
    "                best_action_net = np.argmax(Q_vals_next[step])\n",
    "                target_Qs[step] = rewards[step] + self.gamma * Q_vals_next_target[step][best_action_net]\n",
    "\n",
    "        # Second run to optimize\n",
    "        _, loss, errors = sess.run([self.dqnet.train_op, self.dqnet.loss, self.dqnet.errors],\n",
    "                                   feed_dict={self.dqnet.inputs: states,\n",
    "                                              self.dqnet.action: actions,\n",
    "                                              self.dqnet.target_Q: target_Qs,\n",
    "                                              self.dqnet.is_weight: is_weights})\n",
    "        \n",
    "        # Update priorities in Memory\n",
    "        self.exp_memory.update_priorities(indices_data, errors)\n",
    "        \n",
    "        \n",
    "    def play_and_learn(self, batch_size, learning_rate, total_steps,\n",
    "                       epsilon_start, epsilon_decay, do_decay_espilon_every,\n",
    "                       replay_period, t_upd_target,\n",
    "                       frame_skipping,\n",
    "                       evaluate_after, save_after,\n",
    "                       session):\n",
    "        \n",
    "        epsilon = epsilon_start\n",
    "        \n",
    "        # Reset state\n",
    "        state = self.env.reset()\n",
    "\n",
    "        T = time.time()\n",
    "        for step in range(1, total_steps + 1):\n",
    "            action = self.choose_action(state, epsilon, session)\n",
    "\n",
    "            # Apply action to env and get next state, reward, and done bool\n",
    "            next_state, reward, done, _ = self.env.step(action)\n",
    "            clipped_reward = np.clip(reward, -1, 1)\n",
    "\n",
    "            self.exp_memory.add((state, action, clipped_reward, next_state, done))\n",
    "\n",
    "            state = next_state\n",
    "            \n",
    "            if step % do_decay_espilon_every == 0:\n",
    "                epsilon *= epsilon_decay\n",
    "            \n",
    "            if step % replay_period == 0:\n",
    "                self.train_on_batch(batch_size, session)\n",
    "                            \n",
    "            if step % t_upd_target == 0:\n",
    "                upd_ops = self.update_target()\n",
    "                session.run(upd_ops)\n",
    "                #print(\"Updated target weights\")\n",
    "                \n",
    "            if step % evaluate_after == 0:\n",
    "                self.run_episode(self.eval_env, 10000, session, store_in_memory=False)\n",
    "                print(\"DEBUG:\\n\\tepsilon = %f\" % epsilon)\n",
    "                print(\"Time to play %i steps: %f\" %(step, time.time() - T))\n",
    "                    \n",
    "            if step % save_after == 0:\n",
    "                self.saver.save(sess, \"./models/model.ckpt\")\n",
    "                print(\"Saved model after %i steps.\" % step)        \n",
    "        \n",
    "        \n",
    "    def update_target(self):\n",
    "        # Get the parameters of the DQNNet\n",
    "        from_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, dqnet.scope_name)\n",
    "\n",
    "        # Get the parameters of the DQTarget\n",
    "        to_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, dqtarget.scope_name)\n",
    "\n",
    "        op_holder = []\n",
    "\n",
    "        # Update our target_network parameters with DQNNetwork parameters\n",
    "        for from_var, to_var in zip(from_vars,to_vars):\n",
    "            op_holder.append(to_var.assign(from_var))\n",
    "        return op_holder\n",
    "    \n",
    "    \n",
    "    def restore(self, session):\n",
    "        self.saver.restore(session, self.ckpt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz_to_process = (110,84)\n",
    "stack_size = 4\n",
    "state_size = (*sz_to_process, stack_size)\n",
    "\n",
    "frame_skipping = 2\n",
    "\n",
    "# Create the environments\n",
    "env = EnvWrapper('SpaceInvaders-v0', state_size, frame_skipping)\n",
    "eval_env = EnvWrapper('SpaceInvaders-v0', state_size, frame_skipping)\n",
    "\n",
    "nb_actions = env.nb_actions\n",
    "\n",
    "max_mem_size = int(1e4)\n",
    "\n",
    "gamma = 0.95\n",
    "\n",
    "batch_size = 32\n",
    "learning_rate = 0.00025\n",
    "total_steps = int(1e6)\n",
    "\n",
    "epsilon_start = 1.\n",
    "epsilon_decay = 0.997\n",
    "do_decay_espilon_every = total_steps / 1e4\n",
    "\n",
    "replay_period = 4\n",
    "t_upd_target = 5000\n",
    "evaluate_after = 500\n",
    "save_after = 20000\n",
    "\n",
    "PER_epsilon = 0.01\n",
    "PER_alpha = 0.6\n",
    "PER_beta = 0.5\n",
    "PER_beta_increment = PER_beta / (total_steps / replay_period)\n",
    "\n",
    "ckpt_file = \"./models/model.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_memory = Memory(max_mem_size, PER_epsilon, PER_alpha, PER_beta, PER_beta_increment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    dqnet = DQGraph(state_size=state_size,\n",
    "                    nb_actions=nb_actions,\n",
    "                    learning_rate=learning_rate,\n",
    "                    scope_name='DQNet')\n",
    "    dqtarget = DQGraph(state_size=state_size,\n",
    "                       nb_actions=nb_actions,\n",
    "                       learning_rate=learning_rate,\n",
    "                       scope_name='DQTarget')\n",
    "\n",
    "    dqtrainer = DQTrainer(dqnet, dqtarget,\n",
    "                          env=env,\n",
    "                          eval_env=eval_env,\n",
    "                          nb_actions=nb_actions,\n",
    "                          gamma=gamma,\n",
    "                          exp_memory=exp_memory,\n",
    "                          ckpt_file=ckpt_file)\n",
    "    \n",
    "    # Setup TensorBoard\n",
    "    writer = tf.summary.FileWriter(\"./tensorboard/\", sess.graph)\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Fill memory\n",
    "    dqtrainer.run_episode(env,\n",
    "                          5000,\n",
    "                          sess, render=False, epsilon=1.)\n",
    "    dqtrainer.play_and_learn(batch_size, learning_rate, total_steps,\n",
    "                             1., epsilon_decay, do_decay_espilon_every,\n",
    "                             replay_period, t_upd_target,\n",
    "                             frame_skipping,\n",
    "                             evaluate_after, save_after,\n",
    "                             session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    dqnet = DQGraph(state_size=state_size,\n",
    "                    nb_actions=nb_actions,\n",
    "                    learning_rate=learning_rate,\n",
    "                    scope_name='DQNet')\n",
    "    dqtarget = DQGraph(state_size=state_size,\n",
    "                       nb_actions=nb_actions,\n",
    "                       learning_rate=learning_rate,\n",
    "                       scope_name='DQTarget')\n",
    "\n",
    "    dqtrainer = DQTrainer(dqnet, dqtarget,\n",
    "                          env=env,\n",
    "                          possible_actions=possible_actions,\n",
    "                          gamma=gamma,\n",
    "                          exp_memory=exp_memory,\n",
    "                          state_size=state_size)\n",
    "    \n",
    "    dqtrainer.restore(sess)\n",
    "    dqtrainer.run_episode(10000, 1, sess, render=True, epsilon=0.)\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
