{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "from collections import deque, defaultdict\n",
    "import string\n",
    "import math\n",
    "from random import shuffle\n",
    "\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_corpus = nltk.corpus.gutenberg.fileids()\n",
    "corpus = \"../data/small_wiki_en/*\"\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop_through_nltk_corpus(corpus):\n",
    "    # Loop on each file\n",
    "    for doc in corpus:\n",
    "        for w in nltk.corpus.gutenberg.words(doc):\n",
    "            # Set to lower case\n",
    "            w = w.lower()\n",
    "            # Do not process stop words and punctuation\n",
    "            if w in stop_words or w in string.punctuation:\n",
    "                continue\n",
    "            yield w\n",
    "        yield '<ENDOFDOC>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop_through_corpus(corpus):\n",
    "    # Loop on each file\n",
    "    for file in glob.glob(corpus):\n",
    "        print(\"Processing file %s...\" % file)\n",
    "        lines =  open(file, encoding='ISO-8859-1').readlines()\n",
    "        for l in lines:\n",
    "            if l[0] == '<':\n",
    "                continue\n",
    "            elif 'ENDOFARTICLE' in l:\n",
    "                yield '<ENDOFDOC>'\n",
    "            for w in tokenizer.tokenize(l):\n",
    "                # Set to lower case\n",
    "                w = w.lower()\n",
    "                yield w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_corpus(corpus, stop_words):\n",
    "    word2id = {}\n",
    "    id2word = []\n",
    "    n_words = 0\n",
    "    word_freq = defaultdict(int)\n",
    "    \n",
    "    for w in loop_through_corpus(corpus):\n",
    "        word_freq[w] += 1\n",
    "\n",
    "    for w in loop_through_corpus(corpus):\n",
    "        if w not in word2id and w != '<ENDOFDOC>' and word_freq[w] > 10 and w not in stop_words:\n",
    "            word2id[w] = n_words\n",
    "            id2word.append(w)\n",
    "            n_words += 1\n",
    "    \n",
    "    del word_freq\n",
    "    return word2id, id2word, n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_cooccurences(corpus, word2id, window_size):\n",
    "    cur_context = deque(maxlen=window_size)\n",
    "    for w in loop_through_corpus(corpus):\n",
    "        if w not in word2id or w in stop_words:\n",
    "            continue\n",
    "        if w == '<ENDOFDOC>':\n",
    "            cur_context.clear()\n",
    "            continue\n",
    "        for i in range(len(cur_context)):\n",
    "            c = cur_context[i]\n",
    "            d = len(cur_context) - i\n",
    "            yield word2id[w], word2id[c], d\n",
    "        cur_context.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hash_key(w, c):\n",
    "    bits_min = min(w, c) << 16\n",
    "    bits_max = max(w, c)\n",
    "    return bits_min + bits_max\n",
    "    \n",
    "def build_cooccur_hashtable(corpus, word2id, window_size=5):\n",
    "    coocur_hashtable = defaultdict(float)\n",
    "    for (w, c, d) in gen_cooccurences(corpus, word2id, window_size):\n",
    "        hash_key = get_hash_key(w, c)\n",
    "        coocur_hashtable[hash_key] += 1 / d\n",
    "    return coocur_hashtable\n",
    "\n",
    "def get_cooccurence_value(table, w, c):\n",
    "    hash_key = get_hash_key(w, c)\n",
    "    return table[hash_key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_data(coocur_table):\n",
    "    all_data = []\n",
    "    for key, value in coocur_table.items():\n",
    "        if value < 2:\n",
    "            continue\n",
    "        w = key >> 16\n",
    "        c = key - (w << 16)\n",
    "        all_data.append((w, c, value))\n",
    "        all_data.append((c, w, value))\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_func(x, xmax=100, alpha=3/4):\n",
    "    return min(1, (x/xmax) ** alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO check how AdaGrad works\n",
    "#TODO Check initialization vectors\n",
    "#TODO Check training procedure (how to sample cooccurences? What about zeros in cooc?)\n",
    "def train_glove(cooccur_list, voc_size, embed_size, learning_rate, n_epoch,\n",
    "                init_W_word=None, init_b_word=None, init_W_context=None, init_b_context=None):\n",
    "    \n",
    "    W_word    = 0.1 * np.random.rand(voc_size, embed_size) - 0.05 if init_W_word    is None else init_W_word\n",
    "    b_word    = 0.1 * np.random.rand(voc_size)             - 0.05 if init_b_word    is None else init_b_word\n",
    "    W_context = 0.1 * np.random.rand(voc_size, embed_size) - 0.05 if init_W_context is None else init_W_context\n",
    "    b_context = 0.1 * np.random.rand(voc_size)             - 0.05 if init_b_context is None else init_b_context\n",
    "    \n",
    "    #sum_gradsq_W_word    = np.zeros(embed_size)\n",
    "    #sum_gradsq_b_word    = 0.\n",
    "    #sum_gradsq_W_context = np.zeros(embed_size)\n",
    "    #sum_gradsq_b_context = 0.\n",
    "    #epsilon = 1e-6\n",
    "    \n",
    "    common_grad = np.float64()  # Overflow can occur with 32 bits\n",
    "    \n",
    "    for ep in range(1, n_epoch + 1):\n",
    "        print(\"Start epoch %i\" % ep)\n",
    "        total_cost = 0\n",
    "        shuffle(cooccur_list)\n",
    "        for (w, c, cooc_value) in cooccur_list:\n",
    "            weighted_cooc_value = weight_func(cooc_value)\n",
    "            \n",
    "            common_grad = np.dot(W_word[w], W_context[c]) + b_word[w] + b_context[c] - math.log(cooc_value)\n",
    "            \n",
    "            grad_W_word    = weighted_cooc_value * W_context[c] * common_grad  # factor 2 in learning rate\n",
    "            grad_b_word    = weighted_cooc_value * common_grad\n",
    "            grad_W_context = weighted_cooc_value * W_word[w] * common_grad\n",
    "            grad_b_context = weighted_cooc_value * common_grad\n",
    "\n",
    "            W_word[w]    -= learning_rate * grad_W_word #/ np.sqrt(sum_gradsq_W_word + epsilon)\n",
    "            b_word[w]    -= learning_rate * grad_b_word #/ math.sqrt(sum_gradsq_b_word + epsilon)\n",
    "            W_context[c] -= learning_rate * grad_W_context #/ np.sqrt(sum_gradsq_W_context + epsilon)\n",
    "            b_context[c] -= learning_rate * grad_b_context #/ math.sqrt(sum_gradsq_b_context + epsilon)\n",
    "            \n",
    "            #sum_gradsq_W_word    += np.square(grad_W_word)\n",
    "            #sum_gradsq_b_word    += grad_b_word ** 2\n",
    "            #sum_gradsq_W_context += np.square(grad_W_context)\n",
    "            #sum_gradsq_b_context += grad_b_context ** 2\n",
    "            \n",
    "            total_cost += weighted_cooc_value * (common_grad ** 2)\n",
    "            \n",
    "        # Test embeddings\n",
    "        embeddings = W_word + W_context\n",
    "\n",
    "        print(\"Total cost for epoch: %f\" % total_cost)\n",
    "        \n",
    "        print(\"Distance between queen and king: %f\" % cosine(embeddings[word2id['queen']], embeddings[word2id['king']]))\n",
    "        print(\"Distance between blue and green: %f\" % cosine(embeddings[word2id['blue']], embeddings[word2id['green']]))\n",
    "        print(\"Distance between good and imagine: %f\" % cosine(embeddings[word2id['good']], embeddings[word2id['imagine']]))\n",
    "        \n",
    "    return embeddings, W_word, b_word, W_context, b_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file ../data/small_wiki_en/englishText_10000_20000...\n",
      "Processing file ../data/small_wiki_en/englishText_0_10000...\n",
      "Processing file ../data/small_wiki_en/englishText_60000_70000...\n",
      "Processing file ../data/small_wiki_en/englishText_50000_60000...\n",
      "Processing file ../data/small_wiki_en/englishText_30000_40000...\n",
      "Processing file ../data/small_wiki_en/englishText_20000_30000...\n",
      "Processing file ../data/small_wiki_en/englishText_40000_50000...\n",
      "Processing file ../data/small_wiki_en/englishText_70000_80000...\n",
      "Processing file ../data/small_wiki_en/englishText_10000_20000...\n",
      "Processing file ../data/small_wiki_en/englishText_0_10000...\n",
      "Processing file ../data/small_wiki_en/englishText_60000_70000...\n",
      "Processing file ../data/small_wiki_en/englishText_50000_60000...\n",
      "Processing file ../data/small_wiki_en/englishText_30000_40000...\n",
      "Processing file ../data/small_wiki_en/englishText_20000_30000...\n",
      "Processing file ../data/small_wiki_en/englishText_40000_50000...\n",
      "Processing file ../data/small_wiki_en/englishText_70000_80000...\n",
      "76548\n"
     ]
    }
   ],
   "source": [
    "# Read corpus\n",
    "word2id, id2word, voc_size = process_corpus(corpus, stop_words)\n",
    "print(voc_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file ../data/small_wiki_en/englishText_10000_20000...\n",
      "Processing file ../data/small_wiki_en/englishText_0_10000...\n",
      "Processing file ../data/small_wiki_en/englishText_60000_70000...\n",
      "Processing file ../data/small_wiki_en/englishText_50000_60000...\n",
      "Processing file ../data/small_wiki_en/englishText_30000_40000...\n",
      "Processing file ../data/small_wiki_en/englishText_20000_30000...\n",
      "Processing file ../data/small_wiki_en/englishText_40000_50000...\n",
      "Processing file ../data/small_wiki_en/englishText_70000_80000...\n"
     ]
    }
   ],
   "source": [
    "# Build cooccurence matrix\n",
    "cooccur_hashtable = build_cooccur_hashtable(corpus, word2id, window_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "cooccur_list = list_data(cooccur_hashtable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start epoch 1\n",
      "Total cost for epoch: 570327.541914\n",
      "Similartity between queen and king: 0.811120\n",
      "Similartity between blue and green: 0.438657\n",
      "Similartity between good and imagine: 1.024532\n",
      "Start epoch 2\n",
      "Total cost for epoch: 534188.566462\n",
      "Similartity between queen and king: 0.722311\n",
      "Similartity between blue and green: 0.350025\n",
      "Similartity between good and imagine: 0.994354\n",
      "Start epoch 3\n",
      "Total cost for epoch: 502735.532637\n",
      "Similartity between queen and king: 0.624445\n",
      "Similartity between blue and green: 0.279142\n",
      "Similartity between good and imagine: 0.951835\n",
      "Start epoch 4\n",
      "Total cost for epoch: 474695.909722\n",
      "Similartity between queen and king: 0.515200\n",
      "Similartity between blue and green: 0.221121\n",
      "Similartity between good and imagine: 0.901978\n",
      "Start epoch 5\n",
      "Total cost for epoch: 449253.340990\n",
      "Similartity between queen and king: 0.436076\n",
      "Similartity between blue and green: 0.184151\n",
      "Similartity between good and imagine: 0.851536\n",
      "Start epoch 6\n",
      "Total cost for epoch: 425876.572075\n",
      "Similartity between queen and king: 0.389024\n",
      "Similartity between blue and green: 0.154894\n",
      "Similartity between good and imagine: 0.801879\n"
     ]
    }
   ],
   "source": [
    "# Train GloVe\n",
    "embeddings, W_word, b_word, W_context, b_context = train_glove(cooccur_list, voc_size,\n",
    "                                                               embed_size=128,\n",
    "                                                               learning_rate=0.01,\n",
    "                                                               n_epoch=6,\n",
    "                                                               init_W_word=W_word,\n",
    "                                                               init_b_word=b_word,\n",
    "                                                               init_W_context=W_context,\n",
    "                                                               init_b_context=b_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7591438679300859"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine(embeddings[word2id['point']], embeddings[word2id['comma']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine([2, 2, 2], [-2, -2, -2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('embeddings.npy', embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
